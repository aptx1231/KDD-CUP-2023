{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sess.shape = (3606249, 3), df_test.shape = (316972, 2)\n",
      "products.shape = (1551057, 11)\n",
      "df_train.shape = (3245624, 3), df_valid.shape = (360625, 3)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "import random\n",
    "from utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "exp_id = int(random.SystemRandom().random() * 100000)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"log/narm_{}.log\".format(exp_id),\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] - %(message)s'\n",
    ")\n",
    "model_name = 'narm'\n",
    "\n",
    "df_sess = pd.read_csv('data/sessions_train.csv')\n",
    "df_test = pd.read_csv('data/phase2/sessions_test_task1.csv')\n",
    "print(f'df_sess.shape = {df_sess.shape}, df_test.shape = {df_test.shape}')\n",
    "logging.info(f'df_sess.shape = {df_sess.shape}, df_test.shape = {df_test.shape}')\n",
    "products = pd.read_csv('data/products_train.csv')\n",
    "print(f'products.shape = {products.shape}')\n",
    "logging.info(f'products.shape = {products.shape}')\n",
    "product2idx = dict(zip(products['id'].unique(), range(1, products['id'].nunique()+1)))\n",
    "idx2product = dict(zip(range(1, products['id'].nunique()+1), products['id'].unique()))\n",
    "product_num = products['id'].nunique() + 1\n",
    "\n",
    "product_dict = dict()\n",
    "locales = ['UK', 'DE', 'JP', 'IT', 'FR', 'ES']\n",
    "\n",
    "for locale in locales:\n",
    "    product_dict[locale] = [product2idx[x] for x in list(products[products['locale']==locale]['id'].unique())]\n",
    "    \n",
    "def str2list(x):\n",
    "    x = x.replace('[', '').replace(']', '').replace(\"'\", '').replace('\\n', ' ').replace('\\r', ' ')\n",
    "    l = [product2idx[i] for i in x.split() if i]\n",
    "    return l\n",
    "\n",
    "\n",
    "df_sess['prev_items'] = df_sess['prev_items'].apply(lambda x: str2list(x))\n",
    "df_test['prev_items'] = df_test['prev_items'].apply(lambda x: str2list(x))\n",
    "\n",
    "df_sess['next_item'] = df_sess['next_item'].apply(lambda x: product2idx[x])\n",
    "\n",
    "df_train, df_valid, _, _ = train_test_split(\n",
    "    df_sess, df_sess['locale'], test_size=0.1, random_state=2023, stratify=df_sess['locale'])\n",
    "\n",
    "print(f'df_train.shape = {df_train.shape}, df_valid.shape = {df_valid.shape}')\n",
    "logging.info(f'df_train.shape = {df_train.shape}, df_valid.shape = {df_valid.shape}')\n",
    "train = (list(df_train[\"prev_items\"]), list(df_train[\"next_item\"]))\n",
    "valid = (list(df_valid[\"prev_items\"]), list(df_valid[\"next_item\"]))\n",
    "\n",
    "\n",
    "def collate_fn_train(data):\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)  #  按长度排序\n",
    "    lens = [len(hist) for hist, _ in data]\n",
    "    labels = []\n",
    "    padded_seq = torch.zeros(len(data), max(lens)).long()\n",
    "    for i, (hist, label) in enumerate(data):\n",
    "        padded_seq[i, :lens[i]] = torch.LongTensor(hist)\n",
    "        labels.append(label)\n",
    "\n",
    "    return padded_seq, torch.tensor(labels).long(), lens\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[0][index], self.data[1][index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "def TrainDataLoader(data, bs=512):\n",
    "    data_set = TrainDataset(data)\n",
    "    data_loader = DataLoader(data_set, batch_size=bs, shuffle=True, collate_fn=collate_fn_train, drop_last=True)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, n_items, hidden_size, embedding_dim, n_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.n_items = n_items\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.emb = nn.Embedding(self.n_items + 1, self.embedding_dim, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(0.25) # 0.25\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, self.n_layers, batch_first=True)\n",
    "        self.a_1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.a_2 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.v_t = nn.Linear(self.hidden_size, 1, bias=False)\n",
    "        self.ct_dropout = nn.Dropout(0.5) # 0.5\n",
    "        self.b = nn.Linear(self.embedding_dim, 2 * self.hidden_size, bias=False)\n",
    "        # self.sf = nn.Softmax()\n",
    "        self.device = device\n",
    "\n",
    "        # self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, seq, lengths):\n",
    "        hidden = self.init_hidden(seq.size(0))\n",
    "        embs = self.emb_dropout(self.emb(seq))\n",
    "        embs = pack_padded_sequence(embs, lengths, batch_first=True)\n",
    "        gru_out, hidden = self.gru(embs, hidden)\n",
    "        gru_out, lengths = pad_packed_sequence(gru_out, batch_first=True)\n",
    "\n",
    "        # fetch the last hidden state of last timestamp\n",
    "        ht = hidden[-1]\n",
    "        # gru_out = gru_out.permute(1, 0, 2)\n",
    "\n",
    "        c_global = ht\n",
    "        q1 = self.a_1(gru_out.contiguous().view(-1, self.hidden_size)).view(gru_out.size())\n",
    "        q2 = self.a_2(ht)\n",
    "        mask = torch.where(seq > 0, torch.tensor([1.], device=self.device),\n",
    "                           torch.tensor([0.], device=self.device))\n",
    "        q2_expand = q2.unsqueeze(1).expand_as(q1)\n",
    "        q2_masked = mask.unsqueeze(2).expand_as(q1) * q2_expand\n",
    "\n",
    "        alpha = self.v_t(torch.sigmoid(q1 + q2_masked).view(-1, self.hidden_size)).view(mask.size())\n",
    "        c_local = torch.sum(alpha.unsqueeze(2).expand_as(gru_out) * gru_out, 1)\n",
    "\n",
    "        c_t = torch.cat([c_local, c_global], 1)\n",
    "        c_t = self.ct_dropout(c_t)\n",
    "\n",
    "        # c_t = self.tanh(c_t)\n",
    "\n",
    "        item_embs = self.emb(torch.arange(self.n_items + 1).to(self.device))\n",
    "        scores = torch.matmul(c_t, self.b(item_embs).permute(1, 0))\n",
    "        # scores = self.sf(scores)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros((self.n_layers, batch_size, self.hidden_size), requires_grad=True).to(self.device)\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, reg=0, eps=1e-6):\n",
    "        super(Loss, self).__init__()\n",
    "        self.reg = reg\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, p, n):\n",
    "        p = torch.exp(p)\n",
    "        n = torch.exp(n)\n",
    "        prob = - torch.log(p / (p + torch.sum(n, dim=1, keepdim=True)) + self.eps)\n",
    "\n",
    "        return prob.sum() + self.reg\n",
    "\n",
    "\n",
    "def evaluate(rec_matrix, targets, match_num):\n",
    "    # (B, 100), (B,), (100, )\n",
    "    target_repeats = torch.repeat_interleave(targets.view(-1, 1), dim=1, repeats=match_num)  # (B, 100)\n",
    "    judge = torch.where(rec_matrix - target_repeats == 0)\n",
    "    hit = len(judge[0])\n",
    "    mrr = 0\n",
    "    ndcg = 0\n",
    "    for pos in judge[1]:\n",
    "        mrr += 1 / (pos.float() + 1)\n",
    "        ndcg += 1 / torch.log2(pos.float() + 2)\n",
    "    return hit, ndcg, mrr\n",
    "\n",
    "\n",
    "item_nuniq = product_num\n",
    "emb_dim = 64\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "match_num = 100\n",
    "gamma = 1e-5\n",
    "mix_recall_num = 100\n",
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (emb): Embedding(1410677, 64, padding_idx=0)\n",
      "  (emb_dropout): Dropout(p=0.25, inplace=False)\n",
      "  (gru): GRU(64, 100, batch_first=True)\n",
      "  (a_1): Linear(in_features=100, out_features=100, bias=False)\n",
      "  (a_2): Linear(in_features=100, out_features=100, bias=False)\n",
      "  (v_t): Linear(in_features=100, out_features=1, bias=False)\n",
      "  (ct_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (b): Linear(in_features=64, out_features=200, bias=False)\n",
      ")\n",
      "emb.weight\ttorch.Size([1410677, 64])\tcuda:0\tTrue\n",
      "gru.weight_ih_l0\ttorch.Size([300, 64])\tcuda:0\tTrue\n",
      "gru.weight_hh_l0\ttorch.Size([300, 100])\tcuda:0\tTrue\n",
      "gru.bias_ih_l0\ttorch.Size([300])\tcuda:0\tTrue\n",
      "gru.bias_hh_l0\ttorch.Size([300])\tcuda:0\tTrue\n",
      "a_1.weight\ttorch.Size([100, 100])\tcuda:0\tTrue\n",
      "a_2.weight\ttorch.Size([100, 100])\tcuda:0\tTrue\n",
      "v_t.weight\ttorch.Size([1, 100])\tcuda:0\tTrue\n",
      "b.weight\ttorch.Size([200, 64])\tcuda:0\tTrue\n",
      "90366028\n"
     ]
    }
   ],
   "source": [
    "model = Model(item_nuniq, hidden_size, emb_dim, n_layers).to(device)\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(str(name) + '\\t' + str(param.shape) + '\\t' +\n",
    "                              str(param.device) + '\\t' + str(param.requires_grad))\n",
    "total_num = sum([param.nelement() for param in model.parameters()])\n",
    "print(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./ckpt/62307/62307_narm.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_test(data):\n",
    "    data.sort(key=lambda x: len(x), reverse=True)  #  按长度排序\n",
    "    lens = [len(hist) for hist in data]\n",
    "    padded_seq = torch.zeros(len(data), max(lens)).long()\n",
    "    for i, (hist) in enumerate(data):\n",
    "        padded_seq[i, :lens[i]] = torch.LongTensor(hist)\n",
    "    return padded_seq, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def TestDataLoader(data, bs=512):\n",
    "    data_set = TestDataset(data)\n",
    "    data_loader = DataLoader(data_set, batch_size=bs, shuffle=True, collate_fn=collate_fn_test, drop_last=False)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(df_test[\"prev_items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = TestDataLoader(test, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2477/2477 [02:47<00:00, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([316972, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_res = []\n",
    "model.eval()\n",
    "for hist_click, lens in tqdm(test_loader, total=len(test_loader)):\n",
    "    # (B, len)\n",
    "    hist_click = hist_click.to(device)\n",
    "    candidates_score = F.softmax(model(hist_click, lens)[:, 1:], dim=1)\n",
    "    candidate_argsort = candidates_score.argsort(dim=1, descending=True)\n",
    "    rec_matrix = candidate_argsort[:, :match_num] + 1  # (B, 100)\n",
    "    # print(hist_click.shape, rec_matrix.shape)\n",
    "    preds_res.append(rec_matrix)\n",
    "\n",
    "preds_res = torch.cat(preds_res, axis=0)\n",
    "print(preds_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316972/316972 [00:24<00:00, 12788.31it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test_origin = pd.read_csv('data/phase2/sessions_test_task1.csv')\n",
    "preds_res = preds_res.cpu().numpy()\n",
    "assert len(preds_res) == len(df_test_origin)\n",
    "test_res_unencoded = []\n",
    "for ind, x in tqdm(enumerate(preds_res), total=len(preds_res)):\n",
    "    x_unencoded = [idx2product[id_] for id_ in x]\n",
    "    test_res_unencoded.append(x_unencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_origin['next_item_prediction'] = test_res_unencoded\n",
    "df_test_origin[['locale', 'next_item_prediction']].to_parquet('output/{}_{}.parquet'.format(exp_id, model_name), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07FCL6SJS, B08GKYXSXV, B01LYL85UF, B0BFRPVB8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07KBFZC3K, B0B4MPKSKF, B009SKHAOC, B009S7401...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B01M7SABBH, B09BVFPXF9, B071P67LCY, B09HCH96J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B08F2V295C, B08K99ZQK8, B0B79JGGDG, B09C254YJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09SPDBCSN, B0BD3QMH91, B07BNF842T, B09GB6F7L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B000VCVPY2, B0BC1DBK44, B09NVYSYYJ, B0772WYFP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B09YJVTLBN, B0BDF6TQ2L, B0000TZ4XE, B0B6154H6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00Q4TNDWS, B00Q4TMZ9K, B09V1L3SXP, B01E6XBBE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08NSX3R2H, B06XPD3BP2, B07ZHTNGPD, B07F84MRM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B01EA0EG3W, 0241186862, B093CC7XJQ, B07GTWD7Z...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B07FCL6SJS, B08GKYXSXV, B01LYL85UF, B0BFRPVB8...\n",
       "1          DE  [B07KBFZC3K, B0B4MPKSKF, B009SKHAOC, B009S7401...\n",
       "2          DE  [B01M7SABBH, B09BVFPXF9, B071P67LCY, B09HCH96J...\n",
       "3          DE  [B08F2V295C, B08K99ZQK8, B0B79JGGDG, B09C254YJ...\n",
       "4          DE  [B09SPDBCSN, B0BD3QMH91, B07BNF842T, B09GB6F7L...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B000VCVPY2, B0BC1DBK44, B09NVYSYYJ, B0772WYFP...\n",
       "316968     UK  [B09YJVTLBN, B0BDF6TQ2L, B0000TZ4XE, B0B6154H6...\n",
       "316969     UK  [B00Q4TNDWS, B00Q4TMZ9K, B09V1L3SXP, B01E6XBBE...\n",
       "316970     UK  [B08NSX3R2H, B06XPD3BP2, B07ZHTNGPD, B07F84MRM...\n",
       "316971     UK  [B01EA0EG3W, 0241186862, B093CC7XJQ, B07GTWD7Z...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_origin[['locale', 'next_item_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_parquet('output/2023_1634_MatchModelV2withATTMatchFold0_99.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07SDFLVKD, B093X59B31, B0BGC82WVW, B091CK241...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B084CB7GX9, B07BDNST44, B0024NKBQE, B004P4QFJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09Z4PZQBF, B0936P3P8D, B09Z4PYG8Q, B0936K9LT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07T6Y2HG7, B07Y1KLF25, B07HQ83TFF, B07T5XY2C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B08SHZHRQ7, B08P94RML3, B09C89S7WG, B08YK8FQJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07GKP2LCF, B07GKYSHB4, B07GKM97YF, B006DDGCI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00M35Y326, B08B395NHL, B000FHC0QK, B091DWY6C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08VD5DC5L, B08VDHH6QF, B0BK7QC4H3, B08VDGMBG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B0B7M72LFQ, B09WCQYGX8, B08W2JJZBM, B0B6C8MZD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B0871BGZL8, B0BFXPL6PS, B07H48412Q, B0BF2ZX1S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B07SDFLVKD, B093X59B31, B0BGC82WVW, B091CK241...\n",
       "1          DE  [B084CB7GX9, B07BDNST44, B0024NKBQE, B004P4QFJ...\n",
       "2          DE  [B09Z4PZQBF, B0936P3P8D, B09Z4PYG8Q, B0936K9LT...\n",
       "3          DE  [B07T6Y2HG7, B07Y1KLF25, B07HQ83TFF, B07T5XY2C...\n",
       "4          DE  [B08SHZHRQ7, B08P94RML3, B09C89S7WG, B08YK8FQJ...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B07GKP2LCF, B07GKYSHB4, B07GKM97YF, B006DDGCI...\n",
       "316968     UK  [B00M35Y326, B08B395NHL, B000FHC0QK, B091DWY6C...\n",
       "316969     UK  [B08VD5DC5L, B08VDHH6QF, B0BK7QC4H3, B08VDGMBG...\n",
       "316970     UK  [B0B7M72LFQ, B09WCQYGX8, B08W2JJZBM, B0B6C8MZD...\n",
       "316971     UK  [B0871BGZL8, B0BFXPL6PS, B07H48412Q, B0BF2ZX1S...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_parquet('output/phase2_submission_task1_rule_next_one_zhaohui_new.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07SDFLVKD, B091CK241X, B0B9GJLV2D, B0BGC82WV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B084CB7GX9, B08XW4W667, B09YD8XV6M, B004P4OF1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09Z4T2GJ3, B09Z3FBXMB, B09Z4PZQBF, B0936K9LT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07T2NBLX9, B07Y1KLF25, B07T6Y2HG7, B07HQ83TF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0B2DRKZ6X, B0B2JY9THB, B08YK8FQJ8, B08SHZHRQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07GKP2LCF, B07GKYSHB4, B016RAAUEM, B006DDGCI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00M35Y326, B000FHC0QK, B08B395NHL, B091DWY6C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08VDGMBGP, B08VDSL596, B08VD5DC5L, B08VDNCZT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B089CZWB4C, B08T1ZJYHV, B08DTYFYGP, B08W2JJZB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07H48412Q, B00FMB9A30, B07H9J1YXN, B09FKB1WV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B07SDFLVKD, B091CK241X, B0B9GJLV2D, B0BGC82WV...\n",
       "1          DE  [B084CB7GX9, B08XW4W667, B09YD8XV6M, B004P4OF1...\n",
       "2          DE  [B09Z4T2GJ3, B09Z3FBXMB, B09Z4PZQBF, B0936K9LT...\n",
       "3          DE  [B07T2NBLX9, B07Y1KLF25, B07T6Y2HG7, B07HQ83TF...\n",
       "4          DE  [B0B2DRKZ6X, B0B2JY9THB, B08YK8FQJ8, B08SHZHRQ...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B07GKP2LCF, B07GKYSHB4, B016RAAUEM, B006DDGCI...\n",
       "316968     UK  [B00M35Y326, B000FHC0QK, B08B395NHL, B091DWY6C...\n",
       "316969     UK  [B08VDGMBGP, B08VDSL596, B08VD5DC5L, B08VDNCZT...\n",
       "316970     UK  [B089CZWB4C, B08T1ZJYHV, B08DTYFYGP, B08W2JJZB...\n",
       "316971     UK  [B07H48412Q, B00FMB9A30, B07H9J1YXN, B09FKB1WV...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libcityng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
