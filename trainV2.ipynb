{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "# import re\n",
    "# import math\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "# from xgboost import XGBRegressor, XGBClassifier\n",
    "# from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import catboost as cab\n",
    "from sklearn import metrics\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "# from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
    "# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "# from sklearn import metrics\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import json \n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# import sentence_transformers \n",
    "# from sklearn.preprocessing import KBinsDiscretizer\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\n",
    "from data import NNDataset, NNDatasetV3\n",
    "from model import MatchModel, BaseModel, MatchModelV2, BinaryModel\n",
    "\n",
    "\"\"\"\n",
    "相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\n",
    "SeqFeatureEmbedding现在使用2层全连接，可以换多层！\n",
    "\"\"\"\n",
    "\n",
    "emb_dim = 16\n",
    "dense_bins = 10\n",
    "hid_dim = 256\n",
    "dropout = 0.3\n",
    "layers = 4\n",
    "bidirectional = False\n",
    "seq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "len_candidate_set = 10\n",
    "Fold = 0\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "train = True\n",
    "load_init = False\n",
    "load_epoch = 28\n",
    "load_exp_id = 11088\n",
    "\n",
    "# TODO: 调整batch-size， 调整hidden-size\n",
    "# TODO: 跑Fold 4\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "early_stop_lr = 1e-6\n",
    "lr_patience = 5\n",
    "lr_decay_ratio = 0.1\n",
    "clip = 5\n",
    "log_every = 100\n",
    "early_stop = True\n",
    "patience = 10\n",
    "kfold = 5\n",
    "attn_match = True \n",
    "\n",
    "w2v_window = 3\n",
    "w2v_min_count = 1\n",
    "w2v_epochs = 500\n",
    "w2v_vector_size = 128\n",
    "\n",
    "seed = 2023\n",
    "set_random_seed(seed)\n",
    "\n",
    "model_name = 'BinaryModelFold{}'.format(Fold)\n",
    "loc2id = {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}\n",
    "\n",
    "config = locals()\n",
    "\n",
    "dense_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:43:58,769 - INFO - Log directory: ./log\n",
      "2023-05-24 14:43:58,770 - INFO - Exp_id 99521\n",
      "2023-05-24 14:43:58,770 - INFO - {'__name__': '__main__', '__doc__': '\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import numpy as np \\nimport pandas as pd\\nimport time\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\nfrom sklearn import metrics\\n# from sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\n\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\n\\nfrom utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\\nfrom data import NNDataset, NNDatasetV3\\nfrom model import MatchModel, BaseModel, MatchModelV2, BinaryModel\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n\"\"\"\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.3\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 10\\nFold = 0\\ndevice = torch.device(\\'cuda:2\\')\\n\\ntrain = True\\nload_init = False\\nload_epoch = 28\\nload_exp_id = 11088\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'BinaryModelFold{}\\'.format(Fold)\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()\\n\\ndense_norm = True', \"# 加载必要的数据\\n\\nexp_id = config.get('exp_id', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\n# top200 = pickle.load(open('data/top200_new.pkl', 'rb'))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\ndf_test_encoded = pd.read_csv('data/df_test_encoded_phase2.csv')\\nproducts_encoded = pd.read_csv('./data/products_encoded.csv')\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\"], '_oh': {}, '_dh': [PosixPath('/home/panda/private/jjw/competition/KDDCUP2023'), PosixPath('/home/panda/private/jjw/competition/KDDCUP2023')], 'In': ['', 'import numpy as np \\nimport pandas as pd\\nimport time\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\nfrom sklearn import metrics\\n# from sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\n\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\n\\nfrom utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\\nfrom data import NNDataset, NNDatasetV3\\nfrom model import MatchModel, BaseModel, MatchModelV2, BinaryModel\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n\"\"\"\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.3\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 10\\nFold = 0\\ndevice = torch.device(\\'cuda:2\\')\\n\\ntrain = True\\nload_init = False\\nload_epoch = 28\\nload_exp_id = 11088\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'BinaryModelFold{}\\'.format(Fold)\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()\\n\\ndense_norm = True', \"# 加载必要的数据\\n\\nexp_id = config.get('exp_id', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\n# top200 = pickle.load(open('data/top200_new.pkl', 'rb'))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\ndf_test_encoded = pd.read_csv('data/df_test_encoded_phase2.csv')\\nproducts_encoded = pd.read_csv('./data/products_encoded.csv')\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\"], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f37876ec550>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f378467b1f0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f378467b1f0>, 'open': <function open at 0x7f3788a29a60>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/home/panda/private/jjw/competition/KDDCUP2023/trainV2.ipynb', '_i': 'import numpy as np \\nimport pandas as pd\\nimport time\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\nfrom sklearn import metrics\\n# from sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\n\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\n\\nfrom utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\\nfrom data import NNDataset, NNDatasetV3\\nfrom model import MatchModel, BaseModel, MatchModelV2, BinaryModel\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n\"\"\"\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.3\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 10\\nFold = 0\\ndevice = torch.device(\\'cuda:2\\')\\n\\ntrain = True\\nload_init = False\\nload_epoch = 28\\nload_exp_id = 11088\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'BinaryModelFold{}\\'.format(Fold)\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()\\n\\ndense_norm = True', '_ii': '', '_iii': '', '_i1': 'import numpy as np \\nimport pandas as pd\\nimport time\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\nfrom sklearn import metrics\\n# from sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\n\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\n\\nfrom utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\\nfrom data import NNDataset, NNDatasetV3\\nfrom model import MatchModel, BaseModel, MatchModelV2, BinaryModel\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n\"\"\"\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.3\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 10\\nFold = 0\\ndevice = torch.device(\\'cuda:2\\')\\n\\ntrain = True\\nload_init = False\\nload_epoch = 28\\nload_exp_id = 11088\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'BinaryModelFold{}\\'.format(Fold)\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()\\n\\ndense_norm = True', 'np': <module 'numpy' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/site-packages/numpy/__init__.py'>, 'pd': <module 'pandas' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/site-packages/pandas/__init__.py'>, 'time': <module 'time' (built-in)>, 'metrics': <module 'sklearn.metrics' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/site-packages/sklearn/metrics/__init__.py'>, 'cross_val_score': <function cross_val_score at 0x7f3663b77ee0>, 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'plt': <module 'matplotlib.pyplot' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/site-packages/matplotlib/pyplot.py'>, 'warnings': <module 'warnings' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/warnings.py'>, 'json': <module 'json' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/json/__init__.py'>, 'pickle': <module 'pickle' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/pickle.py'>, 'random': <module 'random' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/random.py'>, 'tqdm': <class 'tqdm.std.tqdm'>, 'LabelEncoder': <class 'sklearn.preprocessing._label.LabelEncoder'>, 'MinMaxScaler': <class 'sklearn.preprocessing._data.MinMaxScaler'>, 'torch': <module 'torch' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/site-packages/torch/__init__.py'>, 'nn': <module 'torch.nn' from '/home/panda/anaconda3/envs/cikm/lib/python3.9/site-packages/torch/nn/__init__.py'>, 'pack_padded_sequence': <function pack_padded_sequence at 0x7f36e99b1af0>, 'pad_packed_sequence': <function pad_packed_sequence at 0x7f36e99b1b80>, 'random_split': <function random_split at 0x7f35f50ac310>, 'DataLoader': <class 'torch.utils.data.dataloader.DataLoader'>, 'Dataset': <class 'torch.utils.data.dataset.Dataset'>, 'set_random_seed': <function set_random_seed at 0x7f35f4dff940>, 'get_logger': <function get_logger at 0x7f35f4dffaf0>, 'ensure_dir': <function ensure_dir at 0x7f35f4dffa60>, 'save_model_with_epoch': <function save_model_with_epoch at 0x7f35f4dffb80>, 'load_model_with_epoch': <function load_model_with_epoch at 0x7f35f4dffc10>, 'NNDataset': <class 'data.NNDataset'>, 'NNDatasetV3': <class 'data.NNDatasetV3'>, 'MatchModel': <class 'model.MatchModel'>, 'BaseModel': <class 'model.BaseModel'>, 'MatchModelV2': <class 'model.MatchModelV2'>, 'BinaryModel': <class 'model.BinaryModel'>, 'emb_dim': 16, 'dense_bins': 10, 'hid_dim': 256, 'dropout': 0.3, 'layers': 4, 'bidirectional': False, 'seq_emb_factor': 4, 'batch_size': 1024, 'epochs': 100, 'len_candidate_set': 10, 'Fold': 0, 'device': device(type='cuda', index=2), 'train': True, 'load_init': False, 'load_epoch': 28, 'load_exp_id': 11088, 'learning_rate': 0.001, 'weight_decay': 1e-05, 'early_stop_lr': 1e-06, 'lr_patience': 5, 'lr_decay_ratio': 0.1, 'clip': 5, 'log_every': 100, 'early_stop': True, 'patience': 10, 'kfold': 5, 'attn_match': True, 'w2v_window': 3, 'w2v_min_count': 1, 'w2v_epochs': 500, 'w2v_vector_size': 128, 'seed': 2023, 'model_name': 'BinaryModelFold0', 'loc2id': {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}, 'config': {...}, 'dense_norm': True, '_i2': \"# 加载必要的数据\\n\\nexp_id = config.get('exp_id', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\n# top200 = pickle.load(open('data/top200_new.pkl', 'rb'))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\ndf_test_encoded = pd.read_csv('data/df_test_encoded_phase2.csv')\\nproducts_encoded = pd.read_csv('./data/products_encoded.csv')\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\", 'exp_id': 99521, 'logger': <RootLogger root (INFO)>}\n",
      "2023-05-24 14:43:58,772 - INFO - read data\n",
      "2023-05-24 14:45:10,026 - INFO - titles_embedding: (1410675, 384)\n",
      "2023-05-24 14:45:10,028 - INFO - descs_embedding: (1410675, 384)\n",
      "2023-05-24 14:45:12,708 - INFO - product2id: 1410675\n",
      "2023-05-24 14:45:12,710 - INFO - id2product: 1410675\n",
      "2023-05-24 14:45:14,666 - INFO - word2vec_embedding: (1410675, 128)\n",
      "2023-05-24 14:45:19,604 - INFO - df_train_encoded: (3606249, 4)\n",
      "2023-05-24 14:45:19,606 - INFO - df_test_encoded: (316972, 4)\n",
      "2023-05-24 14:45:19,606 - INFO - products_encoded: (1410675, 14)\n"
     ]
    }
   ],
   "source": [
    "# 加载必要的数据\n",
    "\n",
    "exp_id = config.get('exp_id', None)\n",
    "if exp_id is None:\n",
    "    exp_id = int(random.SystemRandom().random() * 100000)\n",
    "    config['exp_id'] = exp_id\n",
    "\n",
    "logger = get_logger(config)\n",
    "logger.info('Exp_id {}'.format(exp_id))\n",
    "logger.info(config)\n",
    "\n",
    "logger.info('read data')\n",
    "\n",
    "titles_embedding = np.load('./data/titles_embedding.npy')\n",
    "descs_embedding = np.load('./data/descs_embedding.npy')\n",
    "logger.info('titles_embedding: {}'.format(titles_embedding.shape))\n",
    "logger.info('descs_embedding: {}'.format(descs_embedding.shape))\n",
    "\n",
    "product2id = json.load(open('data/product2id.json', 'r'))\n",
    "id2product = json.load(open('data/id2product.json', 'r'))\n",
    "id2product = {int(k): v for k, v in id2product.items()}\n",
    "logger.info('product2id: {}'.format(len(product2id)))\n",
    "logger.info('id2product: {}'.format(len(id2product)))\n",
    "\n",
    "word2vec_embedding = np.load('./data/word2vec_embedding.npy')\n",
    "logger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\n",
    "\n",
    "# top200 = pickle.load(open('data/top200_new.pkl', 'rb'))\n",
    "\n",
    "df_train_encoded = pd.read_csv('data/df_train_encoded.csv')\n",
    "df_test_encoded = pd.read_csv('data/df_test_encoded_phase2.csv')\n",
    "products_encoded = pd.read_csv('./data/products_encoded.csv')\n",
    "logger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\n",
    "logger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\n",
    "logger.info('products_encoded: {}'.format(products_encoded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:45:19,655 - INFO - MinMaxScaler Norm products_num_feas\n",
      "2023-05-24 14:46:13,571 - INFO - train_preds_encoded: 3606249\n",
      "2023-05-24 14:46:13,572 - INFO - test_preds_encoded: 316972\n",
      "2023-05-24 14:46:13,573 - INFO - test_preds: 316972\n",
      "2023-05-24 14:46:13,574 - INFO - Cutting the candidate_set to 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3606249/3606249 [00:17<00:00, 209310.46it/s]\n",
      "100%|██████████| 316972/316972 [00:08<00:00, 38382.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:46:39,435 - INFO - Eval the prev_items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = ['price', 'len_title', 'len_desc']\n",
    "if dense_norm:\n",
    "    logger.info('MinMaxScaler Norm products_num_feas')\n",
    "    mms = MinMaxScaler(feature_range=(0,1))\n",
    "    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\n",
    "for fe in num_features:\n",
    "    products_encoded[fe] = products_encoded[fe].astype('float32')\n",
    "    assert products_encoded[fe].dtypes == 'float32'\n",
    "\n",
    "id_count = products_encoded.shape[0]\n",
    "\n",
    "train_preds_encoded = pickle.load(open('./data/train_preds_all_encoded_new_phase2.pkl', 'rb'))  # (len_train, 100)\n",
    "test_preds_encoded = pickle.load(open('./data/test_preds_all_encoded_new_phase2.pkl', 'rb'))  # (len_test, 100)\n",
    "test_preds = pickle.load(open('./data/test_preds_phase2.pkl', 'rb'))\n",
    "logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\n",
    "logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\n",
    "logger.info('test_preds: {}'.format(len(test_preds)))\n",
    "\n",
    "logger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\n",
    "# TODO: 可以改成保障一个正样本，补充9个负样本\n",
    "cut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\n",
    "df_train_encoded['recall'] = cut_train_preds_encoded\n",
    "cut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\n",
    "df_test_encoded['recall'] = cut_test_preds_encoded\n",
    "\n",
    "logger.info('Eval the prev_items')\n",
    "df_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\n",
    "df_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:47:28,376 - INFO - Load Hand-made Seq Features\n",
      "2023-05-24 14:47:38,736 - INFO - df_train_seqs_feas_all: (3606249, 29)\n",
      "2023-05-24 14:47:38,737 - INFO - df_test_seqs_feas_all: (316972, 29)\n",
      "2023-05-24 14:47:38,738 - INFO - seqs_cat_feas: ['idNUNIQUE', 'idCOUNT', 'brandNUNIQUE', 'brandCOUNT', 'colorNUNIQUE', 'colorCOUNT', 'sizeNUNIQUE', 'sizeCOUNT', 'modelNUNIQUE', 'modelCOUNT', 'materialNUNIQUE', 'materialCOUNT', 'authorNUNIQUE', 'authorCOUNT']\n",
      "2023-05-24 14:47:38,738 - INFO - seqs_num_feas: ['priceMEAN', 'priceSTD', 'priceMIN', 'priceMAX', 'priceSUM', 'len_titleMEAN', 'len_titleSTD', 'len_titleMIN', 'len_titleMAX', 'len_titleSUM', 'len_descMEAN', 'len_descSTD', 'len_descMIN', 'len_descMAX', 'len_descSUM']\n",
      "2023-05-24 14:47:38,739 - INFO - MinMaxScaler Norm seqs_num_feas\n",
      "2023-05-24 14:47:40,284 - INFO - df_train_all: (3606249, 34)\n",
      "2023-05-24 14:47:40,285 - INFO - df_test_all: (316972, 34)\n",
      "2023-05-24 14:48:02,765 - INFO - df_train_all_exploded: (36062490, 36)\n",
      "2023-05-24 14:48:02,766 - INFO - df_test_all_exploded: (3169720, 36)\n",
      "2023-05-24 14:48:02,788 - INFO - df_train_all_exploded.label.sum: 2794545\n",
      "2023-05-24 14:48:02,791 - INFO - df_test_all_exploded.label.sum: 0\n",
      "2023-05-24 14:48:07,231 - INFO - df_train_encoded_exploded: (36062490, 6)\n",
      "2023-05-24 14:48:07,232 - INFO - df_train_seqs_cat_feas: (36062490, 14)\n",
      "2023-05-24 14:48:07,233 - INFO - df_train_seqs_num_feas: (36062490, 15)\n",
      "2023-05-24 14:48:07,233 - INFO - df_test_encoded_exploded: (3169720, 6)\n",
      "2023-05-24 14:48:07,234 - INFO - df_test_seqs_cat_feas: (3169720, 14)\n",
      "2023-05-24 14:48:07,234 - INFO - df_test_seqs_num_feas: (3169720, 15)\n"
     ]
    }
   ],
   "source": [
    "logger.info('Load Hand-made Seq Features')\n",
    "df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\n",
    "df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2.csv')\n",
    "logger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\n",
    "logger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\n",
    "seqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f]\n",
    "seqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\n",
    "logger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\n",
    "logger.info('seqs_num_feas: {}'.format(seqs_num_feas))\n",
    "\n",
    "if dense_norm:\n",
    "    logger.info('MinMaxScaler Norm seqs_num_feas')\n",
    "    mms = MinMaxScaler(feature_range=(0,1))\n",
    "    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\n",
    "    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\n",
    "\n",
    "for fe in seqs_num_feas:\n",
    "    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\n",
    "    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\n",
    "\n",
    "df_train_all = pd.concat([df_train_encoded, df_train_seqs_feas_all], axis=1)\n",
    "df_test_all = pd.concat([df_test_encoded, df_test_seqs_feas_all], axis=1)\n",
    "logger.info('df_train_all: {}'.format(df_train_all.shape))\n",
    "logger.info('df_test_all: {}'.format(df_test_all.shape))\n",
    "\n",
    "df_train_all_exploded = df_train_all.explode('recall')\n",
    "df_test_all_exploded = df_test_all.explode('recall')\n",
    "\n",
    "df_train_all_exploded['label'] = df_train_all_exploded['next_item'] == df_train_all_exploded['recall']\n",
    "df_test_all_exploded['label'] = df_test_all_exploded['next_item'] == df_test_all_exploded['recall']\n",
    "\n",
    "df_train_all_exploded['index'] = df_train_all_exploded.index \n",
    "df_test_all_exploded['index'] = df_test_all_exploded.index \n",
    "\n",
    "logger.info('df_train_all_exploded: {}'.format(df_train_all_exploded.shape))\n",
    "logger.info('df_test_all_exploded: {}'.format(df_test_all_exploded.shape))\n",
    "\n",
    "logger.info('df_train_all_exploded.label.sum: {}'.format(df_train_all_exploded['label'].sum()))\n",
    "logger.info('df_test_all_exploded.label.sum: {}'.format(df_test_all_exploded['label'].sum()))\n",
    "\n",
    "df_train_encoded_exploded = df_train_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\n",
    "df_train_seqs_cat_feas = df_train_all_exploded[seqs_cat_feas]\n",
    "df_train_seqs_num_feas = df_train_all_exploded[seqs_num_feas]\n",
    "df_test_encoded_exploded = df_test_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\n",
    "df_test_seqs_cat_feas = df_test_all_exploded[seqs_cat_feas]\n",
    "df_test_seqs_num_feas = df_test_all_exploded[seqs_num_feas]\n",
    "\n",
    "del df_train_all_exploded\n",
    "del df_test_all_exploded\n",
    "del df_train_all\n",
    "del df_test_all\n",
    "\n",
    "logger.info('df_train_encoded_exploded: {}'.format(df_train_encoded_exploded.shape))\n",
    "logger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\n",
    "logger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\n",
    "logger.info('df_test_encoded_exploded: {}'.format(df_test_encoded_exploded.shape))\n",
    "logger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\n",
    "logger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:48:07,651 - INFO - df_test: (316972, 2)\n",
      "2023-05-24 14:48:08,067 - INFO - data_feature:\n",
      "2023-05-24 14:48:08,068 - INFO - {'len_encode_brand': 177190, 'len_encode_color': 203261, 'len_encode_size': 218061, 'len_encode_model': 524102, 'len_encode_material': 45569, 'len_encode_author': 30836, 'len_locale': 6, 'dense_bins': 10, 'id_count': 1410675, 'len_features': 13, 'len_emb_features': 3, 'len_candidate_set': 10, 'w2v_vector_size': 128, 'sentence_vector_size': 384, 'len_seqs_cat_feas': 14, 'len_seqs_num_feas': 15, 'seq_emb_factor': 4, 'idNUNIQUE': 133, 'idCOUNT': 475, 'brandNUNIQUE': 39, 'brandCOUNT': 475, 'colorNUNIQUE': 54, 'colorCOUNT': 475, 'sizeNUNIQUE': 116, 'sizeCOUNT': 475, 'modelNUNIQUE': 59, 'modelCOUNT': 475, 'materialNUNIQUE': 26, 'materialCOUNT': 475, 'authorNUNIQUE': 65, 'authorCOUNT': 475}\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/phase2/sessions_test_task1.csv')\n",
    "logger.info('df_test: {}'.format(df_test.shape))\n",
    "\n",
    "tmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\n",
    "tmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\n",
    "\n",
    "data_feature = {}\n",
    "data_feature['len_encode_brand'] = products_encoded['encode_brand'].nunique()\n",
    "data_feature['len_encode_color'] = products_encoded['encode_color'].nunique()\n",
    "data_feature['len_encode_size'] = products_encoded['encode_size'].nunique()\n",
    "data_feature['len_encode_model'] = products_encoded['encode_model'].nunique()\n",
    "data_feature['len_encode_material'] = products_encoded['encode_material'].nunique()\n",
    "data_feature['len_encode_author'] = products_encoded['encode_author'].nunique()\n",
    "data_feature['len_locale'] = len(loc2id)\n",
    "data_feature['dense_bins'] = dense_bins\n",
    "data_feature['id_count'] = id_count\n",
    "data_feature['len_features'] = products_encoded.shape[1] - 1\n",
    "data_feature['len_emb_features'] = 3\n",
    "data_feature['len_candidate_set'] = len_candidate_set\n",
    "data_feature['w2v_vector_size'] = w2v_vector_size\n",
    "data_feature['sentence_vector_size'] = 384\n",
    "data_feature['len_seqs_cat_feas'] = len(seqs_cat_feas)\n",
    "data_feature['len_seqs_num_feas'] = len(seqs_num_feas)\n",
    "data_feature['seq_emb_factor'] = seq_emb_factor\n",
    "data_feature.update(tmp_nunique)\n",
    "logger.info('data_feature:')\n",
    "logger.info(data_feature)\n",
    "\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_size = int(df_train_encoded_exploded.shape[0] / len_candidate_set // kfold) * len_candidate_set  # 7212490\n",
    "index_list = list(range(0, len(df_train_encoded_exploded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7212490\n",
      "7212490 14424980\n",
      "14424980 21637470\n",
      "21637470 28849960\n",
      "28849960 36062450\n"
     ]
    }
   ],
   "source": [
    "val_indexes = []\n",
    "start = 0\n",
    "for i in range(kfold):\n",
    "    end = min(start + fold_size, len(index_list))\n",
    "    print(start, end)\n",
    "    val_indexes.append(index_list[start:end])\n",
    "    # print(np.min(val_indexes[-1]), np.max(val_indexes[-1]), len(val_indexes[-1]))\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n",
      "Fold 2:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n",
      "Fold 3:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n",
      "Fold 4:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n",
      "Fold 5:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n"
     ]
    }
   ],
   "source": [
    "train_indexes = []\n",
    "for i, fold in enumerate(val_indexes):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(\"Validation set:\", len(fold))\n",
    "    train_indexes.append([index for sublist in val_indexes[:i] + val_indexes[i+1:] for index in sublist])\n",
    "    print(\"Training set:\", len(train_indexes[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/5fold_trn_idx_list_binary_phase2_{}.npy'.format(len_candidate_set), train_indexes)\n",
    "np.save('data/5fold_val_idx_list_binary_phase2_{}.npy'.format(len_candidate_set), val_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:49:45,930 - INFO - create model\n"
     ]
    }
   ],
   "source": [
    "# 加载模型等\n",
    "\n",
    "logger.info('create model')\n",
    "\n",
    "products_input = {name: torch.tensor(products_encoded[name].values).to(device) for name in products_encoded.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['heads'] = 8\n",
    "config['seq_model'] = 'transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'add_title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m BinaryModel(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m logger\u001b[39m.\u001b[39minfo(model)\n\u001b[1;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay)\n",
      "File \u001b[0;32m~/private/jjw/competition/KDDCUP2023/model.py:915\u001b[0m, in \u001b[0;36mBinaryModel.__init__\u001b[0;34m(self, config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_count \u001b[39m=\u001b[39m data_feature[\u001b[39m'\u001b[39m\u001b[39mid_count\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    913\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m=\u001b[39m data_feature[\u001b[39m'\u001b[39m\u001b[39mlen_features\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_dim \u001b[39m+\u001b[39m data_feature[\u001b[39m'\u001b[39m\u001b[39mlen_emb_features\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_dim\n\u001b[0;32m--> 915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproduct_emb \u001b[39m=\u001b[39m ProductEmbedding(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding)\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_fea_emb \u001b[39m=\u001b[39m SeqFeatureEmbedding(config, data_feature)\n\u001b[1;32m    917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_model\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlstm\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/private/jjw/competition/KDDCUP2023/model.py:76\u001b[0m, in \u001b[0;36mProductEmbedding.__init__\u001b[0;34m(self, config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_dim \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39memb_dim\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_title \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39;49m\u001b[39madd_title\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_desc \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39madd_desc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_w2v \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39madd_w2v\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'add_title'"
     ]
    }
   ],
   "source": [
    "model = BinaryModel(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\n",
    "logger.info(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', patience=lr_patience, factor=lr_decay_ratio)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    logger.info(str(name) + '\\t' + str(param.shape) + '\\t' +\n",
    "                              str(param.device) + '\\t' + str(param.requires_grad))\n",
    "total_num = sum([param.nelement() for param in model.parameters()])\n",
    "logger.info('Total parameter numbers: {}'.format(total_num))\n",
    "\n",
    "\n",
    "# 数据集DataLoader\n",
    "\n",
    "trn_idx_list = np.load('data/5fold_trn_idx_list_binary_phase2_{}.npy'.format(len_candidate_set), allow_pickle=True)\n",
    "val_idx_list = np.load('data/5fold_val_idx_list_binary_phase2_{}.npy'.format(len_candidate_set), allow_pickle=True)\n",
    "logger.info('Fold {}: trn_idx {}'.format(Fold, len(trn_idx_list[Fold])))\n",
    "logger.info('Fold {}: val_idx {}'.format(Fold, len(val_idx_list[Fold])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 05:38:55,366 - INFO - train_set: 28849960\n",
      "2023-05-24 05:38:55,368 - INFO - val_set: 7212490\n",
      "2023-05-24 05:38:55,369 - INFO - test_set: 3169720\n"
     ]
    }
   ],
   "source": [
    "train_set = NNDatasetV3(df_train_encoded_exploded.iloc[trn_idx_list[Fold]], \n",
    "                        df_train_seqs_cat_feas.iloc[trn_idx_list[Fold]], \n",
    "                        df_train_seqs_num_feas.iloc[trn_idx_list[Fold]])\n",
    "val_set = NNDatasetV3(df_train_encoded_exploded.iloc[val_idx_list[Fold]], \n",
    "                      df_train_seqs_cat_feas.iloc[val_idx_list[Fold]], \n",
    "                      df_train_seqs_num_feas.iloc[val_idx_list[Fold]])\n",
    "test_set = NNDatasetV3(df_test_encoded_exploded, df_test_seqs_cat_feas, \n",
    "                       df_test_seqs_num_feas)\n",
    "logger.info('train_set: {}'.format(len(train_set)))\n",
    "logger.info('val_set: {}'.format(len(val_set)))\n",
    "logger.info('test_set: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 05:39:10,651 - INFO - train_loader: 28174\n",
      "2023-05-24 05:39:10,653 - INFO - val_loader: 7044\n",
      "2023-05-24 05:39:10,654 - INFO - test_loader: 3096\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(indices):\n",
    "    batch_prev_items = []\n",
    "    batch_locale = []\n",
    "    batch_len = []\n",
    "    batch_mask = []\n",
    "    batch_candidate = []\n",
    "    batch_origin_label = []\n",
    "    batch_label = []\n",
    "    batch_seq_cat = []\n",
    "    batch_seq_num = []\n",
    "    batch_index = []\n",
    "    for item in indices:\n",
    "        batch_len.append(len(item[0]))  # prev_items\n",
    "    max_len = max(batch_len)\n",
    "    for item in indices:\n",
    "        l = len(item[0])\n",
    "        batch_mask.append([1] * (l) + [0] * (max_len - l))  # 0代表padding的位置，需要mask\n",
    "    for item in indices:\n",
    "        # ['prev_items', 'locale', 'recall', 'next_item', label', 'index', 'seqs_cat_feas', 'seqs_num_feas']\n",
    "        prev_items = item[0].copy()\n",
    "        while (len(prev_items) < max_len):\n",
    "            prev_items.append(id_count)  # embdding的时候id_count+1，把id_count作为padding了\n",
    "        batch_prev_items.append(prev_items)\n",
    "        batch_locale.append(item[1])\n",
    "        batch_candidate.append(item[2])\n",
    "        batch_origin_label.append(item[3])\n",
    "        batch_label.append(item[4])\n",
    "        batch_index.append(item[5])\n",
    "        batch_seq_cat.append(item[6])\n",
    "        batch_seq_num.append(item[7])\n",
    "    return [torch.LongTensor(batch_prev_items).to(device), torch.LongTensor(batch_locale).to(device), \n",
    "            torch.LongTensor(batch_len).to(device), torch.LongTensor(batch_mask).to(device), \n",
    "            torch.LongTensor(batch_candidate).to(device), torch.FloatTensor(batch_label).to(device), \n",
    "            torch.LongTensor(batch_origin_label).to(device), torch.LongTensor(batch_index).to(device), \n",
    "            torch.LongTensor(batch_seq_cat).to(device), torch.FloatTensor(batch_seq_num).to(device)]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "logger.info('train_loader: {}'.format(len(train_loader)))\n",
    "logger.info('val_loader: {}'.format(len(val_loader)))\n",
    "logger.info('test_loader: {}'.format(len(test_loader)))\n",
    "\n",
    "\n",
    "output_dir = 'ckpt/{}'.format(exp_id)\n",
    "ensure_dir(output_dir)\n",
    "\n",
    "if load_init:\n",
    "    load_dir = 'ckpt/{}'.format(load_exp_id)\n",
    "    load_path = '{}/{}_{}_{}.pt'.format(load_dir, load_exp_id, model_name, load_epoch)\n",
    "    logger.info('Load Init model from {}'.format(load_path))\n",
    "    model.load_state_dict(torch.load(load_path, map_location='cpu'))\n",
    "    # print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train model 55974:   0%|          | 0/28174 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 27]) torch.Size([1024]) torch.Size([1024]) torch.Size([1024, 27]) torch.Size([1024]) torch.Size([1024]) torch.Size([1024]) torch.Size([1024]) torch.Size([1024, 14]) torch.Size([1024, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_prev_items, batch_locale, batch_len, batch_mask, batch_candidate, \\\n",
    "                batch_label, batch_origin_label, batch_index, batch_seq_cat, batch_seq_num in tqdm(train_loader, desc='train model {}'.format(exp_id), total=len(train_loader)):\n",
    "    print(batch_prev_items.shape, batch_locale.shape, batch_len.shape, batch_mask.shape, batch_candidate.shape, \n",
    "    batch_label.shape, batch_index.shape, batch_origin_label.shape, batch_seq_cat.shape, batch_seq_num.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  75028, 1180767,  282826,  ...,  944979, 1302563,  995582],\n",
       "       device='cuda:2')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_origin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 1.], device='cuda:2')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1241319, 1306907,  276309,  ..., 1044726, 1015445,  995582],\n",
       "       device='cuda:2')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_candidate    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3099351, 3173155, 2763189,  ..., 2227456, 2395310, 2521688],\n",
       "       device='cuda:2')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, loss = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \n",
    "                                        batch_candidate_set=batch_candidate, batch_len=batch_len, \n",
    "                                        batch_label=batch_label.unsqueeze(-1), batch_mask=batch_mask,\n",
    "                                        batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6929, device='cuda:2', grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_label_index(group):\n",
    "    label = group['train_origin_label'].iloc[0]\n",
    "    candidate = group['train_candidate'].tolist()\n",
    "    try:\n",
    "        index = candidate.index(label)\n",
    "        rrank = 1 / (index + 1)\n",
    "    except:\n",
    "        rrank = 0\n",
    "    return pd.Series({'label_rrank': rrank})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_label_index(group):\n",
    "    label = group['val_origin_label'].iloc[0]\n",
    "    candidate = group['val_candidate'].tolist()\n",
    "    try:\n",
    "        index = candidate.index(label)\n",
    "        rrank = 1 / (index + 1)\n",
    "    except:\n",
    "        rrank = 0\n",
    "    return pd.Series({'label_rrank': rrank})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_label_index(group):\n",
    "    label = group['test_origin_label'].iloc[0]\n",
    "    candidate = group['test_candidate'].tolist()\n",
    "    try:\n",
    "        index = candidate.index(label)\n",
    "        rrank = 1 / (index + 1)\n",
    "    except:\n",
    "        rrank = 0\n",
    "    return pd.Series({'label_rrank': rrank})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0786770463862914"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'train_candidate': train_candidate_list,\n",
    "    'train_index': train_index_list,\n",
    "    'train_origin_label': train_origin_label_list,\n",
    "    'train_y_pred': train_y_pred_list\n",
    "})\n",
    "df_sorted = df.sort_values(['train_index', 'train_y_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "df_grouped = df_sorted.groupby('train_index').apply(get_train_label_index).reset_index()\n",
    "train_mrr = np.mean(df_grouped['label_rrank'].values)\n",
    "train_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_mrr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 06:46:53,616 - INFO - Training...\n",
      "2023-05-24 06:46:53,618 - INFO - start train epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train model 55974:   0%|          | 0/28174 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 06:46:57,315 - INFO - Train Epoch 0, Logloss: 0.2517, AUC: 0.6519,                     Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 06:46:57,527 - INFO - Train Epoch 0, MRR: 0.0752, Time: 0.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val model 55974:   0%|          | 0/7044 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 06:46:58,109 - INFO - Val Epoch 0, Logloss: 0.2878, AUC: 0.6492,                     Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 06:46:58,329 - INFO - Train Epoch 0, MRR: 0.0752, Time: 0.2190\n",
      "2023-05-24 06:46:58,330 - INFO - Epoch 0/1 Complete: Train Loss 0.251707, Train MRR 0.075195, Val Loss 0.287762, Val MRR 0.089844, lr 0.001\n",
      "2023-05-24 06:46:58,330 - INFO - Save model to ckpt/55974/55974_MatchModelV2withATTMatchFold0.pt\n",
      "2023-05-24 06:47:15,091 - INFO - best_epoch 0, AUC = 0.6491532935249114\n",
      "2023-05-24 06:47:15,092 - INFO - Load model from ckpt/55974/55974_MatchModelV2withATTMatchFold0.pt\n",
      "2023-05-24 06:47:21,393 - INFO - Save model to ckpt/55974/55974_MatchModelV2withATTMatchFold0_0.pt\n"
     ]
    }
   ],
   "source": [
    "best_epoch = -1\n",
    "# 训练\n",
    "if train:\n",
    "    logger.info('Training...')\n",
    "    output_dir = 'ckpt/{}'.format(exp_id)\n",
    "    ensure_dir(output_dir)\n",
    "    auc_all = []\n",
    "    mrr_all = []\n",
    "    min_val_loss = float('inf')\n",
    "    max_val_auc = 0.0\n",
    "    # best_epoch = -1\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        logger.info('start train epoch {}'.format(epoch))\n",
    "        model.train()\n",
    "        train_loss_list = []\n",
    "        train_candidate_list = []\n",
    "        train_y_pred_list = []\n",
    "        train_index_list = []\n",
    "        train_label_list = []\n",
    "        train_origin_label_list = []\n",
    "        for batch_prev_items, batch_locale, batch_len, batch_mask, batch_candidate, \\\n",
    "                batch_label, batch_origin_label, batch_index, batch_seq_cat, batch_seq_num \\\n",
    "                in tqdm(train_loader, desc='train model {}'.format(exp_id), total=len(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            # y_pred (B, 1), batch_candidate / batch_origin_label / batch_label / batch_index (B, )\n",
    "            y_pred, loss = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \n",
    "                                        batch_candidate_set=batch_candidate, batch_len=batch_len, \n",
    "                                        batch_label=batch_label.unsqueeze(-1), batch_mask=batch_mask,\n",
    "                                        batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)\n",
    "            # TODO: 去掉？\n",
    "            loss.backward(retain_graph=True)\n",
    "            train_loss_list.append(loss.item())\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                train_candidate_list += batch_candidate.tolist()\n",
    "                train_y_pred_list += y_pred.squeeze(-1).tolist()\n",
    "                train_index_list += batch_index.tolist()\n",
    "                train_label_list += batch_label.tolist()\n",
    "                train_origin_label_list += batch_origin_label.tolist()\n",
    "            break\n",
    "        # binary-metric\n",
    "        train_logloss = metrics.log_loss(train_label_list, train_y_pred_list)\n",
    "        train_auc = metrics.roc_auc_score(train_label_list, train_y_pred_list)\n",
    "        train_precision = metrics.precision_score(train_label_list, [1 if i >= 0.5 else 0 for i in train_y_pred_list])\n",
    "        train_recall = metrics.recall_score(train_label_list, [1 if i >= 0.5 else 0 for i in train_y_pred_list])\n",
    "        train_f1 = metrics.f1_score(train_label_list, [1 if i >= 0.5 else 0 for i in train_y_pred_list])\n",
    "        logger.info(f\"Train Epoch {epoch}, Logloss: {train_logloss:.4f}, AUC: {train_auc:.4f}, \\\n",
    "                    Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1 Score: {train_f1:.4f}\")\n",
    "        # MRR\n",
    "        if log_train_mrr:\n",
    "            t1 = time.time()\n",
    "            df = pd.DataFrame({\n",
    "                'train_candidate': train_candidate_list,\n",
    "                'train_index': train_index_list,\n",
    "                'train_origin_label': train_origin_label_list,\n",
    "                'train_y_pred': train_y_pred_list\n",
    "            })\n",
    "            df_sorted = df.sort_values(['train_index', 'train_y_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "            df_grouped = df_sorted.groupby('train_index').apply(get_train_label_index)\n",
    "            train_mrr = np.mean(df_grouped['label_rrank'].values)\n",
    "            t2 = time.time()\n",
    "            logger.info(f\"Train Epoch {epoch}, MRR: {train_mrr:.4f}, Time: {t2-t1:.4f}\")\n",
    "        else:\n",
    "            train_mrr = 0\n",
    "        train_loss = np.mean(train_loss_list)\n",
    "        # val\n",
    "        val_loss_list= []\n",
    "        val_candidate_list = []\n",
    "        val_y_pred_list = []\n",
    "        val_index_list = []\n",
    "        val_label_list = []\n",
    "        val_origin_label_list = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_prev_items, batch_locale, batch_len, batch_mask, batch_candidate, \\\n",
    "                batch_label, batch_origin_label, batch_index, batch_seq_cat, batch_seq_num \\\n",
    "                in tqdm(val_loader, desc='val model {}'.format(exp_id), total=len(val_loader)):\n",
    "                # y_pred (B, 1), batch_candidate / batch_origin_label / batch_label / batch_index (B, )\n",
    "                y_pred, loss = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \n",
    "                                            batch_candidate_set=batch_candidate, batch_len=batch_len, \n",
    "                                            batch_label=batch_label.unsqueeze(-1), batch_mask=batch_mask,\n",
    "                                            batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)\n",
    "                val_loss_list.append(loss.item())\n",
    "                with torch.no_grad():\n",
    "                    val_candidate_list += batch_candidate.tolist()\n",
    "                    val_y_pred_list += y_pred.squeeze(-1).tolist()\n",
    "                    val_index_list += batch_index.tolist()\n",
    "                    val_label_list += batch_label.tolist()\n",
    "                    val_origin_label_list += batch_origin_label.tolist()\n",
    "                break\n",
    "        # binary-metric\n",
    "        val_logloss = metrics.log_loss(val_label_list, val_y_pred_list)\n",
    "        val_auc = metrics.roc_auc_score(val_label_list, val_y_pred_list)\n",
    "        val_precision = metrics.precision_score(val_label_list, [1 if i >= 0.5 else 0 for i in val_y_pred_list])\n",
    "        val_recall = metrics.recall_score(val_label_list, [1 if i >= 0.5 else 0 for i in val_y_pred_list])\n",
    "        val_f1 = metrics.f1_score(val_label_list, [1 if i >= 0.5 else 0 for i in val_y_pred_list])\n",
    "        logger.info(f\"Val Epoch {epoch}, Logloss: {val_logloss:.4f}, AUC: {val_auc:.4f}, \\\n",
    "                    Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "        # MRR\n",
    "        if log_train_mrr:\n",
    "            t1 = time.time()\n",
    "            df = pd.DataFrame({\n",
    "                'val_candidate': val_candidate_list,\n",
    "                'val_index': val_index_list,\n",
    "                'val_origin_label': val_origin_label_list,\n",
    "                'val_y_pred': val_y_pred_list\n",
    "            })\n",
    "            df_sorted = df.sort_values(['val_index', 'val_y_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "            df_grouped = df_sorted.groupby('val_index').apply(get_val_label_index)\n",
    "            val_mrr = np.mean(df_grouped['label_rrank'].values)\n",
    "            t2 = time.time()\n",
    "            logger.info(f\"Val Epoch {epoch}, MRR: {train_mrr:.4f}, Time: {t2-t1:.4f}\")\n",
    "        else:\n",
    "            val_mrr = 0\n",
    "        val_loss = np.mean(val_loss_list)\n",
    "        \n",
    "        mrr_all.append(val_mrr)\n",
    "        auc_all.append(val_auc)\n",
    "        lr_scheduler.step(val_auc)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if log_train_mrr:\n",
    "            logger.info('Epoch {}/{} Complete: Train Loss {:.6f}, Train MRR {:.6f}, Val Loss {:.6f}, Val MRR {:.6f}, lr {}'.format(\n",
    "                epoch, epochs, train_loss, train_mrr, val_loss, val_mrr, lr))\n",
    "        else:\n",
    "            logger.info('Epoch {}/{} Complete: Train Loss {:.6f}, Val Loss {:.6f}, lr {}'.format(epoch, epochs, train_loss, val_loss, lr))\n",
    "        if val_auc > max_val_auc:\n",
    "            min_val_loss = val_loss\n",
    "            max_val_auc = val_auc\n",
    "            best_epoch = epoch\n",
    "            save_path = '{}/{}_{}.pt'.format(output_dir, exp_id, model_name)\n",
    "            logger.info('Save model to {}'.format(save_path))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if lr < early_stop_lr:\n",
    "            logger.info('early stop')\n",
    "            break\n",
    "\n",
    "    # load best epoch\n",
    "    assert best_epoch == np.argmax(auc_all)\n",
    "    logger.info('best_epoch {}, AUC = {}'.format(best_epoch, max_val_auc))\n",
    "    load_path = '{}/{}_{}.pt'.format(output_dir, exp_id, model_name)\n",
    "    logger.info('Load model from {}'.format(load_path))\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    save_path = '{}/{}_{}_{}.pt'.format(output_dir, exp_id, model_name, best_epoch)\n",
    "    logger.info('Save model to {}'.format(save_path))\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 07:00:44,001 - INFO - Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test model 55974:   0%|          | 0/3096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 07:00:44,061 - INFO - Test Epoch 0, MRR: 0.0752, Time: 0.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 开始评估\n",
    "logger.info('Testing...')\n",
    "test_candidate_list = []\n",
    "test_y_pred_list = []\n",
    "test_index_list = []\n",
    "test_label_list = []\n",
    "test_origin_label_list = []\n",
    "model.eval()\n",
    "\n",
    "for batch_prev_items, batch_locale, batch_len, batch_mask, batch_candidate, \\\n",
    "    batch_label, batch_origin_label, batch_index, batch_seq_cat, batch_seq_num \\\n",
    "        in tqdm(test_loader, desc='test model {}'.format(exp_id), total=len(test_loader)):\n",
    "    # y_pred (B, 1), batch_candidate / batch_origin_label / batch_label / batch_index (B, )\n",
    "    y_pred, loss = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \n",
    "                                batch_candidate_set=batch_candidate, batch_len=batch_len, \n",
    "                                batch_label=batch_label.unsqueeze(-1), batch_mask=batch_mask,\n",
    "                                batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)\n",
    "    with torch.no_grad():\n",
    "        test_candidate_list += batch_candidate.tolist()\n",
    "        test_y_pred_list += y_pred.squeeze(-1).tolist()\n",
    "        test_index_list += batch_index.tolist()\n",
    "        test_label_list += batch_label.tolist()\n",
    "        test_origin_label_list += batch_origin_label.tolist()\n",
    "        break\n",
    "\n",
    "# # binary-metric\n",
    "# test_logloss = metrics.log_loss(test_label_list, test_y_pred_list)\n",
    "# test_auc = metrics.roc_auc_score(test_label_list, test_y_pred_list)\n",
    "# test_precision = metrics.precision_score(test_label_list, [1 if i >= 0.5 else 0 for i in test_y_pred_list])\n",
    "# test_recall = metrics.recall_score(test_label_list, [1 if i >= 0.5 else 0 for i in test_y_pred_list])\n",
    "# test_f1 = metrics.f1_score(test_label_list, [1 if i >= 0.5 else 0 for i in test_y_pred_list])\n",
    "# logger.info(f\"Val Epoch {epoch}, Logloss: {test_logloss:.4f}, AUC: {test_auc:.4f}, \\\n",
    "#             Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}\")\n",
    "# MRR\n",
    "t1 = time.time()\n",
    "df = pd.DataFrame({\n",
    "    'test_candidate': test_candidate_list,\n",
    "    'test_index': test_index_list,\n",
    "    'test_origin_label': test_origin_label_list,\n",
    "    'test_y_pred': test_y_pred_list\n",
    "})\n",
    "df_sorted = df.sort_values(['test_index', 'test_y_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "df_grouped = df_sorted.groupby('test_index').apply(get_test_label_index)\n",
    "df_test_preds = df_sorted.groupby('test_index').apply(lambda group : group['test_candidate'].tolist())\n",
    "test_mrr = np.mean(df_grouped['label_rrank'].values)\n",
    "t2 = time.time()\n",
    "logger.info(f\"Test Epoch {epoch}, MRR: {train_mrr:.4f}, Time: {t2-t1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 102/316972 [00:00<00:52, 6055.90it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m ind, x \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(test_preds_encoded))), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(test_preds_encoded)):\n\u001b[1;32m      3\u001b[0m     x \u001b[39m=\u001b[39m df_test_preds\u001b[39m.\u001b[39miloc[ind] \u001b[39m+\u001b[39m test_preds_encoded[ind][len_candidate_set:]\n\u001b[0;32m----> 4\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m==\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      5\u001b[0m     x_unencoded \u001b[39m=\u001b[39m [id2product[id_] \u001b[39mfor\u001b[39;00m id_ \u001b[39min\u001b[39;00m x]\n\u001b[1;32m      6\u001b[0m     test_res_unencoded\u001b[39m.\u001b[39mappend(x_unencoded)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.info('Decoding the results...')\n",
    "test_res_unencoded = []\n",
    "for ind, x in tqdm(enumerate(range(len(test_preds_encoded))), total=len(test_preds_encoded)):\n",
    "    x = df_test_preds.iloc[ind] + test_preds_encoded[ind][len_candidate_set:]\n",
    "    assert len(x) == 100\n",
    "    x_unencoded = [id2product[id_] for id_ in x]\n",
    "    test_res_unencoded.append(x_unencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Saving...')\n",
    "df_test['next_item_prediction'] = test_res_unencoded\n",
    "logger.info(df_test['next_item_prediction'].apply(len).describe())\n",
    "logger.info('Save Fold {} res to {}'.format(Fold, 'output/{}_{}_{}_{}.parquet'.format(seed, exp_id, model_name, best_epoch)))\n",
    "df_test[['locale', 'next_item_prediction']].to_parquet('output/{}_{}_{}_{}.parquet'.format(seed, exp_id, model_name, best_epoch), engine='pyarrow')\n",
    "logger.info('Save Fold {} score to {}, shape = {}'.format(Fold, 'output/{}_{}_{}_{}.npy'.format(seed, exp_id, model_name, best_epoch), test_scores.shape))\n",
    "np.save('output/scores_{}_{}_{}_{}_Fold{}.npy'.format(seed, exp_id, model_name, best_epoch, Fold), test_scores)\n",
    "np.save('output/preds_{}_{}_{}_{}_Fold{}.npy'.format(seed, exp_id, model_name, best_epoch, Fold), test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libcityng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
