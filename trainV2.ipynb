{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "# import re\n",
    "# import math\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "# from xgboost import XGBRegressor, XGBClassifier\n",
    "# from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import catboost as cab\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "# from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
    "# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "# from sklearn import metrics\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import json \n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# import sentence_transformers \n",
    "# from sklearn.preprocessing import KBinsDiscretizer\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\n",
    "from data import NNDataset, NNDatasetV2\n",
    "from model import MatchModel, BaseModel, MatchModelV2\n",
    "\n",
    "\"\"\"\n",
    "相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\n",
    "SeqFeatureEmbedding现在使用2层全连接，可以换多层！\n",
    "\"\"\"\n",
    "\n",
    "emb_dim = 16\n",
    "dense_bins = 10\n",
    "hid_dim = 256\n",
    "dropout = 0.3\n",
    "layers = 4\n",
    "bidirectional = False\n",
    "seq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "len_candidate_set = 100\n",
    "Fold = 0\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "train = True\n",
    "load_init = False\n",
    "load_epoch = 28\n",
    "load_exp_id = 11088\n",
    "\n",
    "# TODO: 调整batch-size， 调整hidden-size\n",
    "# TODO: 跑Fold 4\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "early_stop_lr = 1e-6\n",
    "lr_patience = 5\n",
    "lr_decay_ratio = 0.1\n",
    "clip = 5\n",
    "log_every = 100\n",
    "early_stop = True\n",
    "patience = 10\n",
    "kfold = 5\n",
    "attn_match = True \n",
    "\n",
    "w2v_window = 3\n",
    "w2v_min_count = 1\n",
    "w2v_epochs = 500\n",
    "w2v_vector_size = 128\n",
    "\n",
    "seed = 2023\n",
    "set_random_seed(seed)\n",
    "\n",
    "model_name = 'MatchModelV2withATTMatchFold{}'.format(Fold)\n",
    "loc2id = {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}\n",
    "\n",
    "config = locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-15 03:28:43,691 - INFO - Log directory: ./log\n",
      "2023-05-15 03:28:43,694 - INFO - Exp_id 61323\n",
      "2023-05-15 03:28:43,695 - INFO - {'__name__': '__main__', '__doc__': '\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\n\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\n\\nfrom utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\\nfrom data import NNDataset, NNDatasetV2\\nfrom model import MatchModel, BaseModel, MatchModelV2\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n\"\"\"\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.3\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 100\\nFold = 0\\ndevice = torch.device(\\'cuda:2\\')\\n\\ntrain = True\\nload_init = False\\nload_epoch = 28\\nload_exp_id = 11088\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'MatchModelV2withATTMatchFold{}\\'.format(Fold)\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()', \"# 加载必要的数据\\n\\nexp_id = config.get('exp_id', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\ntop200 = pickle.load(open('data/top200.pkl', 'rb'))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\ndf_test_encoded = pd.read_csv('data/df_test_encoded.csv')\\nproducts_encoded = pd.read_csv('./data/products_encoded.csv')\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\\n\\nlogger.info('MinMaxScaler Norm products_num_feas')\\nmms = MinMaxScaler(feature_range=(0,1))\\nnum_features = ['price', 'len_title', 'len_desc']\\nproducts_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded[fe] = products_encoded[fe].astype('float32')\\n\\nlogger.info('Load Hand-made Seq Features')\\ndf_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\\ndf_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all.csv')\\nlogger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\\nlogger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\\nlogger.info('seqs_num_feas: {}'.format(seqs_num_feas))\\n\\nlogger.info('MinMaxScaler Norm seqs_num_feas')\\nmms = MinMaxScaler(feature_range=(0,1))\\ndf_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\ndf_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\\n    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\\n\\ndf_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\\ndf_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\\nlogger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\\nlogger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\\nlogger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\\nlogger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\\n\\nid_count = products_encoded.shape[0]\\n\\ntrain_preds_encoded = pickle.load(open('./data/train_preds_all_encoded.pkl', 'rb'))  # (len_train, 100)\\ntest_preds_encoded = pickle.load(open('./data/test_preds_all_encoded.pkl', 'rb'))  # (len_test, 100)\\ntest_preds = pickle.load(open('./data/test_preds_all.pkl', 'rb'))\\nlogger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\nlogger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\nlogger.info('test_preds: {}'.format(len(test_preds)))\\n\\nlogger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded['recall'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded['recall'] = cut_test_preds_encoded\\n\\nlogger.info('Eval the prev_items')\\ndf_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\\ndf_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\\n\\ndf_test = pd.read_csv('data/sessions_test_task1.csv')\\nlogger.info('df_test: {}'.format(df_test.shape))\\n\\ntmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\\ntmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\\n\\ndata_feature = {}\\ndata_feature['len_encode_brand'] = products_encoded['encode_brand'].nunique()\\ndata_feature['len_encode_color'] = products_encoded['encode_color'].nunique()\\ndata_feature['len_encode_size'] = products_encoded['encode_size'].nunique()\\ndata_feature['len_encode_model'] = products_encoded['encode_model'].nunique()\\ndata_feature['len_encode_material'] = products_encoded['encode_material'].nunique()\\ndata_feature['len_encode_author'] = products_encoded['encode_author'].nunique()\\ndata_feature['len_locale'] = len(loc2id)\\ndata_feature['dense_bins'] = dense_bins\\ndata_feature['id_count'] = id_count\\ndata_feature['len_features'] = products_encoded.shape[1] - 1\\ndata_feature['len_emb_features'] = 3\\ndata_feature['len_candidate_set'] = len_candidate_set\\ndata_feature['w2v_vector_size'] = w2v_vector_size\\ndata_feature['sentence_vector_size'] = 384\\ndata_feature['len_seqs_cat_feas'] = len(seqs_cat_feas)\\ndata_feature['len_seqs_num_feas'] = len(seqs_num_feas)\\ndata_feature['seq_emb_factor'] = seq_emb_factor\\ndata_feature.update(tmp_nunique)\\nlogger.info('data_feature:')\\nlogger.info(data_feature)\\n\\ndel tmp\"], '_oh': {}, '_dh': ['/home/panda/private/jjw/competition/KDDCUP2023'], 'In': ['', 'import numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\n\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\n\\nfrom utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\\nfrom data import NNDataset, NNDatasetV2\\nfrom model import MatchModel, BaseModel, MatchModelV2\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n\"\"\"\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.3\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 100\\nFold = 0\\ndevice = torch.device(\\'cuda:2\\')\\n\\ntrain = True\\nload_init = False\\nload_epoch = 28\\nload_exp_id = 11088\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'MatchModelV2withATTMatchFold{}\\'.format(Fold)\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()', \"# 加载必要的数据\\n\\nexp_id = config.get('exp_id', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\ntop200 = pickle.load(open('data/top200.pkl', 'rb'))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\ndf_test_encoded = pd.read_csv('data/df_test_encoded.csv')\\nproducts_encoded = pd.read_csv('./data/products_encoded.csv')\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\\n\\nlogger.info('MinMaxScaler Norm products_num_feas')\\nmms = MinMaxScaler(feature_range=(0,1))\\nnum_features = ['price', 'len_title', 'len_desc']\\nproducts_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded[fe] = products_encoded[fe].astype('float32')\\n\\nlogger.info('Load Hand-made Seq Features')\\ndf_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\\ndf_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all.csv')\\nlogger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\\nlogger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\\nlogger.info('seqs_num_feas: {}'.format(seqs_num_feas))\\n\\nlogger.info('MinMaxScaler Norm seqs_num_feas')\\nmms = MinMaxScaler(feature_range=(0,1))\\ndf_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\ndf_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\\n    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\\n\\ndf_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\\ndf_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\\nlogger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\\nlogger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\\nlogger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\\nlogger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\\n\\nid_count = products_encoded.shape[0]\\n\\ntrain_preds_encoded = pickle.load(open('./data/train_preds_all_encoded.pkl', 'rb'))  # (len_train, 100)\\ntest_preds_encoded = pickle.load(open('./data/test_preds_all_encoded.pkl', 'rb'))  # (len_test, 100)\\ntest_preds = pickle.load(open('./data/test_preds_all.pkl', 'rb'))\\nlogger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\nlogger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\nlogger.info('test_preds: {}'.format(len(test_preds)))\\n\\nlogger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded['recall'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded['recall'] = cut_test_preds_encoded\\n\\nlogger.info('Eval the prev_items')\\ndf_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\\ndf_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\\n\\ndf_test = pd.read_csv('data/sessions_test_task1.csv')\\nlogger.info('df_test: {}'.format(df_test.shape))\\n\\ntmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\\ntmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\\n\\ndata_feature = {}\\ndata_feature['len_encode_brand'] = products_encoded['encode_brand'].nunique()\\ndata_feature['len_encode_color'] = products_encoded['encode_color'].nunique()\\ndata_feature['len_encode_size'] = products_encoded['encode_size'].nunique()\\ndata_feature['len_encode_model'] = products_encoded['encode_model'].nunique()\\ndata_feature['len_encode_material'] = products_encoded['encode_material'].nunique()\\ndata_feature['len_encode_author'] = products_encoded['encode_author'].nunique()\\ndata_feature['len_locale'] = len(loc2id)\\ndata_feature['dense_bins'] = dense_bins\\ndata_feature['id_count'] = id_count\\ndata_feature['len_features'] = products_encoded.shape[1] - 1\\ndata_feature['len_emb_features'] = 3\\ndata_feature['len_candidate_set'] = len_candidate_set\\ndata_feature['w2v_vector_size'] = w2v_vector_size\\ndata_feature['sentence_vector_size'] = 384\\ndata_feature['len_seqs_cat_feas'] = len(seqs_cat_feas)\\ndata_feature['len_seqs_num_feas'] = len(seqs_num_feas)\\ndata_feature['seq_emb_factor'] = seq_emb_factor\\ndata_feature.update(tmp_nunique)\\nlogger.info('data_feature:')\\nlogger.info(data_feature)\\n\\ndel tmp\"], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f8435a00280>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f84335f3c40>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f84335f3c40>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/home/panda/private/jjw/competition/KDDCUP2023/trainV2.ipynb', '_i': 'import numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\n\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\n\\nfrom utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\\nfrom data import NNDataset, NNDatasetV2\\nfrom model import MatchModel, BaseModel, MatchModelV2\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n\"\"\"\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.3\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 100\\nFold = 0\\ndevice = torch.device(\\'cuda:2\\')\\n\\ntrain = True\\nload_init = False\\nload_epoch = 28\\nload_exp_id = 11088\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'MatchModelV2withATTMatchFold{}\\'.format(Fold)\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()', '_ii': '', '_iii': '', '_i1': 'import numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\n\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\n\\nfrom utils import set_random_seed, get_logger, ensure_dir, save_model_with_epoch, load_model_with_epoch\\nfrom data import NNDataset, NNDatasetV2\\nfrom model import MatchModel, BaseModel, MatchModelV2\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n\"\"\"\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.3\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 100\\nFold = 0\\ndevice = torch.device(\\'cuda:2\\')\\n\\ntrain = True\\nload_init = False\\nload_epoch = 28\\nload_exp_id = 11088\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'MatchModelV2withATTMatchFold{}\\'.format(Fold)\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()', 'np': <module 'numpy' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/numpy/__init__.py'>, 'pd': <module 'pandas' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/pandas/__init__.py'>, 'plt': <module 'matplotlib.pyplot' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/matplotlib/pyplot.py'>, 'warnings': <module 'warnings' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/warnings.py'>, 'json': <module 'json' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/json/__init__.py'>, 'pickle': <module 'pickle' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/pickle.py'>, 'random': <module 'random' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/random.py'>, 'tqdm': <class 'tqdm.std.tqdm'>, 'LabelEncoder': <class 'sklearn.preprocessing._label.LabelEncoder'>, 'MinMaxScaler': <class 'sklearn.preprocessing._data.MinMaxScaler'>, 'torch': <module 'torch' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/__init__.py'>, 'nn': <module 'torch.nn' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/nn/__init__.py'>, 'pack_padded_sequence': <function pack_padded_sequence at 0x7f8238c77040>, 'pad_packed_sequence': <function pad_packed_sequence at 0x7f8238c770d0>, 'random_split': <function random_split at 0x7f82388c3dc0>, 'DataLoader': <class 'torch.utils.data.dataloader.DataLoader'>, 'Dataset': <class 'torch.utils.data.dataset.Dataset'>, 'set_random_seed': <function set_random_seed at 0x7f8238789a60>, 'get_logger': <function get_logger at 0x7f8238789e50>, 'ensure_dir': <function ensure_dir at 0x7f8238789dc0>, 'save_model_with_epoch': <function save_model_with_epoch at 0x7f8238789ee0>, 'load_model_with_epoch': <function load_model_with_epoch at 0x7f8238789f70>, 'NNDataset': <class 'data.NNDataset'>, 'NNDatasetV2': <class 'data.NNDatasetV2'>, 'MatchModel': <class 'model.MatchModel'>, 'BaseModel': <class 'model.BaseModel'>, 'MatchModelV2': <class 'model.MatchModelV2'>, 'emb_dim': 16, 'dense_bins': 10, 'hid_dim': 256, 'dropout': 0.3, 'layers': 4, 'bidirectional': False, 'seq_emb_factor': 4, 'batch_size': 1024, 'epochs': 100, 'len_candidate_set': 100, 'Fold': 0, 'device': device(type='cuda', index=2), 'train': True, 'load_init': False, 'load_epoch': 28, 'load_exp_id': 11088, 'learning_rate': 0.001, 'weight_decay': 1e-05, 'early_stop_lr': 1e-06, 'lr_patience': 5, 'lr_decay_ratio': 0.1, 'clip': 5, 'log_every': 100, 'early_stop': True, 'patience': 10, 'kfold': 5, 'attn_match': True, 'w2v_window': 3, 'w2v_min_count': 1, 'w2v_epochs': 500, 'w2v_vector_size': 128, 'seed': 2023, 'model_name': 'MatchModelV2withATTMatchFold0', 'loc2id': {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}, 'config': {...}, '_i2': \"# 加载必要的数据\\n\\nexp_id = config.get('exp_id', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\ntop200 = pickle.load(open('data/top200.pkl', 'rb'))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\ndf_test_encoded = pd.read_csv('data/df_test_encoded.csv')\\nproducts_encoded = pd.read_csv('./data/products_encoded.csv')\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\\n\\nlogger.info('MinMaxScaler Norm products_num_feas')\\nmms = MinMaxScaler(feature_range=(0,1))\\nnum_features = ['price', 'len_title', 'len_desc']\\nproducts_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded[fe] = products_encoded[fe].astype('float32')\\n\\nlogger.info('Load Hand-made Seq Features')\\ndf_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\\ndf_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all.csv')\\nlogger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\\nlogger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\\nlogger.info('seqs_num_feas: {}'.format(seqs_num_feas))\\n\\nlogger.info('MinMaxScaler Norm seqs_num_feas')\\nmms = MinMaxScaler(feature_range=(0,1))\\ndf_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\ndf_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\\n    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\\n\\ndf_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\\ndf_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\\nlogger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\\nlogger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\\nlogger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\\nlogger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\\n\\nid_count = products_encoded.shape[0]\\n\\ntrain_preds_encoded = pickle.load(open('./data/train_preds_all_encoded.pkl', 'rb'))  # (len_train, 100)\\ntest_preds_encoded = pickle.load(open('./data/test_preds_all_encoded.pkl', 'rb'))  # (len_test, 100)\\ntest_preds = pickle.load(open('./data/test_preds_all.pkl', 'rb'))\\nlogger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\nlogger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\nlogger.info('test_preds: {}'.format(len(test_preds)))\\n\\nlogger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded['recall'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded['recall'] = cut_test_preds_encoded\\n\\nlogger.info('Eval the prev_items')\\ndf_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\\ndf_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\\n\\ndf_test = pd.read_csv('data/sessions_test_task1.csv')\\nlogger.info('df_test: {}'.format(df_test.shape))\\n\\ntmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\\ntmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\\n\\ndata_feature = {}\\ndata_feature['len_encode_brand'] = products_encoded['encode_brand'].nunique()\\ndata_feature['len_encode_color'] = products_encoded['encode_color'].nunique()\\ndata_feature['len_encode_size'] = products_encoded['encode_size'].nunique()\\ndata_feature['len_encode_model'] = products_encoded['encode_model'].nunique()\\ndata_feature['len_encode_material'] = products_encoded['encode_material'].nunique()\\ndata_feature['len_encode_author'] = products_encoded['encode_author'].nunique()\\ndata_feature['len_locale'] = len(loc2id)\\ndata_feature['dense_bins'] = dense_bins\\ndata_feature['id_count'] = id_count\\ndata_feature['len_features'] = products_encoded.shape[1] - 1\\ndata_feature['len_emb_features'] = 3\\ndata_feature['len_candidate_set'] = len_candidate_set\\ndata_feature['w2v_vector_size'] = w2v_vector_size\\ndata_feature['sentence_vector_size'] = 384\\ndata_feature['len_seqs_cat_feas'] = len(seqs_cat_feas)\\ndata_feature['len_seqs_num_feas'] = len(seqs_num_feas)\\ndata_feature['seq_emb_factor'] = seq_emb_factor\\ndata_feature.update(tmp_nunique)\\nlogger.info('data_feature:')\\nlogger.info(data_feature)\\n\\ndel tmp\", 'exp_id': 61323, 'logger': <RootLogger root (INFO)>}\n",
      "2023-05-15 03:28:43,699 - INFO - read data\n",
      "2023-05-15 03:29:08,912 - INFO - titles_embedding: (1410675, 384)\n",
      "2023-05-15 03:29:08,918 - INFO - descs_embedding: (1410675, 384)\n",
      "2023-05-15 03:29:16,415 - INFO - product2id: 1410675\n",
      "2023-05-15 03:29:16,417 - INFO - id2product: 1410675\n",
      "2023-05-15 03:29:20,999 - INFO - word2vec_embedding: (1410675, 128)\n",
      "2023-05-15 03:29:32,881 - INFO - df_train_encoded: (3606249, 4)\n",
      "2023-05-15 03:29:32,883 - INFO - df_test_encoded: (316971, 4)\n",
      "2023-05-15 03:29:32,884 - INFO - products_encoded: (1410675, 14)\n",
      "2023-05-15 03:29:32,885 - INFO - MinMaxScaler Norm products_num_feas\n",
      "2023-05-15 03:29:34,923 - INFO - Load Hand-made Seq Features\n",
      "2023-05-15 03:30:07,024 - INFO - df_train_seqs_feas_all: (3606249, 29)\n",
      "2023-05-15 03:30:07,028 - INFO - df_test_seqs_feas_all: (316971, 29)\n",
      "2023-05-15 03:30:07,035 - INFO - seqs_cat_feas: ['idNUNIQUE', 'idCOUNT', 'brandNUNIQUE', 'brandCOUNT', 'colorNUNIQUE', 'colorCOUNT', 'sizeNUNIQUE', 'sizeCOUNT', 'modelNUNIQUE', 'modelCOUNT', 'materialNUNIQUE', 'materialCOUNT', 'authorNUNIQUE', 'authorCOUNT']\n",
      "2023-05-15 03:30:07,037 - INFO - seqs_num_feas: ['priceMEAN', 'priceSTD', 'priceMIN', 'priceMAX', 'priceSUM', 'len_titleMEAN', 'len_titleSTD', 'len_titleMIN', 'len_titleMAX', 'len_titleSUM', 'len_descMEAN', 'len_descSTD', 'len_descMIN', 'len_descMAX', 'len_descSUM']\n",
      "2023-05-15 03:30:07,038 - INFO - MinMaxScaler Norm seqs_num_feas\n",
      "2023-05-15 03:30:30,939 - INFO - df_train_seqs_cat_feas: (3606249, 14)\n",
      "2023-05-15 03:30:30,945 - INFO - df_train_seqs_num_feas: (3606249, 15)\n",
      "2023-05-15 03:30:30,947 - INFO - df_test_seqs_cat_feas: (316971, 14)\n",
      "2023-05-15 03:30:30,954 - INFO - df_test_seqs_num_feas: (316971, 15)\n",
      "2023-05-15 03:33:19,413 - INFO - train_preds_encoded: 3606249\n",
      "2023-05-15 03:33:19,416 - INFO - test_preds_encoded: 316971\n",
      "2023-05-15 03:33:19,417 - INFO - test_preds: 316971\n",
      "2023-05-15 03:33:19,418 - INFO - Cutting the candidate_set to 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3606249/3606249 [01:17<00:00, 46743.71it/s] \n",
      "100%|██████████| 316971/316971 [00:03<00:00, 90874.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-15 03:34:41,414 - INFO - Eval the prev_items\n",
      "2023-05-15 03:37:57,542 - INFO - df_test: (316971, 2)\n",
      "2023-05-15 03:37:59,096 - INFO - data_feature:\n",
      "2023-05-15 03:37:59,098 - INFO - {'len_encode_brand': 177190, 'len_encode_color': 203261, 'len_encode_size': 218061, 'len_encode_model': 524102, 'len_encode_material': 45569, 'len_encode_author': 30836, 'len_locale': 6, 'dense_bins': 10, 'id_count': 1410675, 'len_features': 13, 'len_emb_features': 3, 'len_candidate_set': 100, 'w2v_vector_size': 128, 'sentence_vector_size': 384, 'len_seqs_cat_feas': 14, 'len_seqs_num_feas': 15, 'seq_emb_factor': 4, 'idNUNIQUE': 133, 'idCOUNT': 475, 'brandNUNIQUE': 39, 'brandCOUNT': 475, 'colorNUNIQUE': 53, 'colorCOUNT': 269, 'sizeNUNIQUE': 115, 'sizeCOUNT': 475, 'modelNUNIQUE': 59, 'modelCOUNT': 475, 'materialNUNIQUE': 25, 'materialCOUNT': 475, 'authorNUNIQUE': 65, 'authorCOUNT': 190}\n"
     ]
    }
   ],
   "source": [
    "# 加载必要的数据\n",
    "\n",
    "exp_id = config.get('exp_id', None)\n",
    "if exp_id is None:\n",
    "    exp_id = int(random.SystemRandom().random() * 100000)\n",
    "    config['exp_id'] = exp_id\n",
    "\n",
    "logger = get_logger(config)\n",
    "logger.info('Exp_id {}'.format(exp_id))\n",
    "logger.info(config)\n",
    "\n",
    "logger.info('read data')\n",
    "\n",
    "titles_embedding = np.load('./data/titles_embedding.npy')\n",
    "descs_embedding = np.load('./data/descs_embedding.npy')\n",
    "logger.info('titles_embedding: {}'.format(titles_embedding.shape))\n",
    "logger.info('descs_embedding: {}'.format(descs_embedding.shape))\n",
    "\n",
    "product2id = json.load(open('data/product2id.json', 'r'))\n",
    "id2product = json.load(open('data/id2product.json', 'r'))\n",
    "id2product = {int(k): v for k, v in id2product.items()}\n",
    "logger.info('product2id: {}'.format(len(product2id)))\n",
    "logger.info('id2product: {}'.format(len(id2product)))\n",
    "\n",
    "word2vec_embedding = np.load('./data/word2vec_embedding.npy')\n",
    "logger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\n",
    "\n",
    "top200 = pickle.load(open('data/top200.pkl', 'rb'))\n",
    "\n",
    "df_train_encoded = pd.read_csv('data/df_train_encoded.csv')\n",
    "df_test_encoded = pd.read_csv('data/df_test_encoded.csv')\n",
    "products_encoded = pd.read_csv('./data/products_encoded.csv')\n",
    "logger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\n",
    "logger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\n",
    "logger.info('products_encoded: {}'.format(products_encoded.shape))\n",
    "\n",
    "logger.info('MinMaxScaler Norm products_num_feas')\n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "num_features = ['price', 'len_title', 'len_desc']\n",
    "products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\n",
    "for fe in num_features:\n",
    "    products_encoded[fe] = products_encoded[fe].astype('float32')\n",
    "\n",
    "logger.info('Load Hand-made Seq Features')\n",
    "df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\n",
    "df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all.csv')\n",
    "logger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\n",
    "logger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\n",
    "seqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f]\n",
    "seqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\n",
    "logger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\n",
    "logger.info('seqs_num_feas: {}'.format(seqs_num_feas))\n",
    "\n",
    "logger.info('MinMaxScaler Norm seqs_num_feas')\n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\n",
    "df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\n",
    "for fe in seqs_num_feas:\n",
    "    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\n",
    "    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\n",
    "\n",
    "df_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\n",
    "df_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\n",
    "df_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\n",
    "df_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\n",
    "logger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\n",
    "logger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\n",
    "logger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\n",
    "logger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\n",
    "\n",
    "id_count = products_encoded.shape[0]\n",
    "\n",
    "train_preds_encoded = pickle.load(open('./data/train_preds_all_encoded.pkl', 'rb'))  # (len_train, 100)\n",
    "test_preds_encoded = pickle.load(open('./data/test_preds_all_encoded.pkl', 'rb'))  # (len_test, 100)\n",
    "test_preds = pickle.load(open('./data/test_preds_all.pkl', 'rb'))\n",
    "logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\n",
    "logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\n",
    "logger.info('test_preds: {}'.format(len(test_preds)))\n",
    "\n",
    "logger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\n",
    "cut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\n",
    "df_train_encoded['recall'] = cut_train_preds_encoded\n",
    "cut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\n",
    "df_test_encoded['recall'] = cut_test_preds_encoded\n",
    "\n",
    "logger.info('Eval the prev_items')\n",
    "df_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\n",
    "df_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\n",
    "\n",
    "df_test = pd.read_csv('data/sessions_test_task1.csv')\n",
    "logger.info('df_test: {}'.format(df_test.shape))\n",
    "\n",
    "tmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\n",
    "tmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\n",
    "\n",
    "data_feature = {}\n",
    "data_feature['len_encode_brand'] = products_encoded['encode_brand'].nunique()\n",
    "data_feature['len_encode_color'] = products_encoded['encode_color'].nunique()\n",
    "data_feature['len_encode_size'] = products_encoded['encode_size'].nunique()\n",
    "data_feature['len_encode_model'] = products_encoded['encode_model'].nunique()\n",
    "data_feature['len_encode_material'] = products_encoded['encode_material'].nunique()\n",
    "data_feature['len_encode_author'] = products_encoded['encode_author'].nunique()\n",
    "data_feature['len_locale'] = len(loc2id)\n",
    "data_feature['dense_bins'] = dense_bins\n",
    "data_feature['id_count'] = id_count\n",
    "data_feature['len_features'] = products_encoded.shape[1] - 1\n",
    "data_feature['len_emb_features'] = 3\n",
    "data_feature['len_candidate_set'] = len_candidate_set\n",
    "data_feature['w2v_vector_size'] = w2v_vector_size\n",
    "data_feature['sentence_vector_size'] = 384\n",
    "data_feature['len_seqs_cat_feas'] = len(seqs_cat_feas)\n",
    "data_feature['len_seqs_num_feas'] = len(seqs_num_feas)\n",
    "data_feature['seq_emb_factor'] = seq_emb_factor\n",
    "data_feature.update(tmp_nunique)\n",
    "logger.info('data_feature:')\n",
    "logger.info(data_feature)\n",
    "\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-15 04:17:29,954 - INFO - Load Hand-made Seq Features\n",
      "2023-05-15 04:18:01,584 - INFO - df_train_seqs_feas_all: (3606249, 29)\n",
      "2023-05-15 04:18:01,586 - INFO - df_test_seqs_feas_all: (316971, 29)\n",
      "2023-05-15 04:18:01,587 - INFO - seqs_cat_feas: ['idNUNIQUE', 'idCOUNT', 'brandNUNIQUE', 'brandCOUNT', 'colorNUNIQUE', 'colorCOUNT', 'sizeNUNIQUE', 'sizeCOUNT', 'modelNUNIQUE', 'modelCOUNT', 'materialNUNIQUE', 'materialCOUNT', 'authorNUNIQUE', 'authorCOUNT']\n",
      "2023-05-15 04:18:01,588 - INFO - seqs_num_feas: ['priceMEAN', 'priceSTD', 'priceMIN', 'priceMAX', 'priceSUM', 'len_titleMEAN', 'len_titleSTD', 'len_titleMIN', 'len_titleMAX', 'len_titleSUM', 'len_descMEAN', 'len_descSTD', 'len_descMIN', 'len_descMAX', 'len_descSUM']\n",
      "2023-05-15 04:18:01,589 - INFO - MinMaxScaler Norm seqs_num_feas\n",
      "2023-05-15 04:18:25,931 - INFO - df_train_seqs_cat_feas: (3606249, 14)\n",
      "2023-05-15 04:18:25,934 - INFO - df_train_seqs_num_feas: (3606249, 15)\n",
      "2023-05-15 04:18:25,935 - INFO - df_test_seqs_cat_feas: (316971, 14)\n",
      "2023-05-15 04:18:25,936 - INFO - df_test_seqs_num_feas: (316971, 15)\n"
     ]
    }
   ],
   "source": [
    "logger.info('Load Hand-made Seq Features')\n",
    "df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\n",
    "df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all.csv')\n",
    "logger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\n",
    "logger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\n",
    "seqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f]\n",
    "seqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\n",
    "logger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\n",
    "logger.info('seqs_num_feas: {}'.format(seqs_num_feas))\n",
    "\n",
    "logger.info('MinMaxScaler Norm seqs_num_feas')\n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\n",
    "df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\n",
    "for fe in seqs_num_feas:\n",
    "    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\n",
    "    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\n",
    "\n",
    "df_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\n",
    "df_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\n",
    "df_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\n",
    "df_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\n",
    "logger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\n",
    "logger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\n",
    "logger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\n",
    "logger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "priceMEAN         True\n",
       "priceSTD          True\n",
       "priceMIN          True\n",
       "priceMAX          True\n",
       "priceSUM         False\n",
       "len_titleMEAN     True\n",
       "len_titleSTD      True\n",
       "len_titleMIN      True\n",
       "len_titleMAX      True\n",
       "len_titleSUM     False\n",
       "len_descMEAN      True\n",
       "len_descSTD       True\n",
       "len_descMIN       True\n",
       "len_descMAX       True\n",
       "len_descSUM      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_seqs_num_feas.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_nan = df_test_seqs_num_feas[df_test_seqs_num_feas.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idNUNIQUE</th>\n",
       "      <th>idCOUNT</th>\n",
       "      <th>brandNUNIQUE</th>\n",
       "      <th>brandCOUNT</th>\n",
       "      <th>colorNUNIQUE</th>\n",
       "      <th>colorCOUNT</th>\n",
       "      <th>sizeNUNIQUE</th>\n",
       "      <th>sizeCOUNT</th>\n",
       "      <th>modelNUNIQUE</th>\n",
       "      <th>modelCOUNT</th>\n",
       "      <th>materialNUNIQUE</th>\n",
       "      <th>materialCOUNT</th>\n",
       "      <th>authorNUNIQUE</th>\n",
       "      <th>authorCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104606</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104701</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104978</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105063</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105116</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316942</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316951</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316955</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26148 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idNUNIQUE  idCOUNT  brandNUNIQUE  brandCOUNT  colorNUNIQUE  \\\n",
       "104606          3        3             0           0             0   \n",
       "104701          2        4             0           0             0   \n",
       "104978          2        2             1           1             1   \n",
       "105063          2        2             0           0             0   \n",
       "105116          2        3             0           0             0   \n",
       "...           ...      ...           ...         ...           ...   \n",
       "316942          2        2             1           1             1   \n",
       "316951          3        3             0           0             0   \n",
       "316955          2        3             1           1             1   \n",
       "316968          4        6             0           0             0   \n",
       "316969          2        2             1           1             1   \n",
       "\n",
       "        colorCOUNT  sizeNUNIQUE  sizeCOUNT  modelNUNIQUE  modelCOUNT  \\\n",
       "104606           0            0          0             0           0   \n",
       "104701           0            0          0             0           0   \n",
       "104978           1            0          0             0           0   \n",
       "105063           0            0          0             0           0   \n",
       "105116           0            0          0             0           0   \n",
       "...            ...          ...        ...           ...         ...   \n",
       "316942           1            0          0             1           1   \n",
       "316951           0            0          0             0           0   \n",
       "316955           1            0          0             0           0   \n",
       "316968           0            0          0             0           0   \n",
       "316969           1            0          0             1           1   \n",
       "\n",
       "        materialNUNIQUE  materialCOUNT  authorNUNIQUE  authorCOUNT  \n",
       "104606                0              0              0            0  \n",
       "104701                0              0              0            0  \n",
       "104978                1              1              0            0  \n",
       "105063                0              0              0            0  \n",
       "105116                0              0              0            0  \n",
       "...                 ...            ...            ...          ...  \n",
       "316942                1              1              0            0  \n",
       "316951                0              0              0            0  \n",
       "316955                0              0              0            0  \n",
       "316968                0              0              0            0  \n",
       "316969                1              1              0            0  \n",
       "\n",
       "[26148 rows x 14 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_seqs_cat_feas[df_test_seqs_num_feas.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceMEAN</th>\n",
       "      <th>priceSTD</th>\n",
       "      <th>priceMIN</th>\n",
       "      <th>priceMAX</th>\n",
       "      <th>priceSUM</th>\n",
       "      <th>len_titleMEAN</th>\n",
       "      <th>len_titleSTD</th>\n",
       "      <th>len_titleMIN</th>\n",
       "      <th>len_titleMAX</th>\n",
       "      <th>len_titleSUM</th>\n",
       "      <th>len_descMEAN</th>\n",
       "      <th>len_descSTD</th>\n",
       "      <th>len_descMIN</th>\n",
       "      <th>len_descMAX</th>\n",
       "      <th>len_descSUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104606</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104701</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104978</th>\n",
       "      <td>7.450000e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.450000e-05</td>\n",
       "      <td>7.450000e-05</td>\n",
       "      <td>5.730769e-06</td>\n",
       "      <td>0.400411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401639</td>\n",
       "      <td>0.390782</td>\n",
       "      <td>0.019957</td>\n",
       "      <td>0.190381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.190381</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.012773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105063</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105116</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316942</th>\n",
       "      <td>2.247500e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.247500e-07</td>\n",
       "      <td>2.247500e-07</td>\n",
       "      <td>1.728846e-08</td>\n",
       "      <td>0.396304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.397541</td>\n",
       "      <td>0.386774</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>0.188377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.188377</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.012639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316951</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316955</th>\n",
       "      <td>4.212500e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.212500e-07</td>\n",
       "      <td>4.212500e-07</td>\n",
       "      <td>3.240385e-08</td>\n",
       "      <td>0.318275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.319672</td>\n",
       "      <td>0.310621</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>0.201403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201403</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.013513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>1.747500e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.747500e-07</td>\n",
       "      <td>1.747500e-07</td>\n",
       "      <td>1.344231e-08</td>\n",
       "      <td>0.349076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.350410</td>\n",
       "      <td>0.340681</td>\n",
       "      <td>0.017412</td>\n",
       "      <td>0.233467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.233467</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.015664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26148 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           priceMEAN  priceSTD      priceMIN      priceMAX      priceSUM  \\\n",
       "104606           NaN       NaN           NaN           NaN  0.000000e+00   \n",
       "104701           NaN       NaN           NaN           NaN  0.000000e+00   \n",
       "104978  7.450000e-05       NaN  7.450000e-05  7.450000e-05  5.730769e-06   \n",
       "105063           NaN       NaN           NaN           NaN  0.000000e+00   \n",
       "105116           NaN       NaN           NaN           NaN  0.000000e+00   \n",
       "...              ...       ...           ...           ...           ...   \n",
       "316942  2.247500e-07       NaN  2.247500e-07  2.247500e-07  1.728846e-08   \n",
       "316951           NaN       NaN           NaN           NaN  0.000000e+00   \n",
       "316955  4.212500e-07       NaN  4.212500e-07  4.212500e-07  3.240385e-08   \n",
       "316968           NaN       NaN           NaN           NaN  0.000000e+00   \n",
       "316969  1.747500e-07       NaN  1.747500e-07  1.747500e-07  1.344231e-08   \n",
       "\n",
       "        len_titleMEAN  len_titleSTD  len_titleMIN  len_titleMAX  len_titleSUM  \\\n",
       "104606            NaN           NaN           NaN           NaN      0.000000   \n",
       "104701            NaN           NaN           NaN           NaN      0.000000   \n",
       "104978       0.400411           NaN      0.401639      0.390782      0.019957   \n",
       "105063            NaN           NaN           NaN           NaN      0.000000   \n",
       "105116            NaN           NaN           NaN           NaN      0.000000   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "316942       0.396304           NaN      0.397541      0.386774      0.019754   \n",
       "316951            NaN           NaN           NaN           NaN      0.000000   \n",
       "316955       0.318275           NaN      0.319672      0.310621      0.015884   \n",
       "316968            NaN           NaN           NaN           NaN      0.000000   \n",
       "316969       0.349076           NaN      0.350410      0.340681      0.017412   \n",
       "\n",
       "        len_descMEAN  len_descSTD  len_descMIN  len_descMAX  len_descSUM  \n",
       "104606           NaN          NaN          NaN          NaN     0.000000  \n",
       "104701           NaN          NaN          NaN          NaN     0.000000  \n",
       "104978      0.190381          NaN     0.190381        0.190     0.012773  \n",
       "105063           NaN          NaN          NaN          NaN     0.000000  \n",
       "105116           NaN          NaN          NaN          NaN     0.000000  \n",
       "...              ...          ...          ...          ...          ...  \n",
       "316942      0.188377          NaN     0.188377        0.188     0.012639  \n",
       "316951           NaN          NaN          NaN          NaN     0.000000  \n",
       "316955      0.201403          NaN     0.201403        0.201     0.013513  \n",
       "316968           NaN          NaN          NaN          NaN     0.000000  \n",
       "316969      0.233467          NaN     0.233467        0.233     0.015664  \n",
       "\n",
       "[26148 rows x 15 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceMEAN</th>\n",
       "      <th>priceSTD</th>\n",
       "      <th>priceMIN</th>\n",
       "      <th>priceMAX</th>\n",
       "      <th>priceSUM</th>\n",
       "      <th>len_titleMEAN</th>\n",
       "      <th>len_titleSTD</th>\n",
       "      <th>len_titleMIN</th>\n",
       "      <th>len_titleMAX</th>\n",
       "      <th>len_titleSUM</th>\n",
       "      <th>len_descMEAN</th>\n",
       "      <th>len_descSTD</th>\n",
       "      <th>len_descMIN</th>\n",
       "      <th>len_descMAX</th>\n",
       "      <th>len_descSUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606244</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606245</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606246</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606247</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606248</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3606249 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         priceMEAN  priceSTD  priceMIN  priceMAX  priceSUM  len_titleMEAN  \\\n",
       "0              NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "1              NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "2              NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "3              NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "4              NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "...            ...       ...       ...       ...       ...            ...   \n",
       "3606244        NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "3606245        NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "3606246        NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "3606247        NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "3606248        NaN       NaN       NaN       NaN       NaN            NaN   \n",
       "\n",
       "         len_titleSTD  len_titleMIN  len_titleMAX  len_titleSUM  len_descMEAN  \\\n",
       "0                 NaN           NaN           NaN           NaN           NaN   \n",
       "1                 NaN           NaN           NaN           NaN           NaN   \n",
       "2                 NaN           NaN           NaN           NaN           NaN   \n",
       "3                 NaN           NaN           NaN           NaN           NaN   \n",
       "4                 NaN           NaN           NaN           NaN           NaN   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "3606244           NaN           NaN           NaN           NaN           NaN   \n",
       "3606245           NaN           NaN           NaN           NaN           NaN   \n",
       "3606246           NaN           NaN           NaN           NaN           NaN   \n",
       "3606247           NaN           NaN           NaN           NaN           NaN   \n",
       "3606248           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "         len_descSTD  len_descMIN  len_descMAX  len_descSUM  \n",
       "0                NaN          NaN          NaN          NaN  \n",
       "1                NaN          NaN          NaN          NaN  \n",
       "2                NaN          NaN          NaN          NaN  \n",
       "3                NaN          NaN          NaN          NaN  \n",
       "4                NaN          NaN          NaN          NaN  \n",
       "...              ...          ...          ...          ...  \n",
       "3606244          NaN          NaN          NaN          NaN  \n",
       "3606245          NaN          NaN          NaN          NaN  \n",
       "3606246          NaN          NaN          NaN          NaN  \n",
       "3606247          NaN          NaN          NaN          NaN  \n",
       "3606248          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[3606249 rows x 15 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_seqs_num_feas[df_train_seqs_num_feas.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-15 04:22:54,317 - INFO - create model\n",
      "2023-05-15 04:23:32,272 - INFO - MatchModelV2(\n",
      "  (product_emb): ProductEmbedding(\n",
      "    (product_fea): Product(\n",
      "      (locale_emb): Embedding(6, 16)\n",
      "      (price_emb): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (len_title_emb): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (len_desc_emb): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (encode_brand_emb): Embedding(177190, 16)\n",
      "      (encode_color_emb): Embedding(203261, 16)\n",
      "      (encode_size_emb): Embedding(218061, 16)\n",
      "      (encode_model_emb): Embedding(524102, 16)\n",
      "      (encode_material_emb): Embedding(45569, 16)\n",
      "      (encode_author_emb): Embedding(30836, 16)\n",
      "      (encode_price_emb): Embedding(10, 16)\n",
      "      (encode_len_title_emb): Embedding(10, 16)\n",
      "      (encode_len_desc_emb): Embedding(10, 16)\n",
      "    )\n",
      "    (title_emb): Embedding(1410676, 384, padding_idx=1410675)\n",
      "    (title_linear): Linear(in_features=384, out_features=32, bias=True)\n",
      "    (desc_emb): Embedding(1410676, 384, padding_idx=1410675)\n",
      "    (desc_linear): Linear(in_features=384, out_features=32, bias=True)\n",
      "    (w2v_emb): Embedding(1410676, 128, padding_idx=1410675)\n",
      "    (w2v_linear): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      "  (seq_fea_emb): SeqFeatureEmbedding(\n",
      "    (idNUNIQUE_emb): Embedding(133, 16)\n",
      "    (idCOUNT_emb): Embedding(475, 16)\n",
      "    (brandNUNIQUE_emb): Embedding(39, 16)\n",
      "    (brandCOUNT_emb): Embedding(475, 16)\n",
      "    (colorNUNIQUE_emb): Embedding(53, 16)\n",
      "    (colorCOUNT_emb): Embedding(269, 16)\n",
      "    (sizeNUNIQUE_emb): Embedding(115, 16)\n",
      "    (sizeCOUNT_emb): Embedding(475, 16)\n",
      "    (modelNUNIQUE_emb): Embedding(59, 16)\n",
      "    (modelCOUNT_emb): Embedding(475, 16)\n",
      "    (materialNUNIQUE_emb): Embedding(25, 16)\n",
      "    (materialCOUNT_emb): Embedding(475, 16)\n",
      "    (authorNUNIQUE_emb): Embedding(65, 16)\n",
      "    (authorCOUNT_emb): Embedding(190, 16)\n",
      "    (seq_cat_emb): Sequential(\n",
      "      (0): Linear(in_features=224, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "    (seq_num_emb): Sequential(\n",
      "      (0): Linear(in_features=15, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(304, 256, num_layers=4, batch_first=True)\n",
      "  (intra_attn): IntraAttention(\n",
      "    (w1): Linear(in_features=256, out_features=1, bias=False)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=False)\n",
      "    (w3): Linear(in_features=256, out_features=256, bias=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (output): MatcherV2(\n",
      "    (out_linear): Linear(in_features=320, out_features=1, bias=False)\n",
      "    (w1_linear): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (w2_linear): Linear(in_features=304, out_features=320, bias=False)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (loss_func): CrossEntropyLoss()\n",
      ")\n",
      "2023-05-15 04:23:32,281 - INFO - product_emb.product_fea.locale_emb.weight\ttorch.Size([6, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,284 - INFO - product_emb.product_fea.price_emb.weight\ttorch.Size([16, 1])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,285 - INFO - product_emb.product_fea.price_emb.bias\ttorch.Size([16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,286 - INFO - product_emb.product_fea.len_title_emb.weight\ttorch.Size([16, 1])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,287 - INFO - product_emb.product_fea.len_title_emb.bias\ttorch.Size([16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,288 - INFO - product_emb.product_fea.len_desc_emb.weight\ttorch.Size([16, 1])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,290 - INFO - product_emb.product_fea.len_desc_emb.bias\ttorch.Size([16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,291 - INFO - product_emb.product_fea.encode_brand_emb.weight\ttorch.Size([177190, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,292 - INFO - product_emb.product_fea.encode_color_emb.weight\ttorch.Size([203261, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,293 - INFO - product_emb.product_fea.encode_size_emb.weight\ttorch.Size([218061, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,294 - INFO - product_emb.product_fea.encode_model_emb.weight\ttorch.Size([524102, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,295 - INFO - product_emb.product_fea.encode_material_emb.weight\ttorch.Size([45569, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,296 - INFO - product_emb.product_fea.encode_author_emb.weight\ttorch.Size([30836, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,298 - INFO - product_emb.product_fea.encode_price_emb.weight\ttorch.Size([10, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,299 - INFO - product_emb.product_fea.encode_len_title_emb.weight\ttorch.Size([10, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,305 - INFO - product_emb.product_fea.encode_len_desc_emb.weight\ttorch.Size([10, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,306 - INFO - product_emb.title_emb.weight\ttorch.Size([1410676, 384])\tcuda:2\tFalse\n",
      "2023-05-15 04:23:32,307 - INFO - product_emb.title_linear.weight\ttorch.Size([32, 384])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,308 - INFO - product_emb.title_linear.bias\ttorch.Size([32])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,309 - INFO - product_emb.desc_emb.weight\ttorch.Size([1410676, 384])\tcuda:2\tFalse\n",
      "2023-05-15 04:23:32,310 - INFO - product_emb.desc_linear.weight\ttorch.Size([32, 384])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,311 - INFO - product_emb.desc_linear.bias\ttorch.Size([32])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,312 - INFO - product_emb.w2v_emb.weight\ttorch.Size([1410676, 128])\tcuda:2\tFalse\n",
      "2023-05-15 04:23:32,313 - INFO - product_emb.w2v_linear.weight\ttorch.Size([32, 128])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,314 - INFO - product_emb.w2v_linear.bias\ttorch.Size([32])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,315 - INFO - seq_fea_emb.idNUNIQUE_emb.weight\ttorch.Size([133, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,316 - INFO - seq_fea_emb.idCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,317 - INFO - seq_fea_emb.brandNUNIQUE_emb.weight\ttorch.Size([39, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,318 - INFO - seq_fea_emb.brandCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,319 - INFO - seq_fea_emb.colorNUNIQUE_emb.weight\ttorch.Size([53, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,320 - INFO - seq_fea_emb.colorCOUNT_emb.weight\ttorch.Size([269, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,321 - INFO - seq_fea_emb.sizeNUNIQUE_emb.weight\ttorch.Size([115, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,322 - INFO - seq_fea_emb.sizeCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,323 - INFO - seq_fea_emb.modelNUNIQUE_emb.weight\ttorch.Size([59, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,324 - INFO - seq_fea_emb.modelCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,325 - INFO - seq_fea_emb.materialNUNIQUE_emb.weight\ttorch.Size([25, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,326 - INFO - seq_fea_emb.materialCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,328 - INFO - seq_fea_emb.authorNUNIQUE_emb.weight\ttorch.Size([65, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,329 - INFO - seq_fea_emb.authorCOUNT_emb.weight\ttorch.Size([190, 16])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,330 - INFO - seq_fea_emb.seq_cat_emb.0.weight\ttorch.Size([256, 224])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,331 - INFO - seq_fea_emb.seq_cat_emb.0.bias\ttorch.Size([256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,332 - INFO - seq_fea_emb.seq_cat_emb.2.weight\ttorch.Size([32, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,334 - INFO - seq_fea_emb.seq_cat_emb.2.bias\ttorch.Size([32])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,335 - INFO - seq_fea_emb.seq_num_emb.0.weight\ttorch.Size([256, 15])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,336 - INFO - seq_fea_emb.seq_num_emb.0.bias\ttorch.Size([256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,337 - INFO - seq_fea_emb.seq_num_emb.2.weight\ttorch.Size([32, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,338 - INFO - seq_fea_emb.seq_num_emb.2.bias\ttorch.Size([32])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,339 - INFO - lstm.weight_ih_l0\ttorch.Size([1024, 304])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,341 - INFO - lstm.weight_hh_l0\ttorch.Size([1024, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,342 - INFO - lstm.bias_ih_l0\ttorch.Size([1024])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,343 - INFO - lstm.bias_hh_l0\ttorch.Size([1024])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,344 - INFO - lstm.weight_ih_l1\ttorch.Size([1024, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,345 - INFO - lstm.weight_hh_l1\ttorch.Size([1024, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,346 - INFO - lstm.bias_ih_l1\ttorch.Size([1024])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,347 - INFO - lstm.bias_hh_l1\ttorch.Size([1024])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,348 - INFO - lstm.weight_ih_l2\ttorch.Size([1024, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,349 - INFO - lstm.weight_hh_l2\ttorch.Size([1024, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,351 - INFO - lstm.bias_ih_l2\ttorch.Size([1024])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,352 - INFO - lstm.bias_hh_l2\ttorch.Size([1024])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,353 - INFO - lstm.weight_ih_l3\ttorch.Size([1024, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,354 - INFO - lstm.weight_hh_l3\ttorch.Size([1024, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,355 - INFO - lstm.bias_ih_l3\ttorch.Size([1024])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,356 - INFO - lstm.bias_hh_l3\ttorch.Size([1024])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,357 - INFO - intra_attn.w1.weight\ttorch.Size([1, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,359 - INFO - intra_attn.w2.weight\ttorch.Size([256, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,360 - INFO - intra_attn.w3.weight\ttorch.Size([256, 256])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,361 - INFO - output.out_linear.weight\ttorch.Size([1, 320])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,362 - INFO - output.w1_linear.weight\ttorch.Size([320, 320])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,363 - INFO - output.w2_linear.weight\ttorch.Size([320, 304])\tcuda:2\tTrue\n",
      "2023-05-15 04:23:32,365 - INFO - Total parameter numbers: 1285796576\n",
      "2023-05-15 04:23:32,895 - INFO - Fold 0: trn_idx 2884999\n",
      "2023-05-15 04:23:32,896 - INFO - Fold 0: val_idx 721250\n",
      "2023-05-15 04:23:36,111 - INFO - train_set: 2884999\n",
      "2023-05-15 04:23:36,113 - INFO - val_set: 721250\n",
      "2023-05-15 04:23:36,114 - INFO - test_set: 316971\n",
      "2023-05-15 04:23:36,797 - INFO - train_loader: 2818\n",
      "2023-05-15 04:23:36,807 - INFO - val_loader: 705\n",
      "2023-05-15 04:23:36,813 - INFO - test_loader: 310\n"
     ]
    }
   ],
   "source": [
    "# 加载模型等\n",
    "\n",
    "logger.info('create model')\n",
    "\n",
    "products_input = {name: torch.tensor(products_encoded[name].values).to(device) for name in products_encoded.columns}\n",
    "\n",
    "if 'BaseModel' in model_name:\n",
    "    model = BaseModel(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\n",
    "elif 'MatchModel' in model_name:\n",
    "    model = MatchModelV2(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\n",
    "else:\n",
    "    raise ValueError('Error model name {}'.format(model_name))\n",
    "logger.info(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', patience=lr_patience, factor=lr_decay_ratio)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    logger.info(str(name) + '\\t' + str(param.shape) + '\\t' +\n",
    "                              str(param.device) + '\\t' + str(param.requires_grad))\n",
    "total_num = sum([param.nelement() for param in model.parameters()])\n",
    "logger.info('Total parameter numbers: {}'.format(total_num))\n",
    "\n",
    "\n",
    "# 数据集DataLoader\n",
    "\n",
    "trn_idx_list = np.load('data/5fold_trn_idx_list.npy', allow_pickle=True)\n",
    "val_idx_list = np.load('data/5fold_val_idx_list.npy', allow_pickle=True)\n",
    "logger.info('Fold {}: trn_idx {}'.format(Fold, len(trn_idx_list[Fold])))\n",
    "logger.info('Fold {}: val_idx {}'.format(Fold, len(val_idx_list[Fold])))\n",
    "\n",
    "train_set = NNDatasetV2(df_train_encoded.iloc[trn_idx_list[Fold]], \n",
    "                        df_train_seqs_cat_feas.iloc[trn_idx_list[Fold]], \n",
    "                        df_train_seqs_num_feas.iloc[trn_idx_list[Fold]])\n",
    "val_set = NNDatasetV2(df_train_encoded.iloc[val_idx_list[Fold]], \n",
    "                      df_train_seqs_cat_feas.iloc[val_idx_list[Fold]], \n",
    "                      df_train_seqs_num_feas.iloc[val_idx_list[Fold]])\n",
    "test_set = NNDatasetV2(df_test_encoded, df_test_seqs_cat_feas, \n",
    "                       df_test_seqs_num_feas)\n",
    "logger.info('train_set: {}'.format(len(train_set)))\n",
    "logger.info('val_set: {}'.format(len(val_set)))\n",
    "logger.info('test_set: {}'.format(len(test_set)))\n",
    "\n",
    "\n",
    "def collate_fn(indices):\n",
    "    batch_prev_items = []\n",
    "    batch_locale = []\n",
    "    batch_candidate_set = []\n",
    "    batch_len = []\n",
    "    batch_mask = []\n",
    "    batch_label = []\n",
    "    batch_label_index = []  # 交叉熵需要的是label在候选集中的index\n",
    "    batch_seq_cat = []\n",
    "    batch_seq_num = []\n",
    "    for item in indices:\n",
    "        batch_len.append(len(item[0]))  # prev_items\n",
    "    max_len = max(batch_len)\n",
    "    for item in indices:\n",
    "        l = len(item[0])\n",
    "        batch_mask.append([1] * (l) + [0] * (max_len - l))  # 0代表padding的位置，需要mask\n",
    "    for item in indices:\n",
    "        # ['prev_items', 'locale', 'recall', 'next_item', 'seqs_cat_feas', 'seqs_num_feas']\n",
    "        prev_items = item[0].copy()\n",
    "        while (len(prev_items) < max_len):\n",
    "            prev_items.append(id_count)  # embdding的时候id_count+1，把id_count作为padding了\n",
    "        batch_prev_items.append(prev_items)\n",
    "        batch_locale.append(item[1])\n",
    "        batch_candidate_set.append(item[2].copy())\n",
    "        batch_label.append(item[3])\n",
    "        if item[3] in item[2]:\n",
    "            batch_label_index.append(item[2].index(item[3]))\n",
    "        else:\n",
    "            batch_label_index.append(len(item[2]))\n",
    "        batch_seq_cat.append(item[4])\n",
    "        batch_seq_num.append(item[5])\n",
    "    return [torch.LongTensor(batch_prev_items).to(device), torch.LongTensor(batch_locale).to(device), \n",
    "            torch.LongTensor(batch_candidate_set).to(device),\n",
    "            torch.LongTensor(batch_len).to(device), torch.LongTensor(batch_mask).to(device), \n",
    "            torch.LongTensor(batch_label).to(device), torch.LongTensor(batch_label_index).to(device),\n",
    "            torch.LongTensor(batch_seq_cat).to(device), torch.FloatTensor(batch_seq_num).to(device)]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "logger.info('train_loader: {}'.format(len(train_loader)))\n",
    "logger.info('val_loader: {}'.format(len(val_loader)))\n",
    "logger.info('test_loader: {}'.format(len(test_loader)))\n",
    "\n",
    "\n",
    "output_dir = 'ckpt/{}'.format(exp_id)\n",
    "ensure_dir(output_dir)\n",
    "\n",
    "if load_init:\n",
    "    load_dir = 'ckpt/{}'.format(load_exp_id)\n",
    "    load_path = '{}/{}_{}_{}.pt'.format(load_dir, load_exp_id, model_name, load_epoch)\n",
    "    logger.info('Load Init model from {}'.format(load_path))\n",
    "    model.load_state_dict(torch.load(load_path, map_location='cpu'))\n",
    "    # print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train model 61323:   0%|          | 0/2818 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 14]) torch.Size([1024, 15])\n",
      "tensor(False, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_prev_items, batch_locale, batch_candidate_set, batch_len, batch_mask, \\\n",
    "                batch_label, batch_label_index, batch_seq_cat, batch_seq_num in tqdm(train_loader, desc='train model {}'.format(exp_id), total=len(train_loader)):\n",
    "    print(batch_seq_cat.shape, batch_seq_num.shape)\n",
    "    print(torch.isnan(batch_seq_num).any())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train model 61323:   0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 14]) torch.Size([1024, 15])\n",
      "tensor(False, device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_prev_items, batch_locale, batch_candidate_set, batch_len, batch_mask, \\\n",
    "                batch_label, batch_label_index, batch_seq_cat, batch_seq_num in tqdm(test_loader, desc='train model {}'.format(exp_id), total=len(test_loader)):\n",
    "    print(batch_seq_cat.shape, batch_seq_num.shape)\n",
    "    print(torch.isnan(batch_seq_num).any())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "3 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "4 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "5 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "2 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "score, loss = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \n",
    "                                        batch_candidate_set=batch_candidate_set, batch_len=batch_len, \n",
    "                                        batch_label=batch_label_index, batch_mask=batch_mask,\n",
    "                                        batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "3 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "4 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "5 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "2 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "origin_score = model.forward(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \n",
    "                                        batch_candidate_set=batch_candidate_set, batch_len=batch_len, \n",
    "                                        batch_label=batch_label_index, batch_mask=batch_mask,\n",
    "                                        batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "4 tensor(False, device='cuda:2') tensor(False, device='cuda:2')\n",
      "5 tensor(True, device='cuda:2') tensor(False, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "seq_fea = model.seq_fea_emb.forward(batch_seq_cat, batch_seq_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.seq_fea_emb.seq_num_emb[0].forward(batch_seq_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.seq_fea_emb.seq_num_emb.forward(batch_seq_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 15])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_seq_num.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:2')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(batch_seq_num).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:2')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(batch_seq_cat).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=15, out_features=256, bias=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seq_fea_emb.seq_num_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:2')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(a).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:2')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(model.seq_fea_emb.seq_num_emb[0].weight).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:2')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(model.seq_fea_emb.seq_num_emb[0].bias).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:2')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(model.seq_fea_emb.seq_num_emb[2].weight).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:2')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(model.seq_fea_emb.seq_num_emb[2].bias).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1698, -0.1382, -0.0299,  ..., -0.2223, -0.1431,  0.1554],\n",
       "        [ 0.2277, -0.0768,  0.2571,  ...,  0.0304, -0.0268, -0.2199],\n",
       "        [-0.0926,  0.0366,  0.0841,  ..., -0.1207, -0.1244, -0.0577],\n",
       "        ...,\n",
       "        [ 0.1187,  0.1778,  0.0615,  ...,  0.2291, -0.1480, -0.0150],\n",
       "        [ 0.2459,  0.0303,  0.1273,  ...,  0.0031, -0.1471,  0.0767],\n",
       "        [ 0.2048, -0.2471, -0.0897,  ..., -0.1482, -0.0821,  0.0180]],\n",
       "       device='cuda:2', requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seq_fea_emb.seq_num_emb[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0464, -0.0045,  0.0083,  ..., -0.0614,  0.0260, -0.0051],\n",
       "        [ 0.0320,  0.0081, -0.0036,  ..., -0.0488, -0.0134, -0.0376],\n",
       "        [ 0.0469,  0.0510, -0.0363,  ...,  0.0507,  0.0555,  0.0120],\n",
       "        ...,\n",
       "        [-0.0187,  0.0309, -0.0306,  ...,  0.0426,  0.0534, -0.0376],\n",
       "        [-0.0186,  0.0345, -0.0244,  ...,  0.0400, -0.0067,  0.0009],\n",
       "        [-0.0617,  0.0360,  0.0541,  ..., -0.0453,  0.0325,  0.0001]],\n",
       "       device='cuda:2', requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seq_fea_emb.seq_num_emb[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0290,  0.0571,  0.0445,  ..., -0.0549,  0.0114, -0.0176],\n",
       "        [-0.0340,  0.0471,  0.0197,  ...,  0.0143, -0.0160, -0.0311],\n",
       "        [ 0.0572, -0.0158,  0.0650,  ..., -0.0228, -0.0290, -0.0478],\n",
       "        ...,\n",
       "        [ 0.0175, -0.0576,  0.0565,  ...,  0.0112,  0.0423,  0.0325],\n",
       "        [ 0.0220,  0.0642, -0.0455,  ..., -0.0289,  0.0323,  0.0132],\n",
       "        [-0.0546, -0.0563,  0.0144,  ..., -0.0452,  0.0218, -0.0116]],\n",
       "       device='cuda:2', requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seq_fea_emb.seq_cat_emb[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0277, -0.0287, -0.0604,  ..., -0.0460,  0.0555,  0.0345],\n",
       "        [-0.0079,  0.0150,  0.0315,  ...,  0.0191,  0.0371, -0.0398],\n",
       "        [-0.0033,  0.0423,  0.0566,  ...,  0.0389,  0.0315,  0.0210],\n",
       "        ...,\n",
       "        [-0.0579, -0.0562,  0.0390,  ..., -0.0594,  0.0487, -0.0598],\n",
       "        [-0.0621,  0.0214, -0.0616,  ..., -0.0214, -0.0563, -0.0581],\n",
       "        [-0.0032, -0.0371, -0.0409,  ...,  0.0337,  0.0503,  0.0557]],\n",
       "       device='cuda:2', requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seq_fea_emb.seq_cat_emb[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(True, device='cuda:2') tensor(False, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "print(4, torch.isnan(seq_fea).any(), torch.isinf(seq_fea).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11700, device='cuda:2')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(origin_score).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d9dcf8b2d465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "(torch.softmax(origin_score, dim=1) == score).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0085, 0.0097, 0.0083,  ..., 0.0136, 0.0088, 0.0105],\n",
       "        [0.0099, 0.0118, 0.0120,  ..., 0.0104, 0.0091, 0.0104],\n",
       "        [0.0110, 0.0120, 0.0075,  ..., 0.0088, 0.0083, 0.0115],\n",
       "        ...,\n",
       "        [0.0095, 0.0076, 0.0087,  ..., 0.0092, 0.0077, 0.0088],\n",
       "        [0.0097, 0.0113, 0.0110,  ..., 0.0105, 0.0104, 0.0127],\n",
       "        [0.0073, 0.0078, 0.0098,  ..., 0.0130, 0.0101, 0.0119]],\n",
       "       device='cuda:3', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ab6b00ec54a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morigin_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "origin_score.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b77f28f8559a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./origin_score.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./score.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_label_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./batch_label_index.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# .cpu() on the underlying Storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;31m# or on typing_extensions module on Python >= 3.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, dtype, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "torch.save(origin_score, './origin_score.pt')\n",
    "torch.save(score, './score.pt')\n",
    "torch.save(batch_label_index, './batch_label_index.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2161, -0.0138, -0.1616,  ...,  0.0532,  0.1819,  0.1372],\n",
       "        [-0.0826,  0.0503,  0.2164,  ...,  0.1613, -0.0115, -0.0652],\n",
       "        [-0.2265,  0.0434, -0.0520,  ..., -0.0257, -0.0259,  0.1452],\n",
       "        ...,\n",
       "        [ 0.2151, -0.3285, -0.0392,  ..., -0.0140, -0.1214,  0.0703],\n",
       "        [ 0.0691,  0.1465,  0.0234,  ...,  0.1110,  0.2160,  0.3192],\n",
       "        [-0.2882, -0.1319, -0.1053,  ..., -0.1123, -0.0745,  0.1891]],\n",
       "       device='cuda:3', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 100]), torch.Size([1024]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_score.shape, batch_label_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36,  2,  7,  ...,  2, 28, 53], device='cuda:3')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=data_feature['len_candidate_set']).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feature['len_candidate_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1a93e00af3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "loss_func(origin_score, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:3', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 4,  ..., 4, 0, 0],\n",
       "        [8, 9, 4,  ..., 3, 0, 0],\n",
       "        [2, 2, 1,  ..., 1, 0, 0],\n",
       "        ...,\n",
       "        [2, 3, 1,  ..., 3, 0, 0],\n",
       "        [2, 2, 1,  ..., 0, 0, 0],\n",
       "        [1, 2, 1,  ..., 2, 0, 0]], device='cuda:3')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_seq_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8975e-07, 2.6173e-07, 1.7475e-07,  ..., 2.2122e-01, 4.8800e-01,\n",
       "         3.8864e-02],\n",
       "        [5.7172e-05, 2.7616e-05, 3.8225e-05,  ..., 1.3013e-02, 4.7000e-02,\n",
       "         4.5832e-03],\n",
       "        [4.2475e-05, 0.0000e+00, 4.2475e-05,  ..., 1.8318e-01, 1.8300e-01,\n",
       "         7.5902e-03],\n",
       "        ...,\n",
       "        [9.6917e-05, 2.5311e-05, 7.6250e-05,  ..., 6.2062e-02, 6.7000e-02,\n",
       "         4.0647e-03],\n",
       "        [1.8125e-05, 6.2500e-06, 1.5000e-05,  ..., 1.4815e-01, 1.4800e-01,\n",
       "         6.1385e-03],\n",
       "        [4.4975e-07, 0.0000e+00, 4.4975e-07,  ..., 3.4134e-01, 3.4100e-01,\n",
       "         1.4144e-02]], device='cuda:3')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_seq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>locale</th>\n",
       "      <th>price</th>\n",
       "      <th>len_title</th>\n",
       "      <th>len_desc</th>\n",
       "      <th>encode_brand</th>\n",
       "      <th>encode_color</th>\n",
       "      <th>encode_size</th>\n",
       "      <th>encode_model</th>\n",
       "      <th>encode_material</th>\n",
       "      <th>encode_author</th>\n",
       "      <th>encode_price</th>\n",
       "      <th>encode_len_title</th>\n",
       "      <th>encode_len_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.950001</td>\n",
       "      <td>96.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>112134</td>\n",
       "      <td>203260</td>\n",
       "      <td>218060</td>\n",
       "      <td>426630</td>\n",
       "      <td>45568</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>186.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>124505</td>\n",
       "      <td>203260</td>\n",
       "      <td>128007</td>\n",
       "      <td>524101</td>\n",
       "      <td>45568</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>68.889999</td>\n",
       "      <td>181.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>122979</td>\n",
       "      <td>114264</td>\n",
       "      <td>170270</td>\n",
       "      <td>145013</td>\n",
       "      <td>15566</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18.990000</td>\n",
       "      <td>101.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>9834</td>\n",
       "      <td>46931</td>\n",
       "      <td>218060</td>\n",
       "      <td>67408</td>\n",
       "      <td>29357</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7.170000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>105135</td>\n",
       "      <td>117844</td>\n",
       "      <td>170305</td>\n",
       "      <td>174527</td>\n",
       "      <td>23064</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410670</th>\n",
       "      <td>1410670</td>\n",
       "      <td>5</td>\n",
       "      <td>578.979980</td>\n",
       "      <td>124.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>116789</td>\n",
       "      <td>55013</td>\n",
       "      <td>109396</td>\n",
       "      <td>524101</td>\n",
       "      <td>45568</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410671</th>\n",
       "      <td>1410671</td>\n",
       "      <td>5</td>\n",
       "      <td>43.490002</td>\n",
       "      <td>195.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>121227</td>\n",
       "      <td>80328</td>\n",
       "      <td>143053</td>\n",
       "      <td>524101</td>\n",
       "      <td>19188</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410672</th>\n",
       "      <td>1410672</td>\n",
       "      <td>5</td>\n",
       "      <td>8.410000</td>\n",
       "      <td>144.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>52556</td>\n",
       "      <td>39304</td>\n",
       "      <td>6470</td>\n",
       "      <td>257247</td>\n",
       "      <td>45568</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410673</th>\n",
       "      <td>1410673</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>23342</td>\n",
       "      <td>203260</td>\n",
       "      <td>218060</td>\n",
       "      <td>57350</td>\n",
       "      <td>45568</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410674</th>\n",
       "      <td>1410674</td>\n",
       "      <td>5</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>113.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>89982</td>\n",
       "      <td>170638</td>\n",
       "      <td>218060</td>\n",
       "      <td>524101</td>\n",
       "      <td>18060</td>\n",
       "      <td>30835</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1410675 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  locale       price  len_title  len_desc  encode_brand  \\\n",
       "0              0       0   30.950001       96.0     121.0        112134   \n",
       "1              1       0   17.900000      186.0     330.0        124505   \n",
       "2              2       0   68.889999      181.0      95.0        122979   \n",
       "3              3       0   18.990000      101.0     191.0          9834   \n",
       "4              4       0    7.170000       45.0      15.0        105135   \n",
       "...          ...     ...         ...        ...       ...           ...   \n",
       "1410670  1410670       5  578.979980      124.0     250.0        116789   \n",
       "1410671  1410671       5   43.490002      195.0     479.0        121227   \n",
       "1410672  1410672       5    8.410000      144.0      57.0         52556   \n",
       "1410673  1410673       5  100.000000       59.0      85.0         23342   \n",
       "1410674  1410674       5   18.350000      113.0      24.0         89982   \n",
       "\n",
       "         encode_color  encode_size  encode_model  encode_material  \\\n",
       "0              203260       218060        426630            45568   \n",
       "1              203260       128007        524101            45568   \n",
       "2              114264       170270        145013            15566   \n",
       "3               46931       218060         67408            29357   \n",
       "4              117844       170305        174527            23064   \n",
       "...               ...          ...           ...              ...   \n",
       "1410670         55013       109396        524101            45568   \n",
       "1410671         80328       143053        524101            19188   \n",
       "1410672         39304         6470        257247            45568   \n",
       "1410673        203260       218060         57350            45568   \n",
       "1410674        170638       218060        524101            18060   \n",
       "\n",
       "         encode_author  encode_price  encode_len_title  encode_len_desc  \n",
       "0                30835             0                 2                2  \n",
       "1                30835             0                 4                5  \n",
       "2                30835             0                 4                1  \n",
       "3                30835             0                 2                3  \n",
       "4                30835             0                 0                0  \n",
       "...                ...           ...               ...              ...  \n",
       "1410670          30835             0                 2                4  \n",
       "1410671          30835             0                 4                7  \n",
       "1410672          30835             0                 3                1  \n",
       "1410673          30835             0                 1                1  \n",
       "1410674          30835             0                 2                0  \n",
       "\n",
       "[1410675 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([      0,       1,       2,  ..., 1410672, 1410673, 1410674],\n",
       "        device='cuda:3'),\n",
       " 'locale': tensor([0, 0, 0,  ..., 5, 5, 5], device='cuda:3'),\n",
       " 'price': tensor([ 30.9500,  17.9000,  68.8900,  ...,   8.4100, 100.0000,  18.3500],\n",
       "        device='cuda:3'),\n",
       " 'len_title': tensor([ 96., 186., 181.,  ..., 144.,  59., 113.], device='cuda:3'),\n",
       " 'len_desc': tensor([121., 330.,  95.,  ...,  57.,  85.,  24.], device='cuda:3'),\n",
       " 'encode_brand': tensor([112134, 124505, 122979,  ...,  52556,  23342,  89982], device='cuda:3'),\n",
       " 'encode_color': tensor([203260, 203260, 114264,  ...,  39304, 203260, 170638], device='cuda:3'),\n",
       " 'encode_size': tensor([218060, 128007, 170270,  ...,   6470, 218060, 218060], device='cuda:3'),\n",
       " 'encode_model': tensor([426630, 524101, 145013,  ..., 257247,  57350, 524101], device='cuda:3'),\n",
       " 'encode_material': tensor([45568, 45568, 15566,  ..., 45568, 45568, 18060], device='cuda:3'),\n",
       " 'encode_author': tensor([30835, 30835, 30835,  ..., 30835, 30835, 30835], device='cuda:3'),\n",
       " 'encode_price': tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:3'),\n",
       " 'encode_len_title': tensor([2, 4, 4,  ..., 3, 1, 2], device='cuda:3'),\n",
       " 'encode_len_desc': tensor([2, 5, 1,  ..., 1, 1, 0], device='cuda:3')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1054e-01, -3.6703e-01,  4.0416e-01,  ..., -6.3685e+01,\n",
       "         -6.2647e+01,  3.2512e+01],\n",
       "        [-3.1322e-01, -3.3288e-01,  9.6511e-02,  ...,  3.2139e+02,\n",
       "         -1.0420e+02,  8.0631e+02],\n",
       "        [-7.0365e-02, -2.7373e-01, -4.4755e-02,  ...,  2.0069e+00,\n",
       "          6.9152e+01,  2.6755e+02],\n",
       "        ...,\n",
       "        [-1.9821e-01, -3.8917e-01,  1.4973e-01,  ...,  1.8869e+02,\n",
       "          1.3707e+02,  6.3639e+02],\n",
       "        [-2.9664e-01, -4.2047e-01,  1.1163e-01,  ..., -7.9712e+00,\n",
       "          1.4268e+01,  9.6694e+01],\n",
       "        [-4.3668e-02, -2.9515e-01, -3.6954e-02,  ..., -1.9614e+01,\n",
       "         -2.9803e+01,  3.8246e+01]], device='cuda:3', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.seq_fea_emb(batch_seq_cat, batch_seq_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libcityng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
