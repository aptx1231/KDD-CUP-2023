{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cab\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import json \n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 13:45:28,705 - INFO - Log directory: ./log\n",
      "2023-06-03 13:45:28,706 - INFO - Exp_id 17581\n",
      "2023-06-03 13:45:28,707 - INFO - {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'df_train_all_exploded_add_last_recall', \"import numpy as np \\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport warnings\\nimport os\\nimport re\\nimport math\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nfrom plotly.subplots import make_subplots\\n\\nfrom lightgbm import LGBMRegressor, LGBMClassifier\\nfrom xgboost import XGBRegressor, XGBClassifier\\nfrom catboost import CatBoostRegressor, CatBoostClassifier\\nimport lightgbm as lgb\\nimport xgboost as xgb\\nimport catboost as cab\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\nfrom sklearn import metrics\\nfrom sklearn.svm import SVC\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom tqdm import tqdm\\n\\nimport matplotlib.pyplot as plt\\nfrom collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings('ignore')\\nfrom sklearn.preprocessing import KBinsDiscretizer\", \"import time\\nimport numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings('ignore')\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\n\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\nimport argparse\\nfrom utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\\nfrom data import NNDataset, NNDatasetV2, NNDatasetV3\\nfrom model import MatchModel, BaseModel, MatchModelV2, BinaryModel\\nfrom sklearn import metrics\\n\\nseed = 2023\\nFold = 0\\nset_random_seed(seed)\\ntask = 'task1'\\nfeature = 'v3'\\ndense_norm = True\\nrecall = 'window'\\nlen_candidate_set = 10\\nadd_title = False\\nadd_desc = False\\nadd_w2v = False\\nw2v_vector_size = 128\\n\\nmodel_name = 'LGBFold{}'.format(Fold)\\nloc2id = {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}\\n\\nconfig = locals()\\n\\n# 加载必要的数据\\n\\nexp_id = None\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\nif task == 'task1':\\n    df_test_encoded = pd.read_csv('data/df_test_encoded_phase2.csv')\\nelif task == 'task2':\\n    df_test_encoded = pd.read_csv('data/df_test_encoded_phase2_onlytask2.csv')\\n\\nif feature.lower() == 'v1':\\n    products_encoded = pd.read_csv('./data/products_encoded.csv')\\n    num_features = ['price', 'len_title', 'len_desc']\\nelif feature.lower() == 'v2':\\n    products_encoded = pd.read_csv('./data/products_encoded_newfeature.csv')\\n    num_features = ['price']\\nelif feature.lower() == 'v3':\\n    products_encoded = pd.read_csv('./data/products_encoded_phase2_V3.csv')\\n    num_features = ['price']\\n\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\\n\\nif dense_norm:\\n    logger.info('MinMaxScaler Norm products_num_feas')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded[fe] = products_encoded[fe].astype('float32')\\n    assert products_encoded[fe].dtypes == 'float32'\\n\\nid_count = products_encoded.shape[0]\\n\\nif task == 'task1':\\n    train_preds_encoded = pickle.load(open('./data/train_preds_{}_one_phase2_noleak_encoded.pkl'.format(recall), 'rb'))  # (len_train, 100)\\n    test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)\\n    logger.info('./data/train_preds_{}_one_phase2_noleak_encoded.pkl'.format(recall))\\n    logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\n    logger.info('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall))\\n    logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\nelif task == 'task2':\\n    train_preds_encoded = pickle.load(open('./data/train_preds_{}_one_onlytask2_phase2_noleak_encoded.pkl'.format(recall), 'rb'))  # (len_train, 100)\\n    test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_onlytask2_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)\\n    logger.info('./data/train_preds_{}_one_onlytask2_phase2_noleak_encoded.pkl'.format(recall))\\n    logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\n    logger.info('./data/test_preds_{}_one_onlytask2_phase2_encoded.pkl'.format(recall))\\n    logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\n\\nlogger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\\n# TODO: 可以改成保障一个正样本，补充9个负样本\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded['recall'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded['recall'] = cut_test_preds_encoded\\n\\nlogger.info('Eval the prev_items')\\ndf_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\\ndf_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\\ndf_train_encoded['last_item_2'] = df_train_encoded['prev_items'].apply(lambda x: x[-2])\\ndf_test_encoded['last_item_2'] = df_test_encoded['prev_items'].apply(lambda x: x[-2])\\n\\nlogger.info('Load Hand-made Seq Features')\\n\\nif feature.lower() == 'v1':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_onlytask2.csv')\\n    dense_bins = 10\\nelif feature.lower() == 'v2':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all_newfeature.csv')  # 24维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_newfeature_phase2.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_newfeature_phase2_onlytask2.csv')\\n    dense_bins = 100\\nelif feature.lower() == 'v3':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all_phase2_V3.csv')  # 36维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_V3.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_V3_onlytask2.csv')\\n    dense_bins = 100\\n\\nlogger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\\nlogger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f or 'encode_' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\\nlogger.info('seqs_num_feas: {}'.format(seqs_num_feas))\\n\\nif dense_norm:\\n    logger.info('MinMaxScaler Norm seqs_num_feas')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\n    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\n\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\\n    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\\n\\ndf_train_all = pd.concat([df_train_encoded, df_train_seqs_feas_all], axis=1)\\ndf_test_all = pd.concat([df_test_encoded, df_test_seqs_feas_all], axis=1)\\nlogger.info('df_train_all: {}'.format(df_train_all.shape))\\nlogger.info('df_test_all: {}'.format(df_test_all.shape))\\n\\ndf_train_all_exploded = df_train_all.explode('recall')\\ndf_test_all_exploded = df_test_all.explode('recall')\\n\\ndf_train_all_exploded['label'] = df_train_all_exploded['next_item'] == df_train_all_exploded['recall']\\ndf_test_all_exploded['label'] = df_test_all_exploded['next_item'] == df_test_all_exploded['recall']\\n\\ndf_train_all_exploded['index'] = df_train_all_exploded.index \\ndf_test_all_exploded['index'] = df_test_all_exploded.index \\n\\nlogger.info('df_train_all_exploded: {}'.format(df_train_all_exploded.shape))\\nlogger.info('df_test_all_exploded: {}'.format(df_test_all_exploded.shape))\\n\\nlogger.info('df_train_all_exploded.label.sum: {}'.format(df_train_all_exploded['label'].sum()))\\nlogger.info('df_test_all_exploded.label.sum: {}'.format(df_test_all_exploded['label'].sum()))\\n\\ndf_train_encoded_exploded = df_train_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\\ndf_train_seqs_cat_feas = df_train_all_exploded[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_all_exploded[seqs_num_feas]\\ndf_test_encoded_exploded = df_test_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\\ndf_test_seqs_cat_feas = df_test_all_exploded[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_all_exploded[seqs_num_feas]\\n\\nlogger.info('df_train_encoded_exploded: {}'.format(df_train_encoded_exploded.shape))\\nlogger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\\nlogger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\\nlogger.info('df_test_encoded_exploded: {}'.format(df_test_encoded_exploded.shape))\\nlogger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\\nlogger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\\n\\nif task == 'task1':\\n    df_test = pd.read_csv('data/phase2/sessions_test_task1.csv')\\n    logger.info('df_test Task 1 Phase 2: {}'.format(df_test.shape))\\nelif task == 'task2':\\n    df_test = pd.read_csv('data/phase2/sessions_test_task2.csv')\\n    logger.info('df_test Task 2 Phase 2: {}'.format(df_test.shape))\"], '_oh': {}, '_dh': ['/home/panda/private/jjw/competition/KDDCUP2023'], 'In': ['', 'df_train_all_exploded_add_last_recall', \"import numpy as np \\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport warnings\\nimport os\\nimport re\\nimport math\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nfrom plotly.subplots import make_subplots\\n\\nfrom lightgbm import LGBMRegressor, LGBMClassifier\\nfrom xgboost import XGBRegressor, XGBClassifier\\nfrom catboost import CatBoostRegressor, CatBoostClassifier\\nimport lightgbm as lgb\\nimport xgboost as xgb\\nimport catboost as cab\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\nfrom sklearn import metrics\\nfrom sklearn.svm import SVC\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom tqdm import tqdm\\n\\nimport matplotlib.pyplot as plt\\nfrom collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings('ignore')\\nfrom sklearn.preprocessing import KBinsDiscretizer\", \"import time\\nimport numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings('ignore')\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\n\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\nimport argparse\\nfrom utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\\nfrom data import NNDataset, NNDatasetV2, NNDatasetV3\\nfrom model import MatchModel, BaseModel, MatchModelV2, BinaryModel\\nfrom sklearn import metrics\\n\\nseed = 2023\\nFold = 0\\nset_random_seed(seed)\\ntask = 'task1'\\nfeature = 'v3'\\ndense_norm = True\\nrecall = 'window'\\nlen_candidate_set = 10\\nadd_title = False\\nadd_desc = False\\nadd_w2v = False\\nw2v_vector_size = 128\\n\\nmodel_name = 'LGBFold{}'.format(Fold)\\nloc2id = {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}\\n\\nconfig = locals()\\n\\n# 加载必要的数据\\n\\nexp_id = None\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\nif task == 'task1':\\n    df_test_encoded = pd.read_csv('data/df_test_encoded_phase2.csv')\\nelif task == 'task2':\\n    df_test_encoded = pd.read_csv('data/df_test_encoded_phase2_onlytask2.csv')\\n\\nif feature.lower() == 'v1':\\n    products_encoded = pd.read_csv('./data/products_encoded.csv')\\n    num_features = ['price', 'len_title', 'len_desc']\\nelif feature.lower() == 'v2':\\n    products_encoded = pd.read_csv('./data/products_encoded_newfeature.csv')\\n    num_features = ['price']\\nelif feature.lower() == 'v3':\\n    products_encoded = pd.read_csv('./data/products_encoded_phase2_V3.csv')\\n    num_features = ['price']\\n\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\\n\\nif dense_norm:\\n    logger.info('MinMaxScaler Norm products_num_feas')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded[fe] = products_encoded[fe].astype('float32')\\n    assert products_encoded[fe].dtypes == 'float32'\\n\\nid_count = products_encoded.shape[0]\\n\\nif task == 'task1':\\n    train_preds_encoded = pickle.load(open('./data/train_preds_{}_one_phase2_noleak_encoded.pkl'.format(recall), 'rb'))  # (len_train, 100)\\n    test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)\\n    logger.info('./data/train_preds_{}_one_phase2_noleak_encoded.pkl'.format(recall))\\n    logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\n    logger.info('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall))\\n    logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\nelif task == 'task2':\\n    train_preds_encoded = pickle.load(open('./data/train_preds_{}_one_onlytask2_phase2_noleak_encoded.pkl'.format(recall), 'rb'))  # (len_train, 100)\\n    test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_onlytask2_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)\\n    logger.info('./data/train_preds_{}_one_onlytask2_phase2_noleak_encoded.pkl'.format(recall))\\n    logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\n    logger.info('./data/test_preds_{}_one_onlytask2_phase2_encoded.pkl'.format(recall))\\n    logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\n\\nlogger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\\n# TODO: 可以改成保障一个正样本，补充9个负样本\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded['recall'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded['recall'] = cut_test_preds_encoded\\n\\nlogger.info('Eval the prev_items')\\ndf_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\\ndf_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\\ndf_train_encoded['last_item_2'] = df_train_encoded['prev_items'].apply(lambda x: x[-2])\\ndf_test_encoded['last_item_2'] = df_test_encoded['prev_items'].apply(lambda x: x[-2])\\n\\nlogger.info('Load Hand-made Seq Features')\\n\\nif feature.lower() == 'v1':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_onlytask2.csv')\\n    dense_bins = 10\\nelif feature.lower() == 'v2':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all_newfeature.csv')  # 24维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_newfeature_phase2.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_newfeature_phase2_onlytask2.csv')\\n    dense_bins = 100\\nelif feature.lower() == 'v3':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all_phase2_V3.csv')  # 36维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_V3.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_V3_onlytask2.csv')\\n    dense_bins = 100\\n\\nlogger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\\nlogger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f or 'encode_' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\\nlogger.info('seqs_num_feas: {}'.format(seqs_num_feas))\\n\\nif dense_norm:\\n    logger.info('MinMaxScaler Norm seqs_num_feas')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\n    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\n\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\\n    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\\n\\ndf_train_all = pd.concat([df_train_encoded, df_train_seqs_feas_all], axis=1)\\ndf_test_all = pd.concat([df_test_encoded, df_test_seqs_feas_all], axis=1)\\nlogger.info('df_train_all: {}'.format(df_train_all.shape))\\nlogger.info('df_test_all: {}'.format(df_test_all.shape))\\n\\ndf_train_all_exploded = df_train_all.explode('recall')\\ndf_test_all_exploded = df_test_all.explode('recall')\\n\\ndf_train_all_exploded['label'] = df_train_all_exploded['next_item'] == df_train_all_exploded['recall']\\ndf_test_all_exploded['label'] = df_test_all_exploded['next_item'] == df_test_all_exploded['recall']\\n\\ndf_train_all_exploded['index'] = df_train_all_exploded.index \\ndf_test_all_exploded['index'] = df_test_all_exploded.index \\n\\nlogger.info('df_train_all_exploded: {}'.format(df_train_all_exploded.shape))\\nlogger.info('df_test_all_exploded: {}'.format(df_test_all_exploded.shape))\\n\\nlogger.info('df_train_all_exploded.label.sum: {}'.format(df_train_all_exploded['label'].sum()))\\nlogger.info('df_test_all_exploded.label.sum: {}'.format(df_test_all_exploded['label'].sum()))\\n\\ndf_train_encoded_exploded = df_train_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\\ndf_train_seqs_cat_feas = df_train_all_exploded[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_all_exploded[seqs_num_feas]\\ndf_test_encoded_exploded = df_test_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\\ndf_test_seqs_cat_feas = df_test_all_exploded[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_all_exploded[seqs_num_feas]\\n\\nlogger.info('df_train_encoded_exploded: {}'.format(df_train_encoded_exploded.shape))\\nlogger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\\nlogger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\\nlogger.info('df_test_encoded_exploded: {}'.format(df_test_encoded_exploded.shape))\\nlogger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\\nlogger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\\n\\nif task == 'task1':\\n    df_test = pd.read_csv('data/phase2/sessions_test_task1.csv')\\n    logger.info('df_test Task 1 Phase 2: {}'.format(df_test.shape))\\nelif task == 'task2':\\n    df_test = pd.read_csv('data/phase2/sessions_test_task2.csv')\\n    logger.info('df_test Task 2 Phase 2: {}'.format(df_test.shape))\"], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fb88aa8b280>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fb88867bcd0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7fb88867bcd0>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/home/panda/private/jjw/competition/KDDCUP2023/train_lgb.ipynb', '_i': \"import numpy as np \\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport warnings\\nimport os\\nimport re\\nimport math\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nfrom plotly.subplots import make_subplots\\n\\nfrom lightgbm import LGBMRegressor, LGBMClassifier\\nfrom xgboost import XGBRegressor, XGBClassifier\\nfrom catboost import CatBoostRegressor, CatBoostClassifier\\nimport lightgbm as lgb\\nimport xgboost as xgb\\nimport catboost as cab\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\nfrom sklearn import metrics\\nfrom sklearn.svm import SVC\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom tqdm import tqdm\\n\\nimport matplotlib.pyplot as plt\\nfrom collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings('ignore')\\nfrom sklearn.preprocessing import KBinsDiscretizer\", '_ii': 'df_train_all_exploded_add_last_recall', '_iii': '', '_i1': 'df_train_all_exploded_add_last_recall', '_i2': \"import numpy as np \\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport warnings\\nimport os\\nimport re\\nimport math\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nfrom plotly.subplots import make_subplots\\n\\nfrom lightgbm import LGBMRegressor, LGBMClassifier\\nfrom xgboost import XGBRegressor, XGBClassifier\\nfrom catboost import CatBoostRegressor, CatBoostClassifier\\nimport lightgbm as lgb\\nimport xgboost as xgb\\nimport catboost as cab\\n\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\nfrom sklearn import metrics\\nfrom sklearn.svm import SVC\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom tqdm import tqdm\\n\\nimport matplotlib.pyplot as plt\\nfrom collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings('ignore')\\nfrom sklearn.preprocessing import KBinsDiscretizer\", 'np': <module 'numpy' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/numpy/__init__.py'>, 'pd': <module 'pandas' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/pandas/__init__.py'>, 'plt': <module 'matplotlib.pyplot' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/matplotlib/pyplot.py'>, 'sns': <module 'seaborn' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/seaborn/__init__.py'>, 'warnings': <module 'warnings' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/warnings.py'>, 'os': <module 'os' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/os.py'>, 're': <module 're' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/re.py'>, 'math': <module 'math' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/lib-dynload/math.cpython-39-x86_64-linux-gnu.so'>, 'px': <module 'plotly.express' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/plotly/express/__init__.py'>, 'go': <module 'plotly.graph_objects' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/plotly/graph_objects/__init__.py'>, 'make_subplots': <function make_subplots at 0x7fb7edce9550>, 'LGBMRegressor': <class 'lightgbm.sklearn.LGBMRegressor'>, 'LGBMClassifier': <class 'lightgbm.sklearn.LGBMClassifier'>, 'XGBRegressor': <class 'xgboost.sklearn.XGBRegressor'>, 'XGBClassifier': <class 'xgboost.sklearn.XGBClassifier'>, 'CatBoostRegressor': <class 'catboost.core.CatBoostRegressor'>, 'CatBoostClassifier': <class 'catboost.core.CatBoostClassifier'>, 'lgb': <module 'lightgbm' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/__init__.py'>, 'xgb': <module 'xgboost' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/xgboost/__init__.py'>, 'cab': <module 'catboost' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/catboost/__init__.py'>, 'LabelEncoder': <class 'sklearn.preprocessing._label.LabelEncoder'>, 'cross_val_score': <function cross_val_score at 0x7fb7ed32cf70>, 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'LatentDirichletAllocation': <class 'sklearn.decomposition._lda.LatentDirichletAllocation'>, 'NMF': <class 'sklearn.decomposition._nmf.NMF'>, 'TruncatedSVD': <class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>, 'LogisticRegression': <class 'sklearn.linear_model._logistic.LogisticRegression'>, 'SGDClassifier': <class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>, 'RandomForestRegressor': <class 'sklearn.ensemble._forest.RandomForestRegressor'>, 'AdaBoostRegressor': <class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'HistGradientBoostingRegressor': <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>, 'StackingRegressor': <class 'sklearn.ensemble._stacking.StackingRegressor'>, 'RandomForestClassifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>, 'AdaBoostClassifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>, 'GradientBoostingClassifier': <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>, 'HistGradientBoostingClassifier': <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>, 'StackingClassifier': <class 'sklearn.ensemble._stacking.StackingClassifier'>, 'DecisionTreeRegressor': <class 'sklearn.tree._classes.DecisionTreeRegressor'>, 'DecisionTreeClassifier': <class 'sklearn.tree._classes.DecisionTreeClassifier'>, 'metrics': <module 'sklearn.metrics' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/sklearn/metrics/__init__.py'>, 'SVC': <class 'sklearn.svm._classes.SVC'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'PolynomialFeatures': <class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>, 'KNeighborsClassifier': <class 'sklearn.neighbors._classification.KNeighborsClassifier'>, 'tqdm': <class 'tqdm.std.tqdm'>, 'defaultdict': <class 'collections.defaultdict'>, 'Counter': <class 'collections.Counter'>, 'json': <module 'json' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/json/__init__.py'>, 'pickle': <module 'pickle' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/pickle.py'>, 'KBinsDiscretizer': <class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>, '_i3': \"import time\\nimport numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings('ignore')\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\n\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\nimport argparse\\nfrom utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\\nfrom data import NNDataset, NNDatasetV2, NNDatasetV3\\nfrom model import MatchModel, BaseModel, MatchModelV2, BinaryModel\\nfrom sklearn import metrics\\n\\nseed = 2023\\nFold = 0\\nset_random_seed(seed)\\ntask = 'task1'\\nfeature = 'v3'\\ndense_norm = True\\nrecall = 'window'\\nlen_candidate_set = 10\\nadd_title = False\\nadd_desc = False\\nadd_w2v = False\\nw2v_vector_size = 128\\n\\nmodel_name = 'LGBFold{}'.format(Fold)\\nloc2id = {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}\\n\\nconfig = locals()\\n\\n# 加载必要的数据\\n\\nexp_id = None\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config['exp_id'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info('Exp_id {}'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info('read data')\\n\\ntitles_embedding = np.load('./data/titles_embedding.npy')\\ndescs_embedding = np.load('./data/descs_embedding.npy')\\nlogger.info('titles_embedding: {}'.format(titles_embedding.shape))\\nlogger.info('descs_embedding: {}'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open('data/product2id.json', 'r'))\\nid2product = json.load(open('data/id2product.json', 'r'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info('product2id: {}'.format(len(product2id)))\\nlogger.info('id2product: {}'.format(len(id2product)))\\n\\nword2vec_embedding = np.load('./data/word2vec_embedding.npy')\\nlogger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\\n\\ndf_train_encoded = pd.read_csv('data/df_train_encoded.csv')\\nif task == 'task1':\\n    df_test_encoded = pd.read_csv('data/df_test_encoded_phase2.csv')\\nelif task == 'task2':\\n    df_test_encoded = pd.read_csv('data/df_test_encoded_phase2_onlytask2.csv')\\n\\nif feature.lower() == 'v1':\\n    products_encoded = pd.read_csv('./data/products_encoded.csv')\\n    num_features = ['price', 'len_title', 'len_desc']\\nelif feature.lower() == 'v2':\\n    products_encoded = pd.read_csv('./data/products_encoded_newfeature.csv')\\n    num_features = ['price']\\nelif feature.lower() == 'v3':\\n    products_encoded = pd.read_csv('./data/products_encoded_phase2_V3.csv')\\n    num_features = ['price']\\n\\nlogger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\\nlogger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\\nlogger.info('products_encoded: {}'.format(products_encoded.shape))\\n\\nif dense_norm:\\n    logger.info('MinMaxScaler Norm products_num_feas')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded[fe] = products_encoded[fe].astype('float32')\\n    assert products_encoded[fe].dtypes == 'float32'\\n\\nid_count = products_encoded.shape[0]\\n\\nif task == 'task1':\\n    train_preds_encoded = pickle.load(open('./data/train_preds_{}_one_phase2_noleak_encoded.pkl'.format(recall), 'rb'))  # (len_train, 100)\\n    test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)\\n    logger.info('./data/train_preds_{}_one_phase2_noleak_encoded.pkl'.format(recall))\\n    logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\n    logger.info('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall))\\n    logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\nelif task == 'task2':\\n    train_preds_encoded = pickle.load(open('./data/train_preds_{}_one_onlytask2_phase2_noleak_encoded.pkl'.format(recall), 'rb'))  # (len_train, 100)\\n    test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_onlytask2_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)\\n    logger.info('./data/train_preds_{}_one_onlytask2_phase2_noleak_encoded.pkl'.format(recall))\\n    logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\\n    logger.info('./data/test_preds_{}_one_onlytask2_phase2_encoded.pkl'.format(recall))\\n    logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\\n\\nlogger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\\n# TODO: 可以改成保障一个正样本，补充9个负样本\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded['recall'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded['recall'] = cut_test_preds_encoded\\n\\nlogger.info('Eval the prev_items')\\ndf_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\\ndf_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\\ndf_train_encoded['last_item_2'] = df_train_encoded['prev_items'].apply(lambda x: x[-2])\\ndf_test_encoded['last_item_2'] = df_test_encoded['prev_items'].apply(lambda x: x[-2])\\n\\nlogger.info('Load Hand-made Seq Features')\\n\\nif feature.lower() == 'v1':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_onlytask2.csv')\\n    dense_bins = 10\\nelif feature.lower() == 'v2':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all_newfeature.csv')  # 24维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_newfeature_phase2.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_newfeature_phase2_onlytask2.csv')\\n    dense_bins = 100\\nelif feature.lower() == 'v3':\\n    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all_phase2_V3.csv')  # 36维特征\\n    if task == 'task1':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_V3.csv')\\n    elif task == 'task2':\\n        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_V3_onlytask2.csv')\\n    dense_bins = 100\\n\\nlogger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\\nlogger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f or 'encode_' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\\nlogger.info('seqs_num_feas: {}'.format(seqs_num_feas))\\n\\nif dense_norm:\\n    logger.info('MinMaxScaler Norm seqs_num_feas')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\n    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\n\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\\n    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\\n\\ndf_train_all = pd.concat([df_train_encoded, df_train_seqs_feas_all], axis=1)\\ndf_test_all = pd.concat([df_test_encoded, df_test_seqs_feas_all], axis=1)\\nlogger.info('df_train_all: {}'.format(df_train_all.shape))\\nlogger.info('df_test_all: {}'.format(df_test_all.shape))\\n\\ndf_train_all_exploded = df_train_all.explode('recall')\\ndf_test_all_exploded = df_test_all.explode('recall')\\n\\ndf_train_all_exploded['label'] = df_train_all_exploded['next_item'] == df_train_all_exploded['recall']\\ndf_test_all_exploded['label'] = df_test_all_exploded['next_item'] == df_test_all_exploded['recall']\\n\\ndf_train_all_exploded['index'] = df_train_all_exploded.index \\ndf_test_all_exploded['index'] = df_test_all_exploded.index \\n\\nlogger.info('df_train_all_exploded: {}'.format(df_train_all_exploded.shape))\\nlogger.info('df_test_all_exploded: {}'.format(df_test_all_exploded.shape))\\n\\nlogger.info('df_train_all_exploded.label.sum: {}'.format(df_train_all_exploded['label'].sum()))\\nlogger.info('df_test_all_exploded.label.sum: {}'.format(df_test_all_exploded['label'].sum()))\\n\\ndf_train_encoded_exploded = df_train_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\\ndf_train_seqs_cat_feas = df_train_all_exploded[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_all_exploded[seqs_num_feas]\\ndf_test_encoded_exploded = df_test_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\\ndf_test_seqs_cat_feas = df_test_all_exploded[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_all_exploded[seqs_num_feas]\\n\\nlogger.info('df_train_encoded_exploded: {}'.format(df_train_encoded_exploded.shape))\\nlogger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\\nlogger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\\nlogger.info('df_test_encoded_exploded: {}'.format(df_test_encoded_exploded.shape))\\nlogger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\\nlogger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\\n\\nif task == 'task1':\\n    df_test = pd.read_csv('data/phase2/sessions_test_task1.csv')\\n    logger.info('df_test Task 1 Phase 2: {}'.format(df_test.shape))\\nelif task == 'task2':\\n    df_test = pd.read_csv('data/phase2/sessions_test_task2.csv')\\n    logger.info('df_test Task 2 Phase 2: {}'.format(df_test.shape))\", 'time': <module 'time' (built-in)>, 'MinMaxScaler': <class 'sklearn.preprocessing._data.MinMaxScaler'>, 'random': <module 'random' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/random.py'>, 'torch': <module 'torch' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/__init__.py'>, 'nn': <module 'torch.nn' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/nn/__init__.py'>, 'pack_padded_sequence': <function pack_padded_sequence at 0x7fb6d25c3dc0>, 'pad_packed_sequence': <function pad_packed_sequence at 0x7fb6d25c3e50>, 'random_split': <function random_split at 0x7fb6d1f1b430>, 'DataLoader': <class 'torch.utils.data.dataloader.DataLoader'>, 'Dataset': <class 'torch.utils.data.dataset.Dataset'>, 'argparse': <module 'argparse' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/argparse.py'>, 'set_random_seed': <function set_random_seed at 0x7fb6d1de4af0>, 'get_logger': <function get_logger at 0x7fb6d1de4ca0>, 'ensure_dir': <function ensure_dir at 0x7fb6d1de4c10>, 'str2bool': <function str2bool at 0x7fb6d1de4700>, 'str2float': <function str2float at 0x7fb6d1de4a60>, 'NNDataset': <class 'data.NNDataset'>, 'NNDatasetV2': <class 'data.NNDatasetV2'>, 'NNDatasetV3': <class 'data.NNDatasetV3'>, 'MatchModel': <class 'model.MatchModel'>, 'BaseModel': <class 'model.BaseModel'>, 'MatchModelV2': <class 'model.MatchModelV2'>, 'BinaryModel': <class 'model.BinaryModel'>, 'seed': 2023, 'Fold': 0, 'task': 'task1', 'feature': 'v3', 'dense_norm': True, 'recall': 'window', 'len_candidate_set': 10, 'add_title': False, 'add_desc': False, 'add_w2v': False, 'w2v_vector_size': 128, 'model_name': 'LGBFold0', 'loc2id': {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}, 'config': {...}, 'exp_id': 17581, 'logger': <RootLogger root (INFO)>}\n",
      "2023-06-03 13:45:28,709 - INFO - read data\n",
      "2023-06-03 13:45:39,904 - INFO - titles_embedding: (1410675, 384)\n",
      "2023-06-03 13:45:39,906 - INFO - descs_embedding: (1410675, 384)\n",
      "2023-06-03 13:45:42,214 - INFO - product2id: 1410675\n",
      "2023-06-03 13:45:42,215 - INFO - id2product: 1410675\n",
      "2023-06-03 13:45:45,338 - INFO - word2vec_embedding: (1410675, 128)\n",
      "2023-06-03 13:45:49,264 - INFO - df_train_encoded: (3606249, 4)\n",
      "2023-06-03 13:45:49,266 - INFO - df_test_encoded: (316972, 4)\n",
      "2023-06-03 13:45:49,267 - INFO - products_encoded: (1410675, 10)\n",
      "2023-06-03 13:45:49,268 - INFO - MinMaxScaler Norm products_num_feas\n",
      "2023-06-03 13:46:41,489 - INFO - ./data/train_preds_window_one_phase2_noleak_encoded.pkl\n",
      "2023-06-03 13:46:41,490 - INFO - train_preds_encoded: 3606249\n",
      "2023-06-03 13:46:41,492 - INFO - ./data/test_preds_window_one_phase2_encoded.pkl\n",
      "2023-06-03 13:46:41,492 - INFO - test_preds_encoded: 316972\n",
      "2023-06-03 13:46:41,493 - INFO - Cutting the candidate_set to 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3606249/3606249 [00:19<00:00, 182087.17it/s]\n",
      "100%|██████████| 316972/316972 [00:10<00:00, 30032.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 13:47:12,163 - INFO - Eval the prev_items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 13:48:04,493 - INFO - Load Hand-made Seq Features\n",
      "2023-06-03 13:48:13,551 - INFO - df_train_seqs_feas_all: (3606249, 36)\n",
      "2023-06-03 13:48:13,552 - INFO - df_test_seqs_feas_all: (316972, 36)\n",
      "2023-06-03 13:48:13,553 - INFO - seqs_cat_feas: ['idNUNIQUE', 'idCOUNT', 'encode_brandNUNIQUE', 'encode_brandCOUNT', 'encode_brandMYMODE', 'encode_colorNUNIQUE', 'encode_colorCOUNT', 'encode_colorMYMODE', 'encode_sizeNUNIQUE', 'encode_sizeCOUNT', 'encode_sizeMYMODE', 'encode_modelNUNIQUE', 'encode_modelCOUNT', 'encode_modelMYMODE', 'encode_materialNUNIQUE', 'encode_materialCOUNT', 'encode_materialMYMODE', 'encode_authorNUNIQUE', 'encode_authorCOUNT', 'encode_authorMYMODE', 'encode_priceMEAN', 'encode_priceSTD', 'encode_priceMIN', 'encode_priceMAX', 'encode_priceSUM', 'encode_pricePTP', 'encode_priceQUANTILE75', 'encode_priceQUANTILE25']\n",
      "2023-06-03 13:48:13,553 - INFO - seqs_num_feas: ['priceMEAN', 'priceSTD', 'priceMIN', 'priceMAX', 'priceSUM', 'pricePTP', 'priceQUANTILE75', 'priceQUANTILE25']\n",
      "2023-06-03 13:48:13,554 - INFO - MinMaxScaler Norm seqs_num_feas\n",
      "2023-06-03 13:48:20,359 - INFO - df_train_all: (3606249, 42)\n",
      "2023-06-03 13:48:20,362 - INFO - df_test_all: (316972, 42)\n",
      "2023-06-03 13:49:15,866 - INFO - df_train_all_exploded: (36062490, 44)\n",
      "2023-06-03 13:49:15,870 - INFO - df_test_all_exploded: (3169720, 44)\n",
      "2023-06-03 13:49:15,905 - INFO - df_train_all_exploded.label.sum: 1321887\n",
      "2023-06-03 13:49:15,909 - INFO - df_test_all_exploded.label.sum: 0\n",
      "2023-06-03 13:49:28,402 - INFO - df_train_encoded_exploded: (36062490, 6)\n",
      "2023-06-03 13:49:28,404 - INFO - df_train_seqs_cat_feas: (36062490, 28)\n",
      "2023-06-03 13:49:28,405 - INFO - df_train_seqs_num_feas: (36062490, 8)\n",
      "2023-06-03 13:49:28,406 - INFO - df_test_encoded_exploded: (3169720, 6)\n",
      "2023-06-03 13:49:28,407 - INFO - df_test_seqs_cat_feas: (3169720, 28)\n",
      "2023-06-03 13:49:28,408 - INFO - df_test_seqs_num_feas: (3169720, 8)\n",
      "2023-06-03 13:49:28,750 - INFO - df_test Task 1 Phase 2: (316972, 2)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "# import re\n",
    "# import math\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "# from xgboost import XGBRegressor, XGBClassifier\n",
    "# from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import catboost as cab\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "# from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
    "# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "# from sklearn import metrics\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import json \n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# import sentence_transformers \n",
    "# from sklearn.preprocessing import KBinsDiscretizer\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import argparse\n",
    "from utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\n",
    "from data import NNDataset, NNDatasetV2, NNDatasetV3\n",
    "from model import MatchModel, BaseModel, MatchModelV2, BinaryModel\n",
    "from sklearn import metrics\n",
    "\n",
    "seed = 2023\n",
    "Fold = 0\n",
    "set_random_seed(seed)\n",
    "task = 'task1'\n",
    "feature = 'v3'\n",
    "dense_norm = True\n",
    "recall = 'window'\n",
    "len_candidate_set = 10\n",
    "add_title = False\n",
    "add_desc = False\n",
    "add_w2v = False\n",
    "w2v_vector_size = 128\n",
    "\n",
    "model_name = 'LGBFold{}'.format(Fold)\n",
    "loc2id = {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}\n",
    "\n",
    "config = locals()\n",
    "\n",
    "# 加载必要的数据\n",
    "\n",
    "exp_id = None\n",
    "if exp_id is None:\n",
    "    exp_id = int(random.SystemRandom().random() * 100000)\n",
    "    config['exp_id'] = exp_id\n",
    "\n",
    "logger = get_logger(config)\n",
    "logger.info('Exp_id {}'.format(exp_id))\n",
    "logger.info(config)\n",
    "\n",
    "logger.info('read data')\n",
    "\n",
    "titles_embedding = np.load('./data/titles_embedding.npy')\n",
    "descs_embedding = np.load('./data/descs_embedding.npy')\n",
    "logger.info('titles_embedding: {}'.format(titles_embedding.shape))\n",
    "logger.info('descs_embedding: {}'.format(descs_embedding.shape))\n",
    "\n",
    "product2id = json.load(open('data/product2id.json', 'r'))\n",
    "id2product = json.load(open('data/id2product.json', 'r'))\n",
    "id2product = {int(k): v for k, v in id2product.items()}\n",
    "logger.info('product2id: {}'.format(len(product2id)))\n",
    "logger.info('id2product: {}'.format(len(id2product)))\n",
    "\n",
    "word2vec_embedding = np.load('./data/word2vec_embedding.npy')\n",
    "logger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\n",
    "\n",
    "df_train_encoded = pd.read_csv('data/df_train_encoded.csv')\n",
    "if task == 'task1':\n",
    "    df_test_encoded = pd.read_csv('data/df_test_encoded_phase2.csv')\n",
    "elif task == 'task2':\n",
    "    df_test_encoded = pd.read_csv('data/df_test_encoded_phase2_onlytask2.csv')\n",
    "\n",
    "if feature.lower() == 'v1':\n",
    "    products_encoded = pd.read_csv('./data/products_encoded.csv')\n",
    "    num_features = ['price', 'len_title', 'len_desc']\n",
    "elif feature.lower() == 'v2':\n",
    "    products_encoded = pd.read_csv('./data/products_encoded_newfeature.csv')\n",
    "    num_features = ['price']\n",
    "elif feature.lower() == 'v3':\n",
    "    products_encoded = pd.read_csv('./data/products_encoded_phase2_V3.csv')\n",
    "    num_features = ['price']\n",
    "\n",
    "logger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\n",
    "logger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\n",
    "logger.info('products_encoded: {}'.format(products_encoded.shape))\n",
    "\n",
    "if dense_norm:\n",
    "    logger.info('MinMaxScaler Norm products_num_feas')\n",
    "    mms = MinMaxScaler(feature_range=(0,1))\n",
    "    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\n",
    "for fe in num_features:\n",
    "    products_encoded[fe] = products_encoded[fe].astype('float32')\n",
    "    assert products_encoded[fe].dtypes == 'float32'\n",
    "\n",
    "id_count = products_encoded.shape[0]\n",
    "\n",
    "if task == 'task1':\n",
    "    train_preds_encoded = pickle.load(open('./data/train_preds_{}_one_phase2_noleak_encoded.pkl'.format(recall), 'rb'))  # (len_train, 100)\n",
    "    test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)\n",
    "    logger.info('./data/train_preds_{}_one_phase2_noleak_encoded.pkl'.format(recall))\n",
    "    logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\n",
    "    logger.info('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall))\n",
    "    logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\n",
    "elif task == 'task2':\n",
    "    train_preds_encoded = pickle.load(open('./data/train_preds_{}_one_onlytask2_phase2_noleak_encoded.pkl'.format(recall), 'rb'))  # (len_train, 100)\n",
    "    test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_onlytask2_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)\n",
    "    logger.info('./data/train_preds_{}_one_onlytask2_phase2_noleak_encoded.pkl'.format(recall))\n",
    "    logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\n",
    "    logger.info('./data/test_preds_{}_one_onlytask2_phase2_encoded.pkl'.format(recall))\n",
    "    logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\n",
    "\n",
    "logger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\n",
    "# TODO: 可以改成保障一个正样本，补充9个负样本\n",
    "cut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\n",
    "df_train_encoded['recall'] = cut_train_preds_encoded\n",
    "cut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\n",
    "df_test_encoded['recall'] = cut_test_preds_encoded\n",
    "\n",
    "logger.info('Eval the prev_items')\n",
    "df_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\n",
    "df_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\n",
    "df_train_encoded['last_item_2'] = df_train_encoded['prev_items'].apply(lambda x: x[-2])\n",
    "df_test_encoded['last_item_2'] = df_test_encoded['prev_items'].apply(lambda x: x[-2])\n",
    "\n",
    "logger.info('Load Hand-made Seq Features')\n",
    "\n",
    "if feature.lower() == 'v1':\n",
    "    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\n",
    "    if task == 'task1':\n",
    "        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2.csv')\n",
    "    elif task == 'task2':\n",
    "        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_onlytask2.csv')\n",
    "    dense_bins = 10\n",
    "elif feature.lower() == 'v2':\n",
    "    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all_newfeature.csv')  # 24维特征\n",
    "    if task == 'task1':\n",
    "        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_newfeature_phase2.csv')\n",
    "    elif task == 'task2':\n",
    "        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_newfeature_phase2_onlytask2.csv')\n",
    "    dense_bins = 100\n",
    "elif feature.lower() == 'v3':\n",
    "    df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all_phase2_V3.csv')  # 36维特征\n",
    "    if task == 'task1':\n",
    "        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_V3.csv')\n",
    "    elif task == 'task2':\n",
    "        df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all_phase2_V3_onlytask2.csv')\n",
    "    dense_bins = 100\n",
    "\n",
    "logger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\n",
    "logger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\n",
    "seqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f or 'encode_' in f]\n",
    "seqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\n",
    "logger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\n",
    "logger.info('seqs_num_feas: {}'.format(seqs_num_feas))\n",
    "\n",
    "if dense_norm:\n",
    "    logger.info('MinMaxScaler Norm seqs_num_feas')\n",
    "    mms = MinMaxScaler(feature_range=(0,1))\n",
    "    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\n",
    "    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\n",
    "\n",
    "for fe in seqs_num_feas:\n",
    "    df_train_seqs_feas_all[fe] = df_train_seqs_feas_all[fe].astype('float32')\n",
    "    df_test_seqs_feas_all[fe] = df_test_seqs_feas_all[fe].astype('float32')\n",
    "\n",
    "df_train_all = pd.concat([df_train_encoded, df_train_seqs_feas_all], axis=1)\n",
    "df_test_all = pd.concat([df_test_encoded, df_test_seqs_feas_all], axis=1)\n",
    "logger.info('df_train_all: {}'.format(df_train_all.shape))\n",
    "logger.info('df_test_all: {}'.format(df_test_all.shape))\n",
    "\n",
    "df_train_all_exploded = df_train_all.explode('recall')\n",
    "df_test_all_exploded = df_test_all.explode('recall')\n",
    "\n",
    "df_train_all_exploded['label'] = df_train_all_exploded['next_item'] == df_train_all_exploded['recall']\n",
    "df_test_all_exploded['label'] = df_test_all_exploded['next_item'] == df_test_all_exploded['recall']\n",
    "\n",
    "df_train_all_exploded['index'] = df_train_all_exploded.index \n",
    "df_test_all_exploded['index'] = df_test_all_exploded.index \n",
    "\n",
    "logger.info('df_train_all_exploded: {}'.format(df_train_all_exploded.shape))\n",
    "logger.info('df_test_all_exploded: {}'.format(df_test_all_exploded.shape))\n",
    "\n",
    "logger.info('df_train_all_exploded.label.sum: {}'.format(df_train_all_exploded['label'].sum()))\n",
    "logger.info('df_test_all_exploded.label.sum: {}'.format(df_test_all_exploded['label'].sum()))\n",
    "\n",
    "df_train_encoded_exploded = df_train_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\n",
    "df_train_seqs_cat_feas = df_train_all_exploded[seqs_cat_feas]\n",
    "df_train_seqs_num_feas = df_train_all_exploded[seqs_num_feas]\n",
    "df_test_encoded_exploded = df_test_all_exploded[['prev_items', 'next_item', 'locale', 'recall', 'label', 'index']]\n",
    "df_test_seqs_cat_feas = df_test_all_exploded[seqs_cat_feas]\n",
    "df_test_seqs_num_feas = df_test_all_exploded[seqs_num_feas]\n",
    "\n",
    "logger.info('df_train_encoded_exploded: {}'.format(df_train_encoded_exploded.shape))\n",
    "logger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\n",
    "logger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\n",
    "logger.info('df_test_encoded_exploded: {}'.format(df_test_encoded_exploded.shape))\n",
    "logger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\n",
    "logger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\n",
    "\n",
    "if task == 'task1':\n",
    "    df_test = pd.read_csv('data/phase2/sessions_test_task1.csv')\n",
    "    logger.info('df_test Task 1 Phase 2: {}'.format(df_test.shape))\n",
    "elif task == 'task2':\n",
    "    df_test = pd.read_csv('data/phase2/sessions_test_task2.csv')\n",
    "    logger.info('df_test Task 2 Phase 2: {}'.format(df_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列特征如果是平均值，也很奇怪，因为都是类别特征，现在相当于只考虑last-item的特征，以及召回的recall的特征，预测label\n",
    "# w2v desc title怎么加进去？维度太大了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item</th>\n",
       "      <th>locale</th>\n",
       "      <th>last_item</th>\n",
       "      <th>recall</th>\n",
       "      <th>last_item_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[265193, 83226]</td>\n",
       "      <td>387776</td>\n",
       "      <td>0</td>\n",
       "      <td>83226</td>\n",
       "      <td>[265193, 83226, 218958, 174133, 54056, 236419,...</td>\n",
       "      <td>265193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[38788, 85634, 4132, 71046]</td>\n",
       "      <td>335301</td>\n",
       "      <td>0</td>\n",
       "      <td>71046</td>\n",
       "      <td>[335301, 4132, 484264, 307774, 273700, 239289,...</td>\n",
       "      <td>4132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[191882, 516876, 191882, 516876, 191882, 19188...</td>\n",
       "      <td>90141</td>\n",
       "      <td>0</td>\n",
       "      <td>516876</td>\n",
       "      <td>[191882, 516876, 198906, 360214, 123364, 27777...</td>\n",
       "      <td>191882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[136959, 261145, 31496, 136959, 261145, 31496,...</td>\n",
       "      <td>214540</td>\n",
       "      <td>0</td>\n",
       "      <td>469511</td>\n",
       "      <td>[136959, 31496, 375995, 261145, 219917, 474070...</td>\n",
       "      <td>31496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[291068, 410614, 4219]</td>\n",
       "      <td>338089</td>\n",
       "      <td>0</td>\n",
       "      <td>4219</td>\n",
       "      <td>[355730, 410614, 435967, 210849, 43550, 346031...</td>\n",
       "      <td>410614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          prev_items  next_item  locale  \\\n",
       "0                                    [265193, 83226]     387776       0   \n",
       "1                        [38788, 85634, 4132, 71046]     335301       0   \n",
       "2  [191882, 516876, 191882, 516876, 191882, 19188...      90141       0   \n",
       "3  [136959, 261145, 31496, 136959, 261145, 31496,...     214540       0   \n",
       "4                             [291068, 410614, 4219]     338089       0   \n",
       "\n",
       "   last_item                                             recall  last_item_2  \n",
       "0      83226  [265193, 83226, 218958, 174133, 54056, 236419,...       265193  \n",
       "1      71046  [335301, 4132, 484264, 307774, 273700, 239289,...         4132  \n",
       "2     516876  [191882, 516876, 198906, 360214, 123364, 27777...       191882  \n",
       "3     469511  [136959, 31496, 375995, 261145, 219917, 474070...        31496  \n",
       "4       4219  [355730, 410614, 435967, 210849, 43550, 346031...       410614  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_exploded['recall'] = df_train_all_exploded['recall'].astype('int64')\n",
    "df_test_all_exploded['recall'] = df_test_all_exploded['recall'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_exploded_add_last_recall = pd.merge(df_train_all_exploded, products_encoded.add_suffix('_last_item'), left_on='last_item', right_on='id_last_item')\n",
    "df_test_all_exploded_add_last_recall = pd.merge(df_test_all_exploded, products_encoded.add_suffix('_last_item'), left_on='last_item', right_on='id_last_item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_exploded_add_last_recall.drop('id_last_item', axis=1, inplace=True)\n",
    "df_train_all_exploded_add_last_recall.drop('locale_last_item', axis=1, inplace=True)\n",
    "df_test_all_exploded_add_last_recall.drop('id_last_item', axis=1, inplace=True)\n",
    "df_test_all_exploded_add_last_recall.drop('locale_last_item', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43 + 10 - 2 = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36062490, 52), (3169720, 52))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all_exploded_add_last_recall.shape, df_test_all_exploded_add_last_recall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_exploded_add_last_recall = pd.merge(df_train_all_exploded_add_last_recall, products_encoded.add_suffix('_last_item_2'), left_on='last_item_2', right_on='id_last_item_2')\n",
    "df_test_all_exploded_add_last_recall = pd.merge(df_test_all_exploded_add_last_recall, products_encoded.add_suffix('_last_item_2'), left_on='last_item_2', right_on='id_last_item_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_exploded_add_last_recall.drop('id_last_item_2', axis=1, inplace=True)\n",
    "df_train_all_exploded_add_last_recall.drop('locale_last_item_2', axis=1, inplace=True)\n",
    "df_test_all_exploded_add_last_recall.drop('id_last_item_2', axis=1, inplace=True)\n",
    "df_test_all_exploded_add_last_recall.drop('locale_last_item_2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36062490, 60), (3169720, 60))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all_exploded_add_last_recall.shape, df_test_all_exploded_add_last_recall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_exploded_add_last_recall = pd.merge(df_train_all_exploded_add_last_recall, products_encoded.add_suffix('_recall'), left_on='recall', right_on='id_recall')\n",
    "df_test_all_exploded_add_last_recall = pd.merge(df_test_all_exploded_add_last_recall, products_encoded.add_suffix('_recall'), left_on='recall', right_on='id_recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_exploded_add_last_recall.drop('id_recall', axis=1, inplace=True)\n",
    "df_train_all_exploded_add_last_recall.drop('locale_recall', axis=1, inplace=True)\n",
    "df_test_all_exploded_add_last_recall.drop('id_recall', axis=1, inplace=True)\n",
    "df_test_all_exploded_add_last_recall.drop('locale_recall', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51 + 10 - 2 = 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36062490, 68), (3169720, 68))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all_exploded_add_last_recall.shape, df_test_all_exploded_add_last_recall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_all_exploded_add_last_recall.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prev_items', 'next_item', 'locale', 'last_item', 'recall',\n",
       "       'last_item_2', 'idNUNIQUE', 'idCOUNT', 'encode_brandNUNIQUE',\n",
       "       'encode_brandCOUNT', 'encode_brandMYMODE', 'encode_colorNUNIQUE',\n",
       "       'encode_colorCOUNT', 'encode_colorMYMODE', 'encode_sizeNUNIQUE',\n",
       "       'encode_sizeCOUNT', 'encode_sizeMYMODE', 'encode_modelNUNIQUE',\n",
       "       'encode_modelCOUNT', 'encode_modelMYMODE', 'encode_materialNUNIQUE',\n",
       "       'encode_materialCOUNT', 'encode_materialMYMODE', 'encode_authorNUNIQUE',\n",
       "       'encode_authorCOUNT', 'encode_authorMYMODE', 'priceMEAN', 'priceSTD',\n",
       "       'priceMIN', 'priceMAX', 'priceSUM', 'pricePTP', 'priceQUANTILE75',\n",
       "       'priceQUANTILE25', 'encode_priceMEAN', 'encode_priceSTD',\n",
       "       'encode_priceMIN', 'encode_priceMAX', 'encode_priceSUM',\n",
       "       'encode_pricePTP', 'encode_priceQUANTILE75', 'encode_priceQUANTILE25',\n",
       "       'label', 'index', 'price_last_item', 'encode_brand_last_item',\n",
       "       'encode_color_last_item', 'encode_size_last_item',\n",
       "       'encode_model_last_item', 'encode_material_last_item',\n",
       "       'encode_author_last_item', 'encode_price_last_item',\n",
       "       'price_last_item_2', 'encode_brand_last_item_2',\n",
       "       'encode_color_last_item_2', 'encode_size_last_item_2',\n",
       "       'encode_model_last_item_2', 'encode_material_last_item_2',\n",
       "       'encode_author_last_item_2', 'encode_price_last_item_2', 'price_recall',\n",
       "       'encode_brand_recall', 'encode_color_recall', 'encode_size_recall',\n",
       "       'encode_model_recall', 'encode_material_recall', 'encode_author_recall',\n",
       "       'encode_price_recall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all_exploded_add_last_recall.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prev_items                object\n",
       "next_item                  int64\n",
       "locale                     int64\n",
       "last_item                  int64\n",
       "recall                     int64\n",
       "                           ...  \n",
       "encode_size_recall         int64\n",
       "encode_model_recall        int64\n",
       "encode_material_recall     int64\n",
       "encode_author_recall       int64\n",
       "encode_price_recall        int64\n",
       "Length: 68, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all_exploded_add_last_recall.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:07<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# cross fea\n",
    "for f1 in tqdm(['encode_brandNUNIQUE',\n",
    "                'encode_brandCOUNT',\n",
    "                'encode_colorNUNIQUE',\n",
    "                'encode_colorCOUNT',\n",
    "                'encode_sizeNUNIQUE',\n",
    "                'encode_sizeCOUNT',\n",
    "                'encode_modelNUNIQUE',\n",
    "                'encode_modelCOUNT',\n",
    "                'encode_materialNUNIQUE',\n",
    "                'encode_materialCOUNT',\n",
    "                'encode_authorNUNIQUE',\n",
    "                'encode_authorCOUNT'], total=12):\n",
    "    for f2 in ['idNUNIQUE', 'idCOUNT']:\n",
    "        df_train_all_exploded_add_last_recall[f'{f1}/{f2}'] = df_train_all_exploded_add_last_recall[f1] / df_train_all_exploded_add_last_recall[f2]\n",
    "        df_test_all_exploded_add_last_recall[f'{f1}/{f2}'] = df_test_all_exploded_add_last_recall[f1] / df_test_all_exploded_add_last_recall[f2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# cross fea\n",
    "for f1 in tqdm(['price_recall'], total=1):\n",
    "    for f2 in ['price_last_item', 'price_last_item_2']:\n",
    "        df_train_all_exploded_add_last_recall[f'{f1}-{f2}'] = (df_train_all_exploded_add_last_recall[f1] - df_train_all_exploded_add_last_recall[f2])\n",
    "        df_test_all_exploded_add_last_recall[f'{f1}-{f2}'] = (df_test_all_exploded_add_last_recall[f1] - df_test_all_exploded_add_last_recall[f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# cross fea\n",
    "for f in ['price', 'brand', 'color', 'size', 'model', 'material', 'author']:\n",
    "    for f1 in tqdm(['encode_{}_recall'.format(f)], total=1):\n",
    "        for f2 in ['encode_{}_last_item'.format(f), 'encode_{}_last_item_2'.format(f)]:\n",
    "            df_train_all_exploded_add_last_recall[f'{f1}=={f2}'] = (df_train_all_exploded_add_last_recall[f1] == df_train_all_exploded_add_last_recall[f2]).astype('int')\n",
    "            df_test_all_exploded_add_last_recall[f'{f1}=={f2}'] = (df_test_all_exploded_add_last_recall[f1] == df_test_all_exploded_add_last_recall[f2]).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prev_items',\n",
       " 'next_item',\n",
       " 'locale',\n",
       " 'last_item',\n",
       " 'recall',\n",
       " 'last_item_2',\n",
       " 'idNUNIQUE',\n",
       " 'idCOUNT',\n",
       " 'encode_brandNUNIQUE',\n",
       " 'encode_brandCOUNT',\n",
       " 'encode_brandMYMODE',\n",
       " 'encode_colorNUNIQUE',\n",
       " 'encode_colorCOUNT',\n",
       " 'encode_colorMYMODE',\n",
       " 'encode_sizeNUNIQUE',\n",
       " 'encode_sizeCOUNT',\n",
       " 'encode_sizeMYMODE',\n",
       " 'encode_modelNUNIQUE',\n",
       " 'encode_modelCOUNT',\n",
       " 'encode_modelMYMODE',\n",
       " 'encode_materialNUNIQUE',\n",
       " 'encode_materialCOUNT',\n",
       " 'encode_materialMYMODE',\n",
       " 'encode_authorNUNIQUE',\n",
       " 'encode_authorCOUNT',\n",
       " 'encode_authorMYMODE',\n",
       " 'priceMEAN',\n",
       " 'priceSTD',\n",
       " 'priceMIN',\n",
       " 'priceMAX',\n",
       " 'priceSUM',\n",
       " 'pricePTP',\n",
       " 'priceQUANTILE75',\n",
       " 'priceQUANTILE25',\n",
       " 'encode_priceMEAN',\n",
       " 'encode_priceSTD',\n",
       " 'encode_priceMIN',\n",
       " 'encode_priceMAX',\n",
       " 'encode_priceSUM',\n",
       " 'encode_pricePTP',\n",
       " 'encode_priceQUANTILE75',\n",
       " 'encode_priceQUANTILE25',\n",
       " 'label',\n",
       " 'index',\n",
       " 'price_last_item',\n",
       " 'encode_brand_last_item',\n",
       " 'encode_color_last_item',\n",
       " 'encode_size_last_item',\n",
       " 'encode_model_last_item',\n",
       " 'encode_material_last_item',\n",
       " 'encode_author_last_item',\n",
       " 'encode_price_last_item',\n",
       " 'price_last_item_2',\n",
       " 'encode_brand_last_item_2',\n",
       " 'encode_color_last_item_2',\n",
       " 'encode_size_last_item_2',\n",
       " 'encode_model_last_item_2',\n",
       " 'encode_material_last_item_2',\n",
       " 'encode_author_last_item_2',\n",
       " 'encode_price_last_item_2',\n",
       " 'price_recall',\n",
       " 'encode_brand_recall',\n",
       " 'encode_color_recall',\n",
       " 'encode_size_recall',\n",
       " 'encode_model_recall',\n",
       " 'encode_material_recall',\n",
       " 'encode_author_recall',\n",
       " 'encode_price_recall',\n",
       " 'encode_brandNUNIQUE/idNUNIQUE',\n",
       " 'encode_brandNUNIQUE/idCOUNT',\n",
       " 'encode_brandCOUNT/idNUNIQUE',\n",
       " 'encode_brandCOUNT/idCOUNT',\n",
       " 'encode_colorNUNIQUE/idNUNIQUE',\n",
       " 'encode_colorNUNIQUE/idCOUNT',\n",
       " 'encode_colorCOUNT/idNUNIQUE',\n",
       " 'encode_colorCOUNT/idCOUNT',\n",
       " 'encode_sizeNUNIQUE/idNUNIQUE',\n",
       " 'encode_sizeNUNIQUE/idCOUNT',\n",
       " 'encode_sizeCOUNT/idNUNIQUE',\n",
       " 'encode_sizeCOUNT/idCOUNT',\n",
       " 'encode_modelNUNIQUE/idNUNIQUE',\n",
       " 'encode_modelNUNIQUE/idCOUNT',\n",
       " 'encode_modelCOUNT/idNUNIQUE',\n",
       " 'encode_modelCOUNT/idCOUNT',\n",
       " 'encode_materialNUNIQUE/idNUNIQUE',\n",
       " 'encode_materialNUNIQUE/idCOUNT',\n",
       " 'encode_materialCOUNT/idNUNIQUE',\n",
       " 'encode_materialCOUNT/idCOUNT',\n",
       " 'encode_authorNUNIQUE/idNUNIQUE',\n",
       " 'encode_authorNUNIQUE/idCOUNT',\n",
       " 'encode_authorCOUNT/idNUNIQUE',\n",
       " 'encode_authorCOUNT/idCOUNT',\n",
       " 'price_recall-price_last_item',\n",
       " 'price_recall-price_last_item_2',\n",
       " 'encode_price_recall==encode_price_last_item',\n",
       " 'encode_price_recall==encode_price_last_item_2',\n",
       " 'encode_brand_recall==encode_brand_last_item',\n",
       " 'encode_brand_recall==encode_brand_last_item_2',\n",
       " 'encode_color_recall==encode_color_last_item',\n",
       " 'encode_color_recall==encode_color_last_item_2',\n",
       " 'encode_size_recall==encode_size_last_item',\n",
       " 'encode_size_recall==encode_size_last_item_2',\n",
       " 'encode_model_recall==encode_model_last_item',\n",
       " 'encode_model_recall==encode_model_last_item_2',\n",
       " 'encode_material_recall==encode_material_last_item',\n",
       " 'encode_material_recall==encode_material_last_item_2',\n",
       " 'encode_author_recall==encode_author_last_item',\n",
       " 'encode_author_recall==encode_author_last_item_2']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_train_all_exploded_add_last_recall.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [f for f in df_train_all_exploded_add_last_recall.columns if f not in ['prev_items', 'next_item', 'index', 'label']]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['locale',\n",
       " 'last_item',\n",
       " 'recall',\n",
       " 'last_item_2',\n",
       " 'idNUNIQUE',\n",
       " 'idCOUNT',\n",
       " 'encode_brandNUNIQUE',\n",
       " 'encode_brandCOUNT',\n",
       " 'encode_brandMYMODE',\n",
       " 'encode_colorNUNIQUE',\n",
       " 'encode_colorCOUNT',\n",
       " 'encode_colorMYMODE',\n",
       " 'encode_sizeNUNIQUE',\n",
       " 'encode_sizeCOUNT',\n",
       " 'encode_sizeMYMODE',\n",
       " 'encode_modelNUNIQUE',\n",
       " 'encode_modelCOUNT',\n",
       " 'encode_modelMYMODE',\n",
       " 'encode_materialNUNIQUE',\n",
       " 'encode_materialCOUNT',\n",
       " 'encode_materialMYMODE',\n",
       " 'encode_authorNUNIQUE',\n",
       " 'encode_authorCOUNT',\n",
       " 'encode_authorMYMODE',\n",
       " 'priceMEAN',\n",
       " 'priceSTD',\n",
       " 'priceMIN',\n",
       " 'priceMAX',\n",
       " 'priceSUM',\n",
       " 'pricePTP',\n",
       " 'priceQUANTILE75',\n",
       " 'priceQUANTILE25',\n",
       " 'encode_priceMEAN',\n",
       " 'encode_priceSTD',\n",
       " 'encode_priceMIN',\n",
       " 'encode_priceMAX',\n",
       " 'encode_priceSUM',\n",
       " 'encode_pricePTP',\n",
       " 'encode_priceQUANTILE75',\n",
       " 'encode_priceQUANTILE25',\n",
       " 'price_last_item',\n",
       " 'encode_brand_last_item',\n",
       " 'encode_color_last_item',\n",
       " 'encode_size_last_item',\n",
       " 'encode_model_last_item',\n",
       " 'encode_material_last_item',\n",
       " 'encode_author_last_item',\n",
       " 'encode_price_last_item',\n",
       " 'price_last_item_2',\n",
       " 'encode_brand_last_item_2',\n",
       " 'encode_color_last_item_2',\n",
       " 'encode_size_last_item_2',\n",
       " 'encode_model_last_item_2',\n",
       " 'encode_material_last_item_2',\n",
       " 'encode_author_last_item_2',\n",
       " 'encode_price_last_item_2',\n",
       " 'price_recall',\n",
       " 'encode_brand_recall',\n",
       " 'encode_color_recall',\n",
       " 'encode_size_recall',\n",
       " 'encode_model_recall',\n",
       " 'encode_material_recall',\n",
       " 'encode_author_recall',\n",
       " 'encode_price_recall',\n",
       " 'encode_brandNUNIQUE/idNUNIQUE',\n",
       " 'encode_brandNUNIQUE/idCOUNT',\n",
       " 'encode_brandCOUNT/idNUNIQUE',\n",
       " 'encode_brandCOUNT/idCOUNT',\n",
       " 'encode_colorNUNIQUE/idNUNIQUE',\n",
       " 'encode_colorNUNIQUE/idCOUNT',\n",
       " 'encode_colorCOUNT/idNUNIQUE',\n",
       " 'encode_colorCOUNT/idCOUNT',\n",
       " 'encode_sizeNUNIQUE/idNUNIQUE',\n",
       " 'encode_sizeNUNIQUE/idCOUNT',\n",
       " 'encode_sizeCOUNT/idNUNIQUE',\n",
       " 'encode_sizeCOUNT/idCOUNT',\n",
       " 'encode_modelNUNIQUE/idNUNIQUE',\n",
       " 'encode_modelNUNIQUE/idCOUNT',\n",
       " 'encode_modelCOUNT/idNUNIQUE',\n",
       " 'encode_modelCOUNT/idCOUNT',\n",
       " 'encode_materialNUNIQUE/idNUNIQUE',\n",
       " 'encode_materialNUNIQUE/idCOUNT',\n",
       " 'encode_materialCOUNT/idNUNIQUE',\n",
       " 'encode_materialCOUNT/idCOUNT',\n",
       " 'encode_authorNUNIQUE/idNUNIQUE',\n",
       " 'encode_authorNUNIQUE/idCOUNT',\n",
       " 'encode_authorCOUNT/idNUNIQUE',\n",
       " 'encode_authorCOUNT/idCOUNT',\n",
       " 'price_recall-price_last_item',\n",
       " 'price_recall-price_last_item_2',\n",
       " 'encode_price_recall==encode_price_last_item',\n",
       " 'encode_price_recall==encode_price_last_item_2',\n",
       " 'encode_brand_recall==encode_brand_last_item',\n",
       " 'encode_brand_recall==encode_brand_last_item_2',\n",
       " 'encode_color_recall==encode_color_last_item',\n",
       " 'encode_color_recall==encode_color_last_item_2',\n",
       " 'encode_size_recall==encode_size_last_item',\n",
       " 'encode_size_recall==encode_size_last_item_2',\n",
       " 'encode_model_recall==encode_model_last_item',\n",
       " 'encode_model_recall==encode_model_last_item_2',\n",
       " 'encode_material_recall==encode_material_last_item',\n",
       " 'encode_material_recall==encode_material_last_item_2',\n",
       " 'encode_author_recall==encode_author_last_item',\n",
       " 'encode_author_recall==encode_author_last_item_2']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "l = df_train_all_exploded_add_last_recall.shape[1]\n",
    "# shuffled_df_train_all_exploded_add_last_recall.to_csv('./data/shuffled_df_train_all_exploded_add_last_recall_features_{}.csv'.format(l), index=False)\n",
    "df_train_all_exploded_add_last_recall.to_csv('./data/df_train_all_exploded_add_last_recall_{}.csv'.format(l), index=False)\n",
    "df_test_all_exploded_add_last_recall.to_csv('./data/df_test_all_exploded_add_last_recall_{}.csv'.format(l), index=False)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 108\n",
    "shuffled_df_train_all_exploded_add_last_recall = pd.read_csv('./data/shuffled_df_train_all_exploded_add_last_recall_features_{}.csv'.format(l))\n",
    "df_train_all_exploded_add_last_recall = pd.read_csv('./data/df_train_all_exploded_add_last_recall_{}.csv'.format(l))\n",
    "df_test_all_exploded_add_last_recall = pd.read_csv('./data/df_test_all_exploded_add_last_recall_{}.csv'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36062490, 108)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_all_exploded_add_last_recall.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [f for f in df_train_all_exploded_add_last_recall.columns if f not in ['prev_items', 'next_item', 'index', 'label']]\n",
    "len(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "kfold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_index = np.random.permutation(df_train_all_exploded_add_last_recall.index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_df_train_all_exploded_add_last_recall = df_train_all_exploded_add_last_recall.loc[shuffled_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_df_train_all_exploded_add_last_recall.to_csv('./data/shuffled_df_train_all_exploded_add_last_recall_features_{}.csv'.format(l), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = [f for f in df_train_all_exploded_add_last_recall.columns if f not in ['prev_items', 'next_item', 'index', 'label']]\n",
    "# len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['priceMEAN', 'priceSTD', 'priceMIN', 'priceMAX', 'priceSUM', 'pricePTP', 'priceQUANTILE75', 'priceQUANTILE25', 'price_last_item', 'price_last_item_2', 'price_recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [f for f in features if f not in num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = shuffled_df_train_all_exploded_add_last_recall[cat_features]\n",
    "# test = df_test_all_exploded_add_last_recall[cat_features]\n",
    "# y = shuffled_df_train_all_exploded_add_last_recall['label'].astype('int64')\n",
    "\n",
    "# train.shape, test.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36062490, 104), (3169720, 104), (36062490,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = shuffled_df_train_all_exploded_add_last_recall[features]\n",
    "test = df_test_all_exploded_add_last_recall[features]\n",
    "y = shuffled_df_train_all_exploded_add_last_recall['label'].astype('int64')\n",
    "\n",
    "train.shape, test.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36062490,), (36062490,), (36062490,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_label = shuffled_df_train_all_exploded_add_last_recall['next_item']\n",
    "candidate = shuffled_df_train_all_exploded_add_last_recall['recall']\n",
    "index_list = shuffled_df_train_all_exploded_add_last_recall['index']\n",
    "origin_label.shape, candidate.shape, index_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_label_index(group):\n",
    "    label = group['val_origin_label'].iloc[0]\n",
    "    candidate = group['val_candidate'].tolist()\n",
    "    try:\n",
    "        index = candidate.index(label)\n",
    "        rrank = 1 / (index + 1)\n",
    "    except:\n",
    "        rrank = 0\n",
    "    return pd.Series({'label_rrank': rrank})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_label_index(group):\n",
    "    label = group['train_origin_label'].iloc[0]\n",
    "    candidate = group['train_candidate'].tolist()\n",
    "    try:\n",
    "        index = candidate.index(label)\n",
    "        rrank = 1 / (index + 1)\n",
    "    except:\n",
    "        rrank = 0\n",
    "    return pd.Series({'label_rrank': rrank})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 2 ** 6,\n",
    "    'max_depth': 8,\n",
    "    'tree_learner': 'serial',\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    'subsample': 0.8,\n",
    "    'num_boost_round': 1000,\n",
    "    'max_bin': 255,\n",
    "    'verbose': -1,\n",
    "    'nthread' : -1,\n",
    "    'seed': seed,\n",
    "    'bagging_seed': seed,\n",
    "    'feature_fraction_seed': seed,\n",
    "    'early_stopping_rounds': 100,\n",
    "    # 'device': 'gpu',  # 设置使用 GPU 加速\n",
    "    # 'gpu_platform_id': 0,  # 设置 GPU 平台 id\n",
    "    # 'gpu_device_id': 0  # 设置 GPU 设备 id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_candidate_set = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7212490\n",
      "7212490 14424980\n",
      "14424980 21637470\n",
      "21637470 28849960\n",
      "28849960 36062450\n",
      "Fold 1:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n",
      "Fold 2:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n",
      "Fold 3:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n",
      "Fold 4:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n",
      "Fold 5:\n",
      "Validation set: 7212490\n",
      "Training set: 28849960\n"
     ]
    }
   ],
   "source": [
    "fold_size = int(df_train_all_exploded_add_last_recall.shape[0] / len_candidate_set // kfold) * len_candidate_set  # 7212490\n",
    "index_list_fold = list(range(0, len(df_train_all_exploded_add_last_recall)))\n",
    "val_indexes = []\n",
    "start = 0\n",
    "for i in range(kfold):\n",
    "    end = min(start + fold_size, len(index_list_fold))\n",
    "    print(start, end)\n",
    "    val_indexes.append(index_list_fold[start:end])\n",
    "    # print(np.min(val_indexes[-1]), np.max(val_indexes[-1]), len(val_indexes[-1]))\n",
    "    start = end\n",
    "train_indexes = []\n",
    "for i, fold in enumerate(val_indexes):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(\"Validation set:\", len(fold))\n",
    "    train_indexes.append([index for sublist in val_indexes[:i] + val_indexes[i+1:] for index in sublist])\n",
    "    print(\"Training set:\", len(train_indexes[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "fold n°0\n",
      "28849960 7212490\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttraining's auc: 0.840402\tvalid_1's auc: 0.840657\n",
      "[100]\ttraining's auc: 0.849153\tvalid_1's auc: 0.849284\n",
      "[150]\ttraining's auc: 0.854011\tvalid_1's auc: 0.853974\n",
      "[200]\ttraining's auc: 0.856712\tvalid_1's auc: 0.856491\n",
      "[250]\ttraining's auc: 0.858759\tvalid_1's auc: 0.858373\n",
      "[300]\ttraining's auc: 0.860335\tvalid_1's auc: 0.859769\n",
      "[350]\ttraining's auc: 0.86161\tvalid_1's auc: 0.860855\n",
      "[400]\ttraining's auc: 0.862713\tvalid_1's auc: 0.861773\n",
      "[450]\ttraining's auc: 0.863762\tvalid_1's auc: 0.862636\n",
      "[500]\ttraining's auc: 0.8646\tvalid_1's auc: 0.863281\n",
      "[550]\ttraining's auc: 0.865293\tvalid_1's auc: 0.863782\n",
      "[600]\ttraining's auc: 0.865933\tvalid_1's auc: 0.864201\n",
      "[650]\ttraining's auc: 0.866577\tvalid_1's auc: 0.864655\n",
      "[700]\ttraining's auc: 0.867215\tvalid_1's auc: 0.865098\n",
      "[750]\ttraining's auc: 0.867864\tvalid_1's auc: 0.865534\n",
      "[800]\ttraining's auc: 0.868392\tvalid_1's auc: 0.865857\n",
      "[850]\ttraining's auc: 0.868948\tvalid_1's auc: 0.866213\n",
      "[900]\ttraining's auc: 0.869467\tvalid_1's auc: 0.866546\n",
      "[950]\ttraining's auc: 0.86998\tvalid_1's auc: 0.86686\n",
      "[1000]\ttraining's auc: 0.870504\tvalid_1's auc: 0.867181\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.870504\tvalid_1's auc: 0.867181\n",
      "Calculating MRR!\n",
      "Fold 0, Val MRR 0.0722719040277573.\n",
      "fold n°1\n",
      "28849960 7212490\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttraining's auc: 0.84007\tvalid_1's auc: 0.840032\n",
      "[100]\ttraining's auc: 0.848677\tvalid_1's auc: 0.848512\n",
      "[150]\ttraining's auc: 0.854034\tvalid_1's auc: 0.853678\n",
      "[200]\ttraining's auc: 0.856945\tvalid_1's auc: 0.856441\n",
      "[250]\ttraining's auc: 0.858798\tvalid_1's auc: 0.858123\n",
      "[300]\ttraining's auc: 0.860385\tvalid_1's auc: 0.85956\n",
      "[350]\ttraining's auc: 0.861678\tvalid_1's auc: 0.860678\n",
      "[400]\ttraining's auc: 0.862821\tvalid_1's auc: 0.861649\n",
      "[450]\ttraining's auc: 0.863815\tvalid_1's auc: 0.862462\n",
      "[500]\ttraining's auc: 0.864675\tvalid_1's auc: 0.863133\n",
      "[550]\ttraining's auc: 0.865445\tvalid_1's auc: 0.863716\n",
      "[600]\ttraining's auc: 0.866172\tvalid_1's auc: 0.864255\n",
      "[650]\ttraining's auc: 0.86682\tvalid_1's auc: 0.864695\n",
      "[700]\ttraining's auc: 0.867479\tvalid_1's auc: 0.865175\n",
      "[750]\ttraining's auc: 0.868037\tvalid_1's auc: 0.865531\n",
      "[800]\ttraining's auc: 0.868595\tvalid_1's auc: 0.865896\n",
      "[850]\ttraining's auc: 0.869151\tvalid_1's auc: 0.866265\n",
      "[900]\ttraining's auc: 0.869691\tvalid_1's auc: 0.866613\n",
      "[950]\ttraining's auc: 0.870245\tvalid_1's auc: 0.866963\n",
      "[1000]\ttraining's auc: 0.870763\tvalid_1's auc: 0.867295\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.870763\tvalid_1's auc: 0.867295\n",
      "Calculating MRR!\n",
      "Fold 1, Val MRR 0.07196271864119.\n",
      "fold n°2\n",
      "28849960 7212490\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttraining's auc: 0.839932\tvalid_1's auc: 0.839544\n",
      "[100]\ttraining's auc: 0.848842\tvalid_1's auc: 0.848197\n",
      "[150]\ttraining's auc: 0.854131\tvalid_1's auc: 0.853327\n",
      "[200]\ttraining's auc: 0.857121\tvalid_1's auc: 0.856176\n",
      "[250]\ttraining's auc: 0.859007\tvalid_1's auc: 0.85789\n",
      "[300]\ttraining's auc: 0.86075\tvalid_1's auc: 0.85947\n",
      "[350]\ttraining's auc: 0.862007\tvalid_1's auc: 0.860554\n",
      "[400]\ttraining's auc: 0.863079\tvalid_1's auc: 0.861435\n",
      "[450]\ttraining's auc: 0.86405\tvalid_1's auc: 0.862212\n",
      "[500]\ttraining's auc: 0.864868\tvalid_1's auc: 0.862822\n",
      "[550]\ttraining's auc: 0.865668\tvalid_1's auc: 0.863412\n",
      "[600]\ttraining's auc: 0.866365\tvalid_1's auc: 0.863906\n",
      "[650]\ttraining's auc: 0.867032\tvalid_1's auc: 0.864378\n",
      "[700]\ttraining's auc: 0.867683\tvalid_1's auc: 0.864825\n",
      "[750]\ttraining's auc: 0.868297\tvalid_1's auc: 0.865235\n",
      "[800]\ttraining's auc: 0.868851\tvalid_1's auc: 0.865587\n",
      "[850]\ttraining's auc: 0.869428\tvalid_1's auc: 0.865968\n",
      "[900]\ttraining's auc: 0.869948\tvalid_1's auc: 0.866286\n",
      "[950]\ttraining's auc: 0.870435\tvalid_1's auc: 0.866566\n",
      "[1000]\ttraining's auc: 0.87091\tvalid_1's auc: 0.866845\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.87091\tvalid_1's auc: 0.866845\n",
      "Calculating MRR!\n",
      "Fold 2, Val MRR 0.07198367612840877.\n",
      "fold n°3\n",
      "28849960 7212490\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttraining's auc: 0.839516\tvalid_1's auc: 0.839105\n",
      "[100]\ttraining's auc: 0.848729\tvalid_1's auc: 0.848144\n",
      "[150]\ttraining's auc: 0.854197\tvalid_1's auc: 0.853444\n",
      "[200]\ttraining's auc: 0.857144\tvalid_1's auc: 0.856209\n",
      "[250]\ttraining's auc: 0.859206\tvalid_1's auc: 0.858088\n",
      "[300]\ttraining's auc: 0.860823\tvalid_1's auc: 0.859508\n",
      "[350]\ttraining's auc: 0.862036\tvalid_1's auc: 0.860543\n",
      "[400]\ttraining's auc: 0.863123\tvalid_1's auc: 0.861434\n",
      "[450]\ttraining's auc: 0.864\tvalid_1's auc: 0.862083\n",
      "[500]\ttraining's auc: 0.864811\tvalid_1's auc: 0.862688\n",
      "[550]\ttraining's auc: 0.865649\tvalid_1's auc: 0.863337\n",
      "[600]\ttraining's auc: 0.86631\tvalid_1's auc: 0.863797\n",
      "[650]\ttraining's auc: 0.866973\tvalid_1's auc: 0.864265\n",
      "[700]\ttraining's auc: 0.867633\tvalid_1's auc: 0.864714\n",
      "[750]\ttraining's auc: 0.868235\tvalid_1's auc: 0.865113\n",
      "[800]\ttraining's auc: 0.868763\tvalid_1's auc: 0.865425\n",
      "[850]\ttraining's auc: 0.869324\tvalid_1's auc: 0.865796\n",
      "[900]\ttraining's auc: 0.869873\tvalid_1's auc: 0.866142\n",
      "[950]\ttraining's auc: 0.87042\tvalid_1's auc: 0.866489\n",
      "[1000]\ttraining's auc: 0.870911\tvalid_1's auc: 0.866792\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.870911\tvalid_1's auc: 0.866792\n",
      "Calculating MRR!\n",
      "Fold 3, Val MRR 0.07189351990332941.\n",
      "fold n°4\n",
      "28849960 7212490\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttraining's auc: 0.839475\tvalid_1's auc: 0.839002\n",
      "[100]\ttraining's auc: 0.848733\tvalid_1's auc: 0.848158\n",
      "[150]\ttraining's auc: 0.853908\tvalid_1's auc: 0.85322\n",
      "[200]\ttraining's auc: 0.856817\tvalid_1's auc: 0.855998\n",
      "[250]\ttraining's auc: 0.858965\tvalid_1's auc: 0.858005\n",
      "[300]\ttraining's auc: 0.860439\tvalid_1's auc: 0.859298\n",
      "[350]\ttraining's auc: 0.861768\tvalid_1's auc: 0.860455\n",
      "[400]\ttraining's auc: 0.862956\tvalid_1's auc: 0.861468\n",
      "[450]\ttraining's auc: 0.863794\tvalid_1's auc: 0.862134\n",
      "[500]\ttraining's auc: 0.864693\tvalid_1's auc: 0.862875\n",
      "[550]\ttraining's auc: 0.865427\tvalid_1's auc: 0.86342\n",
      "[600]\ttraining's auc: 0.866094\tvalid_1's auc: 0.863883\n",
      "[650]\ttraining's auc: 0.866791\tvalid_1's auc: 0.86439\n",
      "[700]\ttraining's auc: 0.867437\tvalid_1's auc: 0.864856\n",
      "[750]\ttraining's auc: 0.868018\tvalid_1's auc: 0.865254\n",
      "[800]\ttraining's auc: 0.868571\tvalid_1's auc: 0.865633\n",
      "[850]\ttraining's auc: 0.86906\tvalid_1's auc: 0.865923\n",
      "[900]\ttraining's auc: 0.869597\tvalid_1's auc: 0.866259\n",
      "[950]\ttraining's auc: 0.870042\tvalid_1's auc: 0.866515\n",
      "[1000]\ttraining's auc: 0.870546\tvalid_1's auc: 0.866828\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.870546\tvalid_1's auc: 0.866828\n",
      "Calculating MRR!\n",
      "Fold 4, Val MRR 0.07197747956347149.\n",
      "AUC score: 0.8669871197240945\n",
      "F1 score: 0.09522273992840502\n",
      "Precision score: 0.7791545104785373\n",
      "Recall score: 0.05071008338836829\n"
     ]
    }
   ],
   "source": [
    "feat_imp_lgb = list()\n",
    "\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros((len(test)))\n",
    "print(train.shape[1])\n",
    "\n",
    "# 模型训练\n",
    "for fold_ in range(kfold):\n",
    "    trn_idx = train_indexes[fold_]\n",
    "    val_idx = val_indexes[fold_]\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    print(len(trn_idx), len(val_idx))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx], label=y.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx], label=y.iloc[val_idx])\n",
    "    \n",
    "    num_round = 1000\n",
    "    clf = lgb.train(\n",
    "        lgb_params,\n",
    "        trn_data,\n",
    "        num_round,\n",
    "        valid_sets=[trn_data, val_data],\n",
    "        verbose_eval=50,\n",
    "        early_stopping_rounds=100,\n",
    "    )\n",
    "\n",
    "    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    predictions_lgb[:] += clf.predict(test, num_iteration=clf.best_iteration) / kfold\n",
    "    feat_imp_lgb.append(clf.feature_importance())\n",
    "\n",
    "    # print('Calculating MRR!')\n",
    "    # df = pd.DataFrame({\n",
    "    #         'val_candidate': candidate.iloc[val_idx].values,  # recall\n",
    "    #         'val_index': index_list.iloc[val_idx].values,  # recall\n",
    "    #         'val_origin_label': origin_label.iloc[val_idx].values,  # next-item\n",
    "    #         'val_y_pred': oof_lgb[val_idx]\n",
    "    #     })\n",
    "    # df_sorted = df.sort_values(['val_index', 'val_y_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "    # df_grouped = df_sorted.groupby('val_index').apply(get_val_label_index)\n",
    "    # val_mrr = np.mean(df_grouped['label_rrank'].values)\n",
    "    # print('Fold {}, Val MRR {}.'.format(fold_, val_mrr))\n",
    "\n",
    "print(\"AUC score: {}\".format(metrics.roc_auc_score(y, oof_lgb)))\n",
    "print(\"F1 score: {}\".format(metrics.f1_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))\n",
    "print(\"Precision score: {}\".format(metrics.precision_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))\n",
    "print(\"Recall score: {}\".format(metrics.recall_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.8102795953390152\n",
      "F1 score: 0.002675196277987787\n",
      "Precision score: 0.8322368421052632\n",
      "Recall score: 0.0013397514310981196\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC score: {}\".format(metrics.roc_auc_score(y, oof_lgb)))\n",
    "print(\"F1 score: {}\".format(metrics.f1_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))\n",
    "print(\"Precision score: {}\".format(metrics.precision_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))\n",
    "print(\"Recall score: {}\".format(metrics.recall_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logloss = metrics.log_loss(y, oof_lgb)\n",
    "train_auc = metrics.roc_auc_score(y, oof_lgb)\n",
    "train_precision = metrics.precision_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])\n",
    "train_recall = metrics.recall_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])\n",
    "train_f1 = metrics.f1_score(y, [1 if i >= 0.5 else 0 for i in oof_lgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Logloss: 0.1184, AUC: 0.8670, Precision: 0.7792, Recall: 0.0507, F1 Score: 0.0952\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Logloss: {:.4f}, AUC: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\".format(train_logloss, train_auc, train_precision, train_recall, train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3169720,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lgb.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'train_candidate': candidate,\n",
    "    'train_index': index_list,\n",
    "    'train_origin_label': origin_label,\n",
    "    'train_y_pred': oof_lgb\n",
    "})\n",
    "\n",
    "df_sorted = df.sort_values(['train_index', 'train_y_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "df_grouped = df_sorted.groupby('train_index').apply(get_train_label_index)\n",
    "train_mrr = np.mean(df_grouped['label_rrank'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23230824719174525"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'test_candidate': df_test_all_exploded_add_last_recall['recall'].values,\n",
    "    'test_index': df_test_all_exploded_add_last_recall['index'].values,\n",
    "    'test_y_pred': predictions_lgb\n",
    "})\n",
    "\n",
    "df_sorted = df.sort_values(['test_index', 'test_y_pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "df_test_preds = df_sorted.groupby('test_index').apply(lambda group : group['test_candidate'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_index\n",
       "0         [364726, 375093, 277862, 296535, 448389, 30854...\n",
       "1         [381461, 381596, 79751, 121801, 440377, 336532...\n",
       "2         [283983, 243200, 54937, 257230, 330806, 80419,...\n",
       "3         [170646, 152287, 60178, 230918, 37043, 236653,...\n",
       "4         [391069, 423381, 399374, 261807, 4017, 303141,...\n",
       "                                ...                        \n",
       "316967    [973160, 985530, 1188024, 1178666, 1308289, 97...\n",
       "316968    [339779, 221172, 255018, 175976, 941877, 27658...\n",
       "316969    [320241, 111293, 40228, 490183, 262850, 422393...\n",
       "316970    [134356, 1073088, 180752, 1252507, 387691, 438...\n",
       "316971    [438197, 491322, 202352, 490567, 66015, 458107...\n",
       "Length: 316972, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = 'window'\n",
    "test_preds_encoded = pickle.load(open('./data/test_preds_{}_one_phase2_encoded.pkl'.format(recall), 'rb'))  # (len_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "product2id = json.load(open('data/product2id.json', 'r'))\n",
    "id2product = json.load(open('data/id2product.json', 'r'))\n",
    "id2product = {int(k): v for k, v in id2product.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/phase2/sessions_test_task1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316972/316972 [00:10<00:00, 28995.77it/s]\n"
     ]
    }
   ],
   "source": [
    "test_res_unencoded = []\n",
    "for ind, x in tqdm(enumerate(range(len(test_preds_encoded))), total=len(test_preds_encoded)):\n",
    "    x = df_test_preds.iloc[ind] + test_preds_encoded[ind][len_candidate_set:]\n",
    "    assert len(x) == 100\n",
    "    x_unencoded = [id2product[id_] for id_ in x]\n",
    "    test_res_unencoded.append(x_unencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    316972.0\n",
       "mean        100.0\n",
       "std           0.0\n",
       "min         100.0\n",
       "25%         100.0\n",
       "50%         100.0\n",
       "75%         100.0\n",
       "max         100.0\n",
       "Name: next_item_prediction, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['next_item_prediction'] = test_res_unencoded\n",
    "df_test['next_item_prediction'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0B9GJLV2D, B093X59B31, B07SDFLVKD, B0BGC82WV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0024NKBQE, B084CB7GX9, B08XW4W667, B004P4QFJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09Z3FBXMB, B09Z4T2GJ3, B09Z4PZQBF, B09Z4PYG8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07T6Y2HG7, B07Y1KLF25, B07T5XY2CJ, B07HQ83TF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0B2JY9THB, B01MRXVY2O, B09B2WLRWX, B08SXLWXH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00V6FIFZ0, B07GKP2LCF, B07GKYSHB4, B09QMK82R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00M35Y326, B01LX5Y7RG, B00NVMIO02, B085C7TCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08VD6TCP9, B08VDGMBGP, B08VDHH6QF, B08VD5DC5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08DTYFYGP, B089CZWB4C, B08W2JJZBM, B08T1ZJYH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07H48412Q, B07H9J1YXN, B08GY1QYXP, B08GYG6T1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B0B9GJLV2D, B093X59B31, B07SDFLVKD, B0BGC82WV...\n",
       "1          DE  [B0024NKBQE, B084CB7GX9, B08XW4W667, B004P4QFJ...\n",
       "2          DE  [B09Z3FBXMB, B09Z4T2GJ3, B09Z4PZQBF, B09Z4PYG8...\n",
       "3          DE  [B07T6Y2HG7, B07Y1KLF25, B07T5XY2CJ, B07HQ83TF...\n",
       "4          DE  [B0B2JY9THB, B01MRXVY2O, B09B2WLRWX, B08SXLWXH...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B00V6FIFZ0, B07GKP2LCF, B07GKYSHB4, B09QMK82R...\n",
       "316968     UK  [B00M35Y326, B01LX5Y7RG, B00NVMIO02, B085C7TCT...\n",
       "316969     UK  [B08VD6TCP9, B08VDGMBGP, B08VDHH6QF, B08VD5DC5...\n",
       "316970     UK  [B08DTYFYGP, B089CZWB4C, B08W2JJZBM, B08T1ZJYH...\n",
       "316971     UK  [B07H48412Q, B07H9J1YXN, B08GY1QYXP, B08GYG6T1...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[['locale', 'next_item_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_parquet('output/2023_32615_MatchModelV2withATTMatchFold0_99.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07SDFLVKD, B091CK241X, B093X59B31, B0BGC82WV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B08HQWQ1SK, B084CB7GX9, B08B66B3GB, B08XW4W66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09Z4PZQBF, B09Z3FBXMB, B09Z4PYG8Q, B09Z4T2GJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07HQ83TFF, B07T6Y2HG7, B07Y1KLF25, B07T2NBLX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0B2JY9THB, B0B2DRKZ6X, B01MRXVY2O, B08YK8FQJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07GKP2LCF, B07GKM97YF, B07GKYSHB4, B00V6FIFZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00NVMIO02, B00M35Y326, B07BMZKCTV, B01LX5Y7R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08VD5DC5L, B08VDHH6QF, B08VDGMBGP, B0BK7QC4H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B0B7M72LFQ, B089CZWB4C, B08W2JJZBM, B08DTYFYG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08GYG6T12, B07H48412Q, B09ZS1PHV4, B07H9J1YX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B07SDFLVKD, B091CK241X, B093X59B31, B0BGC82WV...\n",
       "1          DE  [B08HQWQ1SK, B084CB7GX9, B08B66B3GB, B08XW4W66...\n",
       "2          DE  [B09Z4PZQBF, B09Z3FBXMB, B09Z4PYG8Q, B09Z4T2GJ...\n",
       "3          DE  [B07HQ83TFF, B07T6Y2HG7, B07Y1KLF25, B07T2NBLX...\n",
       "4          DE  [B0B2JY9THB, B0B2DRKZ6X, B01MRXVY2O, B08YK8FQJ...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B07GKP2LCF, B07GKM97YF, B07GKYSHB4, B00V6FIFZ...\n",
       "316968     UK  [B00NVMIO02, B00M35Y326, B07BMZKCTV, B01LX5Y7R...\n",
       "316969     UK  [B08VD5DC5L, B08VDHH6QF, B08VDGMBGP, B0BK7QC4H...\n",
       "316970     UK  [B0B7M72LFQ, B089CZWB4C, B08W2JJZBM, B08DTYFYG...\n",
       "316971     UK  [B08GYG6T12, B07H48412Q, B09ZS1PHV4, B07H9J1YX...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = '17581'\n",
    "Fold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LGBFold{}'.format(Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['locale', 'next_item_prediction']].to_parquet('output/{}_{}_{}.parquet'.format(seed, exp_id, model_name), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3169720,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lgb.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./output/{}_{}_{}_predictions_lgb.npy'.format(seed, exp_id, model_name), predictions_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./output/{}_{}_{}_oof_lgb.npy'.format(seed, exp_id, model_name), oof_lgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "111"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libcityng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
