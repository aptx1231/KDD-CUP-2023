{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-17 11:04:08,115 - INFO - Log directory: ./log\n",
      "2023-05-17 11:04:08,116 - INFO - Exp_id 85406\n",
      "2023-05-17 11:04:08,116 - INFO - {'__name__': '__main__', '__doc__': '\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n# TODO: product-id作为特征之一\\n# TODO: emb_dim变大 不同特征用不同emb_dim等\\n# TODO: DIN等序列模型\\n', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', 'import numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\n\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\nimport argparse\\nfrom utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\\nfrom data import NNDataset, NNDatasetV2\\nfrom model import MatchModel, BaseModel, MatchModelV2\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n# TODO: product-id作为特征之一\\n# TODO: emb_dim变大 不同特征用不同emb_dim等\\n# TODO: DIN等序列模型\\n\"\"\"\\n\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.1\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 100\\ndevice = torch.device(\\'cuda:3\\')\\ndense_norm = True\\nnum_workers = 0\\n\\nload_init = True\\n\\nload_exp_id = [\\n    93545,\\n    43234,\\n    84653,\\n    29020,\\n]\\nload_epoch = [\\n    52,\\n    51,\\n    53,\\n    50\\n]\\nload_Fold = [\\n    0,\\n    1,\\n    2,\\n    3\\n]\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'MatchModelV2withATTMatch\\'\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()\\n\\n# 加载必要的数据\\n\\nexp_id = config.get(\\'exp_id\\', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config[\\'exp_id\\'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info(\\'Exp_id {}\\'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info(\\'load_exp_id = {}\\'.format(load_exp_id))\\nlogger.info(\\'load_epoch = {}\\'.format(load_epoch))\\n\\nlogger.info(\\'read data\\')\\n\\ntitles_embedding = np.load(\\'./data/titles_embedding.npy\\')\\ndescs_embedding = np.load(\\'./data/descs_embedding.npy\\')\\nlogger.info(\\'titles_embedding: {}\\'.format(titles_embedding.shape))\\nlogger.info(\\'descs_embedding: {}\\'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open(\\'data/product2id.json\\', \\'r\\'))\\nid2product = json.load(open(\\'data/id2product.json\\', \\'r\\'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info(\\'product2id: {}\\'.format(len(product2id)))\\nlogger.info(\\'id2product: {}\\'.format(len(id2product)))\\n\\nword2vec_embedding = np.load(\\'./data/word2vec_embedding.npy\\')\\nlogger.info(\\'word2vec_embedding: {}\\'.format(word2vec_embedding.shape))\\n\\ntop200 = pickle.load(open(\\'data/top200_new.pkl\\', \\'rb\\'))\\n\\ndf_train_encoded = pd.read_csv(\\'data/df_train_encoded.csv\\')\\ndf_test_encoded = pd.read_csv(\\'data/df_test_encoded.csv\\')\\nproducts_encoded = pd.read_csv(\\'./data/products_encoded.csv\\')\\nlogger.info(\\'df_train_encoded: {}\\'.format(df_train_encoded.shape))\\nlogger.info(\\'df_test_encoded: {}\\'.format(df_test_encoded.shape))\\nlogger.info(\\'products_encoded: {}\\'.format(products_encoded.shape))\\n\\nnum_features = [\\'price\\', \\'len_title\\', \\'len_desc\\']\\nif dense_norm:\\n    logger.info(\\'MinMaxScaler Norm products_num_feas\\')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded.loc[:, fe] = products_encoded.loc[:, fe].astype(\\'float32\\')\\n\\nlogger.info(\\'Load Hand-made Seq Features\\')\\ndf_train_seqs_feas_all = pd.read_csv(\\'data/df_train_seqs_feas_all.csv\\')  # 29维特征\\ndf_test_seqs_feas_all = pd.read_csv(\\'data/df_test_seqs_feas_all.csv\\')\\nlogger.info(\\'df_train_seqs_feas_all: {}\\'.format(df_train_seqs_feas_all.shape))\\nlogger.info(\\'df_test_seqs_feas_all: {}\\'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if \\'NUNIQUE\\' in f or \\'COUNT\\' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info(\\'seqs_cat_feas: {}\\'.format(seqs_cat_feas))\\nlogger.info(\\'seqs_num_feas: {}\\'.format(seqs_num_feas))\\n\\nif dense_norm:\\n    logger.info(\\'MinMaxScaler Norm seqs_num_feas\\')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\n    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all.loc[:, fe] = df_train_seqs_feas_all.loc[:, fe].astype(\\'float32\\')\\n    df_test_seqs_feas_all.loc[:, fe] = df_test_seqs_feas_all.loc[:, fe].astype(\\'float32\\')\\n\\ndf_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\\ndf_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\\nlogger.info(\\'df_train_seqs_cat_feas: {}\\'.format(df_train_seqs_cat_feas.shape))\\nlogger.info(\\'df_train_seqs_num_feas: {}\\'.format(df_train_seqs_num_feas.shape))\\nlogger.info(\\'df_test_seqs_cat_feas: {}\\'.format(df_test_seqs_cat_feas.shape))\\nlogger.info(\\'df_test_seqs_num_feas: {}\\'.format(df_test_seqs_num_feas.shape))\\n\\nid_count = products_encoded.shape[0]\\n\\ntrain_preds_encoded = pickle.load(open(\\'./data/train_preds_all_encoded_new.pkl\\', \\'rb\\'))  # (len_train, 100)\\ntest_preds_encoded = pickle.load(open(\\'./data/test_preds_all_encoded_new.pkl\\', \\'rb\\'))  # (len_test, 100)\\ntest_preds = pickle.load(open(\\'./data/test_preds_all.pkl\\', \\'rb\\'))\\nlogger.info(\\'train_preds_encoded: {}\\'.format(len(train_preds_encoded)))\\nlogger.info(\\'test_preds_encoded: {}\\'.format(len(test_preds_encoded)))\\nlogger.info(\\'test_preds: {}\\'.format(len(test_preds)))\\n\\nlogger.info(\\'Cutting the candidate_set to {}\\'.format(len_candidate_set))\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded[\\'recall\\'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded[\\'recall\\'] = cut_test_preds_encoded\\n\\nlogger.info(\\'Eval the prev_items\\')\\ndf_train_encoded[\\'prev_items\\'] = df_train_encoded[\\'prev_items\\'].apply(eval)\\ndf_test_encoded[\\'prev_items\\'] = df_test_encoded[\\'prev_items\\'].apply(eval)\\n\\ndf_test = pd.read_csv(\\'data/sessions_test_task1.csv\\')\\nlogger.info(\\'df_test: {}\\'.format(df_test.shape))\\n\\ntmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\\ntmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\\n\\ndata_feature = {}\\ndata_feature[\\'len_encode_brand\\'] = products_encoded[\\'encode_brand\\'].nunique()\\ndata_feature[\\'len_encode_color\\'] = products_encoded[\\'encode_color\\'].nunique()\\ndata_feature[\\'len_encode_size\\'] = products_encoded[\\'encode_size\\'].nunique()\\ndata_feature[\\'len_encode_model\\'] = products_encoded[\\'encode_model\\'].nunique()\\ndata_feature[\\'len_encode_material\\'] = products_encoded[\\'encode_material\\'].nunique()\\ndata_feature[\\'len_encode_author\\'] = products_encoded[\\'encode_author\\'].nunique()\\ndata_feature[\\'len_locale\\'] = len(loc2id)\\ndata_feature[\\'dense_bins\\'] = dense_bins\\ndata_feature[\\'id_count\\'] = id_count\\ndata_feature[\\'len_features\\'] = products_encoded.shape[1] - 1\\ndata_feature[\\'len_emb_features\\'] = 3\\ndata_feature[\\'len_candidate_set\\'] = len_candidate_set\\ndata_feature[\\'w2v_vector_size\\'] = w2v_vector_size\\ndata_feature[\\'sentence_vector_size\\'] = 384\\ndata_feature[\\'len_seqs_cat_feas\\'] = len(seqs_cat_feas)\\ndata_feature[\\'len_seqs_num_feas\\'] = len(seqs_num_feas)\\ndata_feature[\\'seq_emb_factor\\'] = seq_emb_factor\\ndata_feature.update(tmp_nunique)\\nlogger.info(\\'data_feature:\\')\\nlogger.info(data_feature)\\n\\ndel tmp\\n\\n# 加载模型等\\n\\nlogger.info(\\'create model\\')\\n\\nproducts_input = {name: torch.tensor(products_encoded[name].values).to(device) for name in products_encoded.columns}\\n\\nif \\'BaseModel\\' in model_name:\\n    model = BaseModel(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\\nelif \\'MatchModel\\' in model_name:\\n    model = MatchModelV2(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\\nelse:\\n    raise ValueError(\\'Error model name {}\\'.format(model_name))\\nlogger.info(model)\\n\\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\\n\\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=\\'max\\', patience=lr_patience, factor=lr_decay_ratio)\\n\\nfor name, param in model.named_parameters():\\n    logger.info(str(name) + \\'\\\\t\\' + str(param.shape) + \\'\\\\t\\' +\\n                              str(param.device) + \\'\\\\t\\' + str(param.requires_grad))\\ntotal_num = sum([param.nelement() for param in model.parameters()])\\nlogger.info(\\'Total parameter numbers: {}\\'.format(total_num))\\n\\n\\n# 数据集DataLoader\\ntrain_set = NNDatasetV2(df_train_encoded, df_train_seqs_cat_feas, \\n                       df_train_seqs_num_feas)\\nlogger.info(\\'train_set: {}\\'.format(len(train_set)))\\ntest_set = NNDatasetV2(df_test_encoded, df_test_seqs_cat_feas, \\n                       df_test_seqs_num_feas)\\nlogger.info(\\'test_set: {}\\'.format(len(test_set)))\\n\\n\\ndef collate_fn(indices):\\n    batch_prev_items = []\\n    batch_locale = []\\n    batch_candidate_set = []\\n    batch_len = []\\n    batch_mask = []\\n    batch_label = []\\n    batch_label_index = []  # 交叉熵需要的是label在候选集中的index\\n    batch_seq_cat = []\\n    batch_seq_num = []\\n    for item in indices:\\n        batch_len.append(len(item[0]))  # prev_items\\n    max_len = max(batch_len)\\n    for item in indices:\\n        l = len(item[0])\\n        batch_mask.append([1] * (l) + [0] * (max_len - l))  # 0代表padding的位置，需要mask\\n    for item in indices:\\n        # [\\'prev_items\\', \\'locale\\', \\'recall\\', \\'next_item\\', \\'seqs_cat_feas\\', \\'seqs_num_feas\\']\\n        prev_items = item[0].copy()\\n        while (len(prev_items) < max_len):\\n            prev_items.append(id_count)  # embdding的时候id_count+1，把id_count作为padding了\\n        batch_prev_items.append(prev_items)\\n        batch_locale.append(item[1])\\n        batch_candidate_set.append(item[2].copy())\\n        batch_label.append(item[3])\\n        if item[3] in item[2]:\\n            batch_label_index.append(item[2].index(item[3]))\\n        else:\\n            batch_label_index.append(len(item[2]))\\n        batch_seq_cat.append(item[4])\\n        batch_seq_num.append(item[5])\\n    return [torch.LongTensor(batch_prev_items).to(device), torch.LongTensor(batch_locale).to(device), \\n            torch.LongTensor(batch_candidate_set).to(device),\\n            torch.LongTensor(batch_len).to(device), torch.LongTensor(batch_mask).to(device), \\n            torch.LongTensor(batch_label).to(device), torch.LongTensor(batch_label_index).to(device),\\n            torch.LongTensor(batch_seq_cat).to(device), torch.FloatTensor(batch_seq_num).to(device)]\\n\\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\\nlogger.info(\\'train_loader: {}\\'.format(len(train_loader)))\\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\\nlogger.info(\\'test_loader: {}\\'.format(len(test_loader)))\\n\\noutput_dir = \\'ckpt/{}\\'.format(exp_id)\\nensure_dir(output_dir)\\n\\nif load_init:\\n    assert len(load_exp_id) == len(load_epoch)\\n\\n\\nlogger.info(\\'Testing...\\')\\ntest_scores_all = []\\ntest_res_all = []\\nfor index, exp_id_index in enumerate(load_exp_id):\\n    logger.info(\\'Index {}\\'.format(index))\\n    epoch_index = load_epoch[index]\\n    fold_index = load_Fold[index]\\n    model_name_index = \\'MatchModelV2withATTMatchFold{}\\'.format(fold_index)\\n\\n    load_dir = \\'ckpt/{}\\'.format(exp_id_index)\\n    load_path = \\'{}/{}_{}_{}.pt\\'.format(load_dir, exp_id_index, model_name_index, epoch_index)\\n    logger.info(\\'Load Init model from {}\\'.format(load_path))\\n    model.load_state_dict(torch.load(load_path, map_location=\\'cpu\\'))\\n    model.to(device)\\n\\n    # 开始评估\\n    test_scores = []\\n    test_res = []\\n    model.eval()\\n    for batch_prev_items, batch_locale, batch_candidate_set, batch_len, batch_mask, \\\\\\n            _, batch_label_index, batch_seq_cat, batch_seq_num in tqdm(test_loader, desc=\\'test model {}\\'.format(exp_id), total=len(test_loader)):\\n        score, _ = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \\n                                        batch_candidate_set=batch_candidate_set, batch_len=batch_len, \\n                                        batch_label=batch_label_index, batch_mask=batch_mask,\\n                                        batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)  # (batch_size, 10)\\n        test_scores.append(score.detach().cpu().numpy())\\n        sorted_indices = torch.argsort(score, dim=1, descending=True)\\n        sorted_candidate_set = batch_candidate_set.gather(dim=1, index=sorted_indices)  # (B, 10)\\n        test_res.append(sorted_candidate_set.detach().cpu().numpy())\\n    test_scores = np.concatenate(test_scores, axis=0)  # (N, 10)\\n    test_res = np.concatenate(test_res, axis=0)\\n    test_scores_all.append(test_scores)\\n    test_res_all.append(test_res_all)'], '_oh': {}, '_dh': ['/home/panda/private/jjw/competition/KDDCUP2023'], 'In': ['', 'import numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\n\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\nimport argparse\\nfrom utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\\nfrom data import NNDataset, NNDatasetV2\\nfrom model import MatchModel, BaseModel, MatchModelV2\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n# TODO: product-id作为特征之一\\n# TODO: emb_dim变大 不同特征用不同emb_dim等\\n# TODO: DIN等序列模型\\n\"\"\"\\n\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.1\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 100\\ndevice = torch.device(\\'cuda:3\\')\\ndense_norm = True\\nnum_workers = 0\\n\\nload_init = True\\n\\nload_exp_id = [\\n    93545,\\n    43234,\\n    84653,\\n    29020,\\n]\\nload_epoch = [\\n    52,\\n    51,\\n    53,\\n    50\\n]\\nload_Fold = [\\n    0,\\n    1,\\n    2,\\n    3\\n]\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'MatchModelV2withATTMatch\\'\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()\\n\\n# 加载必要的数据\\n\\nexp_id = config.get(\\'exp_id\\', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config[\\'exp_id\\'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info(\\'Exp_id {}\\'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info(\\'load_exp_id = {}\\'.format(load_exp_id))\\nlogger.info(\\'load_epoch = {}\\'.format(load_epoch))\\n\\nlogger.info(\\'read data\\')\\n\\ntitles_embedding = np.load(\\'./data/titles_embedding.npy\\')\\ndescs_embedding = np.load(\\'./data/descs_embedding.npy\\')\\nlogger.info(\\'titles_embedding: {}\\'.format(titles_embedding.shape))\\nlogger.info(\\'descs_embedding: {}\\'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open(\\'data/product2id.json\\', \\'r\\'))\\nid2product = json.load(open(\\'data/id2product.json\\', \\'r\\'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info(\\'product2id: {}\\'.format(len(product2id)))\\nlogger.info(\\'id2product: {}\\'.format(len(id2product)))\\n\\nword2vec_embedding = np.load(\\'./data/word2vec_embedding.npy\\')\\nlogger.info(\\'word2vec_embedding: {}\\'.format(word2vec_embedding.shape))\\n\\ntop200 = pickle.load(open(\\'data/top200_new.pkl\\', \\'rb\\'))\\n\\ndf_train_encoded = pd.read_csv(\\'data/df_train_encoded.csv\\')\\ndf_test_encoded = pd.read_csv(\\'data/df_test_encoded.csv\\')\\nproducts_encoded = pd.read_csv(\\'./data/products_encoded.csv\\')\\nlogger.info(\\'df_train_encoded: {}\\'.format(df_train_encoded.shape))\\nlogger.info(\\'df_test_encoded: {}\\'.format(df_test_encoded.shape))\\nlogger.info(\\'products_encoded: {}\\'.format(products_encoded.shape))\\n\\nnum_features = [\\'price\\', \\'len_title\\', \\'len_desc\\']\\nif dense_norm:\\n    logger.info(\\'MinMaxScaler Norm products_num_feas\\')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded.loc[:, fe] = products_encoded.loc[:, fe].astype(\\'float32\\')\\n\\nlogger.info(\\'Load Hand-made Seq Features\\')\\ndf_train_seqs_feas_all = pd.read_csv(\\'data/df_train_seqs_feas_all.csv\\')  # 29维特征\\ndf_test_seqs_feas_all = pd.read_csv(\\'data/df_test_seqs_feas_all.csv\\')\\nlogger.info(\\'df_train_seqs_feas_all: {}\\'.format(df_train_seqs_feas_all.shape))\\nlogger.info(\\'df_test_seqs_feas_all: {}\\'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if \\'NUNIQUE\\' in f or \\'COUNT\\' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info(\\'seqs_cat_feas: {}\\'.format(seqs_cat_feas))\\nlogger.info(\\'seqs_num_feas: {}\\'.format(seqs_num_feas))\\n\\nif dense_norm:\\n    logger.info(\\'MinMaxScaler Norm seqs_num_feas\\')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\n    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all.loc[:, fe] = df_train_seqs_feas_all.loc[:, fe].astype(\\'float32\\')\\n    df_test_seqs_feas_all.loc[:, fe] = df_test_seqs_feas_all.loc[:, fe].astype(\\'float32\\')\\n\\ndf_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\\ndf_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\\nlogger.info(\\'df_train_seqs_cat_feas: {}\\'.format(df_train_seqs_cat_feas.shape))\\nlogger.info(\\'df_train_seqs_num_feas: {}\\'.format(df_train_seqs_num_feas.shape))\\nlogger.info(\\'df_test_seqs_cat_feas: {}\\'.format(df_test_seqs_cat_feas.shape))\\nlogger.info(\\'df_test_seqs_num_feas: {}\\'.format(df_test_seqs_num_feas.shape))\\n\\nid_count = products_encoded.shape[0]\\n\\ntrain_preds_encoded = pickle.load(open(\\'./data/train_preds_all_encoded_new.pkl\\', \\'rb\\'))  # (len_train, 100)\\ntest_preds_encoded = pickle.load(open(\\'./data/test_preds_all_encoded_new.pkl\\', \\'rb\\'))  # (len_test, 100)\\ntest_preds = pickle.load(open(\\'./data/test_preds_all.pkl\\', \\'rb\\'))\\nlogger.info(\\'train_preds_encoded: {}\\'.format(len(train_preds_encoded)))\\nlogger.info(\\'test_preds_encoded: {}\\'.format(len(test_preds_encoded)))\\nlogger.info(\\'test_preds: {}\\'.format(len(test_preds)))\\n\\nlogger.info(\\'Cutting the candidate_set to {}\\'.format(len_candidate_set))\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded[\\'recall\\'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded[\\'recall\\'] = cut_test_preds_encoded\\n\\nlogger.info(\\'Eval the prev_items\\')\\ndf_train_encoded[\\'prev_items\\'] = df_train_encoded[\\'prev_items\\'].apply(eval)\\ndf_test_encoded[\\'prev_items\\'] = df_test_encoded[\\'prev_items\\'].apply(eval)\\n\\ndf_test = pd.read_csv(\\'data/sessions_test_task1.csv\\')\\nlogger.info(\\'df_test: {}\\'.format(df_test.shape))\\n\\ntmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\\ntmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\\n\\ndata_feature = {}\\ndata_feature[\\'len_encode_brand\\'] = products_encoded[\\'encode_brand\\'].nunique()\\ndata_feature[\\'len_encode_color\\'] = products_encoded[\\'encode_color\\'].nunique()\\ndata_feature[\\'len_encode_size\\'] = products_encoded[\\'encode_size\\'].nunique()\\ndata_feature[\\'len_encode_model\\'] = products_encoded[\\'encode_model\\'].nunique()\\ndata_feature[\\'len_encode_material\\'] = products_encoded[\\'encode_material\\'].nunique()\\ndata_feature[\\'len_encode_author\\'] = products_encoded[\\'encode_author\\'].nunique()\\ndata_feature[\\'len_locale\\'] = len(loc2id)\\ndata_feature[\\'dense_bins\\'] = dense_bins\\ndata_feature[\\'id_count\\'] = id_count\\ndata_feature[\\'len_features\\'] = products_encoded.shape[1] - 1\\ndata_feature[\\'len_emb_features\\'] = 3\\ndata_feature[\\'len_candidate_set\\'] = len_candidate_set\\ndata_feature[\\'w2v_vector_size\\'] = w2v_vector_size\\ndata_feature[\\'sentence_vector_size\\'] = 384\\ndata_feature[\\'len_seqs_cat_feas\\'] = len(seqs_cat_feas)\\ndata_feature[\\'len_seqs_num_feas\\'] = len(seqs_num_feas)\\ndata_feature[\\'seq_emb_factor\\'] = seq_emb_factor\\ndata_feature.update(tmp_nunique)\\nlogger.info(\\'data_feature:\\')\\nlogger.info(data_feature)\\n\\ndel tmp\\n\\n# 加载模型等\\n\\nlogger.info(\\'create model\\')\\n\\nproducts_input = {name: torch.tensor(products_encoded[name].values).to(device) for name in products_encoded.columns}\\n\\nif \\'BaseModel\\' in model_name:\\n    model = BaseModel(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\\nelif \\'MatchModel\\' in model_name:\\n    model = MatchModelV2(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\\nelse:\\n    raise ValueError(\\'Error model name {}\\'.format(model_name))\\nlogger.info(model)\\n\\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\\n\\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=\\'max\\', patience=lr_patience, factor=lr_decay_ratio)\\n\\nfor name, param in model.named_parameters():\\n    logger.info(str(name) + \\'\\\\t\\' + str(param.shape) + \\'\\\\t\\' +\\n                              str(param.device) + \\'\\\\t\\' + str(param.requires_grad))\\ntotal_num = sum([param.nelement() for param in model.parameters()])\\nlogger.info(\\'Total parameter numbers: {}\\'.format(total_num))\\n\\n\\n# 数据集DataLoader\\ntrain_set = NNDatasetV2(df_train_encoded, df_train_seqs_cat_feas, \\n                       df_train_seqs_num_feas)\\nlogger.info(\\'train_set: {}\\'.format(len(train_set)))\\ntest_set = NNDatasetV2(df_test_encoded, df_test_seqs_cat_feas, \\n                       df_test_seqs_num_feas)\\nlogger.info(\\'test_set: {}\\'.format(len(test_set)))\\n\\n\\ndef collate_fn(indices):\\n    batch_prev_items = []\\n    batch_locale = []\\n    batch_candidate_set = []\\n    batch_len = []\\n    batch_mask = []\\n    batch_label = []\\n    batch_label_index = []  # 交叉熵需要的是label在候选集中的index\\n    batch_seq_cat = []\\n    batch_seq_num = []\\n    for item in indices:\\n        batch_len.append(len(item[0]))  # prev_items\\n    max_len = max(batch_len)\\n    for item in indices:\\n        l = len(item[0])\\n        batch_mask.append([1] * (l) + [0] * (max_len - l))  # 0代表padding的位置，需要mask\\n    for item in indices:\\n        # [\\'prev_items\\', \\'locale\\', \\'recall\\', \\'next_item\\', \\'seqs_cat_feas\\', \\'seqs_num_feas\\']\\n        prev_items = item[0].copy()\\n        while (len(prev_items) < max_len):\\n            prev_items.append(id_count)  # embdding的时候id_count+1，把id_count作为padding了\\n        batch_prev_items.append(prev_items)\\n        batch_locale.append(item[1])\\n        batch_candidate_set.append(item[2].copy())\\n        batch_label.append(item[3])\\n        if item[3] in item[2]:\\n            batch_label_index.append(item[2].index(item[3]))\\n        else:\\n            batch_label_index.append(len(item[2]))\\n        batch_seq_cat.append(item[4])\\n        batch_seq_num.append(item[5])\\n    return [torch.LongTensor(batch_prev_items).to(device), torch.LongTensor(batch_locale).to(device), \\n            torch.LongTensor(batch_candidate_set).to(device),\\n            torch.LongTensor(batch_len).to(device), torch.LongTensor(batch_mask).to(device), \\n            torch.LongTensor(batch_label).to(device), torch.LongTensor(batch_label_index).to(device),\\n            torch.LongTensor(batch_seq_cat).to(device), torch.FloatTensor(batch_seq_num).to(device)]\\n\\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\\nlogger.info(\\'train_loader: {}\\'.format(len(train_loader)))\\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\\nlogger.info(\\'test_loader: {}\\'.format(len(test_loader)))\\n\\noutput_dir = \\'ckpt/{}\\'.format(exp_id)\\nensure_dir(output_dir)\\n\\nif load_init:\\n    assert len(load_exp_id) == len(load_epoch)\\n\\n\\nlogger.info(\\'Testing...\\')\\ntest_scores_all = []\\ntest_res_all = []\\nfor index, exp_id_index in enumerate(load_exp_id):\\n    logger.info(\\'Index {}\\'.format(index))\\n    epoch_index = load_epoch[index]\\n    fold_index = load_Fold[index]\\n    model_name_index = \\'MatchModelV2withATTMatchFold{}\\'.format(fold_index)\\n\\n    load_dir = \\'ckpt/{}\\'.format(exp_id_index)\\n    load_path = \\'{}/{}_{}_{}.pt\\'.format(load_dir, exp_id_index, model_name_index, epoch_index)\\n    logger.info(\\'Load Init model from {}\\'.format(load_path))\\n    model.load_state_dict(torch.load(load_path, map_location=\\'cpu\\'))\\n    model.to(device)\\n\\n    # 开始评估\\n    test_scores = []\\n    test_res = []\\n    model.eval()\\n    for batch_prev_items, batch_locale, batch_candidate_set, batch_len, batch_mask, \\\\\\n            _, batch_label_index, batch_seq_cat, batch_seq_num in tqdm(test_loader, desc=\\'test model {}\\'.format(exp_id), total=len(test_loader)):\\n        score, _ = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \\n                                        batch_candidate_set=batch_candidate_set, batch_len=batch_len, \\n                                        batch_label=batch_label_index, batch_mask=batch_mask,\\n                                        batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)  # (batch_size, 10)\\n        test_scores.append(score.detach().cpu().numpy())\\n        sorted_indices = torch.argsort(score, dim=1, descending=True)\\n        sorted_candidate_set = batch_candidate_set.gather(dim=1, index=sorted_indices)  # (B, 10)\\n        test_res.append(sorted_candidate_set.detach().cpu().numpy())\\n    test_scores = np.concatenate(test_scores, axis=0)  # (N, 10)\\n    test_res = np.concatenate(test_res, axis=0)\\n    test_scores_all.append(test_scores)\\n    test_res_all.append(test_res_all)'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f5395e19280>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f5393a0ad60>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f5393a0ad60>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/home/panda/private/jjw/competition/KDDCUP2023/testMerge.ipynb', '_i': '', '_ii': '', '_iii': '', '_i1': 'import numpy as np \\nimport pandas as pd\\n# import matplotlib.pyplot as plt\\n# import seaborn as sns\\n# import os\\n# import re\\n# import math\\n# import plotly.express as px\\n# import plotly.graph_objects as go\\n# from plotly.subplots import make_subplots\\n\\n# from lightgbm import LGBMRegressor, LGBMClassifier\\n# from xgboost import XGBRegressor, XGBClassifier\\n# from catboost import CatBoostRegressor, CatBoostClassifier\\n# import lightgbm as lgb\\n# import xgboost as xgb\\n# import catboost as cab\\n\\n# from sklearn.preprocessing import LabelEncoder\\n# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\\n# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\\n# from sklearn.linear_model import LogisticRegression, SGDClassifier\\n# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\\n# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\\n# from sklearn import metrics\\n# from sklearn.svm import SVC\\n# from sklearn.feature_extraction.text import TfidfVectorizer\\n# from sklearn.preprocessing import PolynomialFeatures\\n# from sklearn.neighbors import KNeighborsClassifier\\n# from sklearn.model_selection import train_test_split\\n\\nimport matplotlib.pyplot as plt\\n# from collections import defaultdict, Counter\\nimport warnings\\nimport json \\nimport pickle\\nwarnings.filterwarnings(\\'ignore\\')\\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\\nimport pickle\\nimport random\\nfrom tqdm import tqdm\\n# import sentence_transformers \\n# from sklearn.preprocessing import KBinsDiscretizer\\n# from sentence_transformers import SentenceTransformer\\n\\n# from gensim.models import Word2Vec\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torch.utils.data import Dataset\\nimport argparse\\nfrom utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\\nfrom data import NNDataset, NNDatasetV2\\nfrom model import MatchModel, BaseModel, MatchModelV2\\n\\n\"\"\"\\n相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\\nSeqFeatureEmbedding现在使用2层全连接，可以换多层！\\n# TODO: product-id作为特征之一\\n# TODO: emb_dim变大 不同特征用不同emb_dim等\\n# TODO: DIN等序列模型\\n\"\"\"\\n\\n\\nemb_dim = 16\\ndense_bins = 10\\nhid_dim = 256\\ndropout = 0.1\\nlayers = 4\\nbidirectional = False\\nseq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\\n\\nbatch_size = 1024\\nepochs = 100\\nlen_candidate_set = 100\\ndevice = torch.device(\\'cuda:3\\')\\ndense_norm = True\\nnum_workers = 0\\n\\nload_init = True\\n\\nload_exp_id = [\\n    93545,\\n    43234,\\n    84653,\\n    29020,\\n]\\nload_epoch = [\\n    52,\\n    51,\\n    53,\\n    50\\n]\\nload_Fold = [\\n    0,\\n    1,\\n    2,\\n    3\\n]\\n\\n# TODO: 调整batch-size， 调整hidden-size\\n# TODO: 跑Fold 4\\nlearning_rate = 0.001\\nweight_decay = 0.00001\\nearly_stop_lr = 1e-6\\nlr_patience = 5\\nlr_decay_ratio = 0.1\\nclip = 5\\nlog_every = 100\\nearly_stop = True\\npatience = 10\\nkfold = 5\\nattn_match = True \\n\\nw2v_window = 3\\nw2v_min_count = 1\\nw2v_epochs = 500\\nw2v_vector_size = 128\\n\\nseed = 2023\\nset_random_seed(seed)\\n\\nmodel_name = \\'MatchModelV2withATTMatch\\'\\nloc2id = {\\'DE\\': 0, \\'JP\\': 1, \\'UK\\': 2, \\'ES\\': 3, \\'FR\\': 4, \\'IT\\': 5}\\n\\nconfig = locals()\\n\\n# 加载必要的数据\\n\\nexp_id = config.get(\\'exp_id\\', None)\\nif exp_id is None:\\n    exp_id = int(random.SystemRandom().random() * 100000)\\n    config[\\'exp_id\\'] = exp_id\\n\\nlogger = get_logger(config)\\nlogger.info(\\'Exp_id {}\\'.format(exp_id))\\nlogger.info(config)\\n\\nlogger.info(\\'load_exp_id = {}\\'.format(load_exp_id))\\nlogger.info(\\'load_epoch = {}\\'.format(load_epoch))\\n\\nlogger.info(\\'read data\\')\\n\\ntitles_embedding = np.load(\\'./data/titles_embedding.npy\\')\\ndescs_embedding = np.load(\\'./data/descs_embedding.npy\\')\\nlogger.info(\\'titles_embedding: {}\\'.format(titles_embedding.shape))\\nlogger.info(\\'descs_embedding: {}\\'.format(descs_embedding.shape))\\n\\nproduct2id = json.load(open(\\'data/product2id.json\\', \\'r\\'))\\nid2product = json.load(open(\\'data/id2product.json\\', \\'r\\'))\\nid2product = {int(k): v for k, v in id2product.items()}\\nlogger.info(\\'product2id: {}\\'.format(len(product2id)))\\nlogger.info(\\'id2product: {}\\'.format(len(id2product)))\\n\\nword2vec_embedding = np.load(\\'./data/word2vec_embedding.npy\\')\\nlogger.info(\\'word2vec_embedding: {}\\'.format(word2vec_embedding.shape))\\n\\ntop200 = pickle.load(open(\\'data/top200_new.pkl\\', \\'rb\\'))\\n\\ndf_train_encoded = pd.read_csv(\\'data/df_train_encoded.csv\\')\\ndf_test_encoded = pd.read_csv(\\'data/df_test_encoded.csv\\')\\nproducts_encoded = pd.read_csv(\\'./data/products_encoded.csv\\')\\nlogger.info(\\'df_train_encoded: {}\\'.format(df_train_encoded.shape))\\nlogger.info(\\'df_test_encoded: {}\\'.format(df_test_encoded.shape))\\nlogger.info(\\'products_encoded: {}\\'.format(products_encoded.shape))\\n\\nnum_features = [\\'price\\', \\'len_title\\', \\'len_desc\\']\\nif dense_norm:\\n    logger.info(\\'MinMaxScaler Norm products_num_feas\\')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\\nfor fe in num_features:\\n    products_encoded.loc[:, fe] = products_encoded.loc[:, fe].astype(\\'float32\\')\\n\\nlogger.info(\\'Load Hand-made Seq Features\\')\\ndf_train_seqs_feas_all = pd.read_csv(\\'data/df_train_seqs_feas_all.csv\\')  # 29维特征\\ndf_test_seqs_feas_all = pd.read_csv(\\'data/df_test_seqs_feas_all.csv\\')\\nlogger.info(\\'df_train_seqs_feas_all: {}\\'.format(df_train_seqs_feas_all.shape))\\nlogger.info(\\'df_test_seqs_feas_all: {}\\'.format(df_test_seqs_feas_all.shape))\\nseqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if \\'NUNIQUE\\' in f or \\'COUNT\\' in f]\\nseqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\\nlogger.info(\\'seqs_cat_feas: {}\\'.format(seqs_cat_feas))\\nlogger.info(\\'seqs_num_feas: {}\\'.format(seqs_num_feas))\\n\\nif dense_norm:\\n    logger.info(\\'MinMaxScaler Norm seqs_num_feas\\')\\n    mms = MinMaxScaler(feature_range=(0,1))\\n    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\\n    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\\nfor fe in seqs_num_feas:\\n    df_train_seqs_feas_all.loc[:, fe] = df_train_seqs_feas_all.loc[:, fe].astype(\\'float32\\')\\n    df_test_seqs_feas_all.loc[:, fe] = df_test_seqs_feas_all.loc[:, fe].astype(\\'float32\\')\\n\\ndf_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\\ndf_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\\ndf_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\\ndf_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\\nlogger.info(\\'df_train_seqs_cat_feas: {}\\'.format(df_train_seqs_cat_feas.shape))\\nlogger.info(\\'df_train_seqs_num_feas: {}\\'.format(df_train_seqs_num_feas.shape))\\nlogger.info(\\'df_test_seqs_cat_feas: {}\\'.format(df_test_seqs_cat_feas.shape))\\nlogger.info(\\'df_test_seqs_num_feas: {}\\'.format(df_test_seqs_num_feas.shape))\\n\\nid_count = products_encoded.shape[0]\\n\\ntrain_preds_encoded = pickle.load(open(\\'./data/train_preds_all_encoded_new.pkl\\', \\'rb\\'))  # (len_train, 100)\\ntest_preds_encoded = pickle.load(open(\\'./data/test_preds_all_encoded_new.pkl\\', \\'rb\\'))  # (len_test, 100)\\ntest_preds = pickle.load(open(\\'./data/test_preds_all.pkl\\', \\'rb\\'))\\nlogger.info(\\'train_preds_encoded: {}\\'.format(len(train_preds_encoded)))\\nlogger.info(\\'test_preds_encoded: {}\\'.format(len(test_preds_encoded)))\\nlogger.info(\\'test_preds: {}\\'.format(len(test_preds)))\\n\\nlogger.info(\\'Cutting the candidate_set to {}\\'.format(len_candidate_set))\\ncut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\\ndf_train_encoded[\\'recall\\'] = cut_train_preds_encoded\\ncut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\\ndf_test_encoded[\\'recall\\'] = cut_test_preds_encoded\\n\\nlogger.info(\\'Eval the prev_items\\')\\ndf_train_encoded[\\'prev_items\\'] = df_train_encoded[\\'prev_items\\'].apply(eval)\\ndf_test_encoded[\\'prev_items\\'] = df_test_encoded[\\'prev_items\\'].apply(eval)\\n\\ndf_test = pd.read_csv(\\'data/sessions_test_task1.csv\\')\\nlogger.info(\\'df_test: {}\\'.format(df_test.shape))\\n\\ntmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\\ntmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\\n\\ndata_feature = {}\\ndata_feature[\\'len_encode_brand\\'] = products_encoded[\\'encode_brand\\'].nunique()\\ndata_feature[\\'len_encode_color\\'] = products_encoded[\\'encode_color\\'].nunique()\\ndata_feature[\\'len_encode_size\\'] = products_encoded[\\'encode_size\\'].nunique()\\ndata_feature[\\'len_encode_model\\'] = products_encoded[\\'encode_model\\'].nunique()\\ndata_feature[\\'len_encode_material\\'] = products_encoded[\\'encode_material\\'].nunique()\\ndata_feature[\\'len_encode_author\\'] = products_encoded[\\'encode_author\\'].nunique()\\ndata_feature[\\'len_locale\\'] = len(loc2id)\\ndata_feature[\\'dense_bins\\'] = dense_bins\\ndata_feature[\\'id_count\\'] = id_count\\ndata_feature[\\'len_features\\'] = products_encoded.shape[1] - 1\\ndata_feature[\\'len_emb_features\\'] = 3\\ndata_feature[\\'len_candidate_set\\'] = len_candidate_set\\ndata_feature[\\'w2v_vector_size\\'] = w2v_vector_size\\ndata_feature[\\'sentence_vector_size\\'] = 384\\ndata_feature[\\'len_seqs_cat_feas\\'] = len(seqs_cat_feas)\\ndata_feature[\\'len_seqs_num_feas\\'] = len(seqs_num_feas)\\ndata_feature[\\'seq_emb_factor\\'] = seq_emb_factor\\ndata_feature.update(tmp_nunique)\\nlogger.info(\\'data_feature:\\')\\nlogger.info(data_feature)\\n\\ndel tmp\\n\\n# 加载模型等\\n\\nlogger.info(\\'create model\\')\\n\\nproducts_input = {name: torch.tensor(products_encoded[name].values).to(device) for name in products_encoded.columns}\\n\\nif \\'BaseModel\\' in model_name:\\n    model = BaseModel(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\\nelif \\'MatchModel\\' in model_name:\\n    model = MatchModelV2(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\\nelse:\\n    raise ValueError(\\'Error model name {}\\'.format(model_name))\\nlogger.info(model)\\n\\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\\n\\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=\\'max\\', patience=lr_patience, factor=lr_decay_ratio)\\n\\nfor name, param in model.named_parameters():\\n    logger.info(str(name) + \\'\\\\t\\' + str(param.shape) + \\'\\\\t\\' +\\n                              str(param.device) + \\'\\\\t\\' + str(param.requires_grad))\\ntotal_num = sum([param.nelement() for param in model.parameters()])\\nlogger.info(\\'Total parameter numbers: {}\\'.format(total_num))\\n\\n\\n# 数据集DataLoader\\ntrain_set = NNDatasetV2(df_train_encoded, df_train_seqs_cat_feas, \\n                       df_train_seqs_num_feas)\\nlogger.info(\\'train_set: {}\\'.format(len(train_set)))\\ntest_set = NNDatasetV2(df_test_encoded, df_test_seqs_cat_feas, \\n                       df_test_seqs_num_feas)\\nlogger.info(\\'test_set: {}\\'.format(len(test_set)))\\n\\n\\ndef collate_fn(indices):\\n    batch_prev_items = []\\n    batch_locale = []\\n    batch_candidate_set = []\\n    batch_len = []\\n    batch_mask = []\\n    batch_label = []\\n    batch_label_index = []  # 交叉熵需要的是label在候选集中的index\\n    batch_seq_cat = []\\n    batch_seq_num = []\\n    for item in indices:\\n        batch_len.append(len(item[0]))  # prev_items\\n    max_len = max(batch_len)\\n    for item in indices:\\n        l = len(item[0])\\n        batch_mask.append([1] * (l) + [0] * (max_len - l))  # 0代表padding的位置，需要mask\\n    for item in indices:\\n        # [\\'prev_items\\', \\'locale\\', \\'recall\\', \\'next_item\\', \\'seqs_cat_feas\\', \\'seqs_num_feas\\']\\n        prev_items = item[0].copy()\\n        while (len(prev_items) < max_len):\\n            prev_items.append(id_count)  # embdding的时候id_count+1，把id_count作为padding了\\n        batch_prev_items.append(prev_items)\\n        batch_locale.append(item[1])\\n        batch_candidate_set.append(item[2].copy())\\n        batch_label.append(item[3])\\n        if item[3] in item[2]:\\n            batch_label_index.append(item[2].index(item[3]))\\n        else:\\n            batch_label_index.append(len(item[2]))\\n        batch_seq_cat.append(item[4])\\n        batch_seq_num.append(item[5])\\n    return [torch.LongTensor(batch_prev_items).to(device), torch.LongTensor(batch_locale).to(device), \\n            torch.LongTensor(batch_candidate_set).to(device),\\n            torch.LongTensor(batch_len).to(device), torch.LongTensor(batch_mask).to(device), \\n            torch.LongTensor(batch_label).to(device), torch.LongTensor(batch_label_index).to(device),\\n            torch.LongTensor(batch_seq_cat).to(device), torch.FloatTensor(batch_seq_num).to(device)]\\n\\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\\nlogger.info(\\'train_loader: {}\\'.format(len(train_loader)))\\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\\nlogger.info(\\'test_loader: {}\\'.format(len(test_loader)))\\n\\noutput_dir = \\'ckpt/{}\\'.format(exp_id)\\nensure_dir(output_dir)\\n\\nif load_init:\\n    assert len(load_exp_id) == len(load_epoch)\\n\\n\\nlogger.info(\\'Testing...\\')\\ntest_scores_all = []\\ntest_res_all = []\\nfor index, exp_id_index in enumerate(load_exp_id):\\n    logger.info(\\'Index {}\\'.format(index))\\n    epoch_index = load_epoch[index]\\n    fold_index = load_Fold[index]\\n    model_name_index = \\'MatchModelV2withATTMatchFold{}\\'.format(fold_index)\\n\\n    load_dir = \\'ckpt/{}\\'.format(exp_id_index)\\n    load_path = \\'{}/{}_{}_{}.pt\\'.format(load_dir, exp_id_index, model_name_index, epoch_index)\\n    logger.info(\\'Load Init model from {}\\'.format(load_path))\\n    model.load_state_dict(torch.load(load_path, map_location=\\'cpu\\'))\\n    model.to(device)\\n\\n    # 开始评估\\n    test_scores = []\\n    test_res = []\\n    model.eval()\\n    for batch_prev_items, batch_locale, batch_candidate_set, batch_len, batch_mask, \\\\\\n            _, batch_label_index, batch_seq_cat, batch_seq_num in tqdm(test_loader, desc=\\'test model {}\\'.format(exp_id), total=len(test_loader)):\\n        score, _ = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \\n                                        batch_candidate_set=batch_candidate_set, batch_len=batch_len, \\n                                        batch_label=batch_label_index, batch_mask=batch_mask,\\n                                        batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)  # (batch_size, 10)\\n        test_scores.append(score.detach().cpu().numpy())\\n        sorted_indices = torch.argsort(score, dim=1, descending=True)\\n        sorted_candidate_set = batch_candidate_set.gather(dim=1, index=sorted_indices)  # (B, 10)\\n        test_res.append(sorted_candidate_set.detach().cpu().numpy())\\n    test_scores = np.concatenate(test_scores, axis=0)  # (N, 10)\\n    test_res = np.concatenate(test_res, axis=0)\\n    test_scores_all.append(test_scores)\\n    test_res_all.append(test_res_all)', 'np': <module 'numpy' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/numpy/__init__.py'>, 'pd': <module 'pandas' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/pandas/__init__.py'>, 'plt': <module 'matplotlib.pyplot' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/matplotlib/pyplot.py'>, 'warnings': <module 'warnings' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/warnings.py'>, 'json': <module 'json' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/json/__init__.py'>, 'pickle': <module 'pickle' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/pickle.py'>, 'LabelEncoder': <class 'sklearn.preprocessing._label.LabelEncoder'>, 'MinMaxScaler': <class 'sklearn.preprocessing._data.MinMaxScaler'>, 'random': <module 'random' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/random.py'>, 'tqdm': <class 'tqdm.std.tqdm'>, 'torch': <module 'torch' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/__init__.py'>, 'nn': <module 'torch.nn' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/nn/__init__.py'>, 'pack_padded_sequence': <function pack_padded_sequence at 0x7f52110a4c10>, 'pad_packed_sequence': <function pad_packed_sequence at 0x7f52110a4ca0>, 'random_split': <function random_split at 0x7f5210c739d0>, 'DataLoader': <class 'torch.utils.data.dataloader.DataLoader'>, 'Dataset': <class 'torch.utils.data.dataset.Dataset'>, 'argparse': <module 'argparse' from '/home/panda/anaconda3/envs/libcityng/lib/python3.9/argparse.py'>, 'set_random_seed': <function set_random_seed at 0x7f5210b3c9d0>, 'get_logger': <function get_logger at 0x7f5210b3cb80>, 'ensure_dir': <function ensure_dir at 0x7f5210b3caf0>, 'str2bool': <function str2bool at 0x7f5210b3c670>, 'str2float': <function str2float at 0x7f5210b3c940>, 'NNDataset': <class 'data.NNDataset'>, 'NNDatasetV2': <class 'data.NNDatasetV2'>, 'MatchModel': <class 'model.MatchModel'>, 'BaseModel': <class 'model.BaseModel'>, 'MatchModelV2': <class 'model.MatchModelV2'>, 'emb_dim': 16, 'dense_bins': 10, 'hid_dim': 256, 'dropout': 0.1, 'layers': 4, 'bidirectional': False, 'seq_emb_factor': 4, 'batch_size': 1024, 'epochs': 100, 'len_candidate_set': 100, 'device': device(type='cuda', index=3), 'dense_norm': True, 'num_workers': 0, 'load_init': True, 'load_exp_id': [93545, 43234, 84653, 29020], 'load_epoch': [52, 51, 53, 50], 'load_Fold': [0, 1, 2, 3], 'learning_rate': 0.001, 'weight_decay': 1e-05, 'early_stop_lr': 1e-06, 'lr_patience': 5, 'lr_decay_ratio': 0.1, 'clip': 5, 'log_every': 100, 'early_stop': True, 'patience': 10, 'kfold': 5, 'attn_match': True, 'w2v_window': 3, 'w2v_min_count': 1, 'w2v_epochs': 500, 'w2v_vector_size': 128, 'seed': 2023, 'model_name': 'MatchModelV2withATTMatch', 'loc2id': {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}, 'config': {...}, 'exp_id': 85406, 'logger': <RootLogger root (INFO)>}\n",
      "2023-05-17 11:04:08,119 - INFO - load_exp_id = [93545, 43234, 84653, 29020]\n",
      "2023-05-17 11:04:08,120 - INFO - load_epoch = [52, 51, 53, 50]\n",
      "2023-05-17 11:04:08,120 - INFO - read data\n",
      "2023-05-17 11:04:17,121 - INFO - titles_embedding: (1410675, 384)\n",
      "2023-05-17 11:04:17,122 - INFO - descs_embedding: (1410675, 384)\n",
      "2023-05-17 11:04:19,522 - INFO - product2id: 1410675\n",
      "2023-05-17 11:04:19,523 - INFO - id2product: 1410675\n",
      "2023-05-17 11:04:21,575 - INFO - word2vec_embedding: (1410675, 128)\n",
      "2023-05-17 11:04:26,093 - INFO - df_train_encoded: (3606249, 4)\n",
      "2023-05-17 11:04:26,094 - INFO - df_test_encoded: (316971, 4)\n",
      "2023-05-17 11:04:26,094 - INFO - products_encoded: (1410675, 14)\n",
      "2023-05-17 11:04:26,095 - INFO - MinMaxScaler Norm products_num_feas\n",
      "2023-05-17 11:04:26,653 - INFO - Load Hand-made Seq Features\n",
      "2023-05-17 11:04:35,393 - INFO - df_train_seqs_feas_all: (3606249, 29)\n",
      "2023-05-17 11:04:35,394 - INFO - df_test_seqs_feas_all: (316971, 29)\n",
      "2023-05-17 11:04:35,395 - INFO - seqs_cat_feas: ['idNUNIQUE', 'idCOUNT', 'brandNUNIQUE', 'brandCOUNT', 'colorNUNIQUE', 'colorCOUNT', 'sizeNUNIQUE', 'sizeCOUNT', 'modelNUNIQUE', 'modelCOUNT', 'materialNUNIQUE', 'materialCOUNT', 'authorNUNIQUE', 'authorCOUNT']\n",
      "2023-05-17 11:04:35,395 - INFO - seqs_num_feas: ['priceMEAN', 'priceSTD', 'priceMIN', 'priceMAX', 'priceSUM', 'len_titleMEAN', 'len_titleSTD', 'len_titleMIN', 'len_titleMAX', 'len_titleSUM', 'len_descMEAN', 'len_descSTD', 'len_descMIN', 'len_descMAX', 'len_descSUM']\n",
      "2023-05-17 11:04:35,396 - INFO - MinMaxScaler Norm seqs_num_feas\n",
      "2023-05-17 11:04:50,777 - INFO - df_train_seqs_cat_feas: (3606249, 14)\n",
      "2023-05-17 11:04:50,779 - INFO - df_train_seqs_num_feas: (3606249, 15)\n",
      "2023-05-17 11:04:50,779 - INFO - df_test_seqs_cat_feas: (316971, 14)\n",
      "2023-05-17 11:04:50,780 - INFO - df_test_seqs_num_feas: (316971, 15)\n",
      "2023-05-17 11:05:52,934 - INFO - train_preds_encoded: 3606249\n",
      "2023-05-17 11:05:52,935 - INFO - test_preds_encoded: 316971\n",
      "2023-05-17 11:05:52,935 - INFO - test_preds: 316971\n",
      "2023-05-17 11:05:52,936 - INFO - Cutting the candidate_set to 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3606249/3606249 [00:35<00:00, 100772.05it/s]\n",
      "100%|██████████| 316971/316971 [00:15<00:00, 19811.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-17 11:06:45,010 - INFO - Eval the prev_items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-17 11:07:37,806 - INFO - df_test: (316971, 2)\n",
      "2023-05-17 11:07:38,300 - INFO - data_feature:\n",
      "2023-05-17 11:07:38,301 - INFO - {'len_encode_brand': 177190, 'len_encode_color': 203261, 'len_encode_size': 218061, 'len_encode_model': 524102, 'len_encode_material': 45569, 'len_encode_author': 30836, 'len_locale': 6, 'dense_bins': 10, 'id_count': 1410675, 'len_features': 13, 'len_emb_features': 3, 'len_candidate_set': 100, 'w2v_vector_size': 128, 'sentence_vector_size': 384, 'len_seqs_cat_feas': 14, 'len_seqs_num_feas': 15, 'seq_emb_factor': 4, 'idNUNIQUE': 133, 'idCOUNT': 475, 'brandNUNIQUE': 39, 'brandCOUNT': 475, 'colorNUNIQUE': 54, 'colorCOUNT': 475, 'sizeNUNIQUE': 116, 'sizeCOUNT': 475, 'modelNUNIQUE': 59, 'modelCOUNT': 475, 'materialNUNIQUE': 26, 'materialCOUNT': 475, 'authorNUNIQUE': 65, 'authorCOUNT': 475}\n",
      "2023-05-17 11:07:38,303 - INFO - create model\n",
      "2023-05-17 11:07:55,913 - INFO - MatchModelV2(\n",
      "  (product_emb): ProductEmbedding(\n",
      "    (product_fea): Product(\n",
      "      (locale_emb): Embedding(6, 16)\n",
      "      (price_emb): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (len_title_emb): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (len_desc_emb): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (encode_brand_emb): Embedding(177190, 16)\n",
      "      (encode_color_emb): Embedding(203261, 16)\n",
      "      (encode_size_emb): Embedding(218061, 16)\n",
      "      (encode_model_emb): Embedding(524102, 16)\n",
      "      (encode_material_emb): Embedding(45569, 16)\n",
      "      (encode_author_emb): Embedding(30836, 16)\n",
      "      (encode_price_emb): Embedding(10, 16)\n",
      "      (encode_len_title_emb): Embedding(10, 16)\n",
      "      (encode_len_desc_emb): Embedding(10, 16)\n",
      "    )\n",
      "    (title_emb): Embedding(1410676, 384, padding_idx=1410675)\n",
      "    (title_linear): Linear(in_features=384, out_features=32, bias=True)\n",
      "    (desc_emb): Embedding(1410676, 384, padding_idx=1410675)\n",
      "    (desc_linear): Linear(in_features=384, out_features=32, bias=True)\n",
      "    (w2v_emb): Embedding(1410676, 128, padding_idx=1410675)\n",
      "    (w2v_linear): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      "  (seq_fea_emb): SeqFeatureEmbedding(\n",
      "    (idNUNIQUE_emb): Embedding(133, 16)\n",
      "    (idCOUNT_emb): Embedding(475, 16)\n",
      "    (brandNUNIQUE_emb): Embedding(39, 16)\n",
      "    (brandCOUNT_emb): Embedding(475, 16)\n",
      "    (colorNUNIQUE_emb): Embedding(54, 16)\n",
      "    (colorCOUNT_emb): Embedding(475, 16)\n",
      "    (sizeNUNIQUE_emb): Embedding(116, 16)\n",
      "    (sizeCOUNT_emb): Embedding(475, 16)\n",
      "    (modelNUNIQUE_emb): Embedding(59, 16)\n",
      "    (modelCOUNT_emb): Embedding(475, 16)\n",
      "    (materialNUNIQUE_emb): Embedding(26, 16)\n",
      "    (materialCOUNT_emb): Embedding(475, 16)\n",
      "    (authorNUNIQUE_emb): Embedding(65, 16)\n",
      "    (authorCOUNT_emb): Embedding(475, 16)\n",
      "    (seq_cat_emb): Sequential(\n",
      "      (0): Linear(in_features=224, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "    (seq_num_emb): Sequential(\n",
      "      (0): Linear(in_features=15, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(304, 256, num_layers=4, batch_first=True)\n",
      "  (intra_attn): IntraAttention(\n",
      "    (w1): Linear(in_features=256, out_features=1, bias=False)\n",
      "    (w2): Linear(in_features=256, out_features=256, bias=False)\n",
      "    (w3): Linear(in_features=256, out_features=256, bias=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output): MatcherV2(\n",
      "    (out_linear): Linear(in_features=320, out_features=1, bias=False)\n",
      "    (w1_linear): Linear(in_features=320, out_features=320, bias=False)\n",
      "    (w2_linear): Linear(in_features=304, out_features=320, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (loss_func): CrossEntropyLoss()\n",
      ")\n",
      "2023-05-17 11:07:55,918 - INFO - product_emb.product_fea.locale_emb.weight\ttorch.Size([6, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,918 - INFO - product_emb.product_fea.price_emb.weight\ttorch.Size([16, 1])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,919 - INFO - product_emb.product_fea.price_emb.bias\ttorch.Size([16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,919 - INFO - product_emb.product_fea.len_title_emb.weight\ttorch.Size([16, 1])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,920 - INFO - product_emb.product_fea.len_title_emb.bias\ttorch.Size([16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,920 - INFO - product_emb.product_fea.len_desc_emb.weight\ttorch.Size([16, 1])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,920 - INFO - product_emb.product_fea.len_desc_emb.bias\ttorch.Size([16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,921 - INFO - product_emb.product_fea.encode_brand_emb.weight\ttorch.Size([177190, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,922 - INFO - product_emb.product_fea.encode_color_emb.weight\ttorch.Size([203261, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,923 - INFO - product_emb.product_fea.encode_size_emb.weight\ttorch.Size([218061, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,923 - INFO - product_emb.product_fea.encode_model_emb.weight\ttorch.Size([524102, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,924 - INFO - product_emb.product_fea.encode_material_emb.weight\ttorch.Size([45569, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,925 - INFO - product_emb.product_fea.encode_author_emb.weight\ttorch.Size([30836, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,925 - INFO - product_emb.product_fea.encode_price_emb.weight\ttorch.Size([10, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,926 - INFO - product_emb.product_fea.encode_len_title_emb.weight\ttorch.Size([10, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,927 - INFO - product_emb.product_fea.encode_len_desc_emb.weight\ttorch.Size([10, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,928 - INFO - product_emb.title_emb.weight\ttorch.Size([1410676, 384])\tcuda:3\tFalse\n",
      "2023-05-17 11:07:55,928 - INFO - product_emb.title_linear.weight\ttorch.Size([32, 384])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,929 - INFO - product_emb.title_linear.bias\ttorch.Size([32])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,929 - INFO - product_emb.desc_emb.weight\ttorch.Size([1410676, 384])\tcuda:3\tFalse\n",
      "2023-05-17 11:07:55,930 - INFO - product_emb.desc_linear.weight\ttorch.Size([32, 384])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,931 - INFO - product_emb.desc_linear.bias\ttorch.Size([32])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,932 - INFO - product_emb.w2v_emb.weight\ttorch.Size([1410676, 128])\tcuda:3\tFalse\n",
      "2023-05-17 11:07:55,932 - INFO - product_emb.w2v_linear.weight\ttorch.Size([32, 128])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,933 - INFO - product_emb.w2v_linear.bias\ttorch.Size([32])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,934 - INFO - seq_fea_emb.idNUNIQUE_emb.weight\ttorch.Size([133, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,934 - INFO - seq_fea_emb.idCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,935 - INFO - seq_fea_emb.brandNUNIQUE_emb.weight\ttorch.Size([39, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,936 - INFO - seq_fea_emb.brandCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,936 - INFO - seq_fea_emb.colorNUNIQUE_emb.weight\ttorch.Size([54, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,936 - INFO - seq_fea_emb.colorCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,937 - INFO - seq_fea_emb.sizeNUNIQUE_emb.weight\ttorch.Size([116, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,937 - INFO - seq_fea_emb.sizeCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,942 - INFO - seq_fea_emb.modelNUNIQUE_emb.weight\ttorch.Size([59, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,942 - INFO - seq_fea_emb.modelCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,942 - INFO - seq_fea_emb.materialNUNIQUE_emb.weight\ttorch.Size([26, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,943 - INFO - seq_fea_emb.materialCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,943 - INFO - seq_fea_emb.authorNUNIQUE_emb.weight\ttorch.Size([65, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,944 - INFO - seq_fea_emb.authorCOUNT_emb.weight\ttorch.Size([475, 16])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,944 - INFO - seq_fea_emb.seq_cat_emb.0.weight\ttorch.Size([256, 224])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,944 - INFO - seq_fea_emb.seq_cat_emb.0.bias\ttorch.Size([256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,945 - INFO - seq_fea_emb.seq_cat_emb.2.weight\ttorch.Size([32, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,946 - INFO - seq_fea_emb.seq_cat_emb.2.bias\ttorch.Size([32])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,947 - INFO - seq_fea_emb.seq_num_emb.0.weight\ttorch.Size([256, 15])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,947 - INFO - seq_fea_emb.seq_num_emb.0.bias\ttorch.Size([256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,948 - INFO - seq_fea_emb.seq_num_emb.2.weight\ttorch.Size([32, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,948 - INFO - seq_fea_emb.seq_num_emb.2.bias\ttorch.Size([32])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,949 - INFO - lstm.weight_ih_l0\ttorch.Size([1024, 304])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,949 - INFO - lstm.weight_hh_l0\ttorch.Size([1024, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,950 - INFO - lstm.bias_ih_l0\ttorch.Size([1024])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,950 - INFO - lstm.bias_hh_l0\ttorch.Size([1024])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,951 - INFO - lstm.weight_ih_l1\ttorch.Size([1024, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,952 - INFO - lstm.weight_hh_l1\ttorch.Size([1024, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,952 - INFO - lstm.bias_ih_l1\ttorch.Size([1024])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,952 - INFO - lstm.bias_hh_l1\ttorch.Size([1024])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,953 - INFO - lstm.weight_ih_l2\ttorch.Size([1024, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,954 - INFO - lstm.weight_hh_l2\ttorch.Size([1024, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,954 - INFO - lstm.bias_ih_l2\ttorch.Size([1024])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,955 - INFO - lstm.bias_hh_l2\ttorch.Size([1024])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,955 - INFO - lstm.weight_ih_l3\ttorch.Size([1024, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,955 - INFO - lstm.weight_hh_l3\ttorch.Size([1024, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,956 - INFO - lstm.bias_ih_l3\ttorch.Size([1024])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,957 - INFO - lstm.bias_hh_l3\ttorch.Size([1024])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,957 - INFO - intra_attn.w1.weight\ttorch.Size([1, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,958 - INFO - intra_attn.w2.weight\ttorch.Size([256, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,958 - INFO - intra_attn.w3.weight\ttorch.Size([256, 256])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,959 - INFO - output.out_linear.weight\ttorch.Size([1, 320])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,959 - INFO - output.w1_linear.weight\ttorch.Size([320, 320])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,960 - INFO - output.w2_linear.weight\ttorch.Size([320, 304])\tcuda:3\tTrue\n",
      "2023-05-17 11:07:55,960 - INFO - Total parameter numbers: 1285804480\n",
      "2023-05-17 11:07:57,392 - INFO - train_set: 3606249\n",
      "2023-05-17 11:07:57,507 - INFO - test_set: 316971\n",
      "2023-05-17 11:07:57,509 - INFO - train_loader: 3522\n",
      "2023-05-17 11:07:57,510 - INFO - test_loader: 310\n",
      "2023-05-17 11:07:57,513 - INFO - Testing...\n",
      "2023-05-17 11:07:57,514 - INFO - Index 0\n",
      "2023-05-17 11:07:57,515 - INFO - Load Init model from ckpt/93545/93545_MatchModelV2withATTMatchFold0_52.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test model 85406:   0%|          | 0/310 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d09b4530e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_prev_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_locale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_candidate_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_seq_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_seq_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test model {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         score, _ = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \n\u001b[0m\u001b[1;32m    360\u001b[0m                                         \u001b[0mbatch_candidate_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_candidate_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                                         \u001b[0mbatch_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_label_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/jjw/competition/KDDCUP2023/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, batch_prev_items, batch_locale, batch_candidate_set, batch_len, batch_label, batch_seq_cat, batch_seq_num, batch_mask)\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mcandidate_prob\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msoftmax\u001b[0m \u001b[0m后对候选集下一跳的概率预测\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \"\"\"\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_prev_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_locale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_candidate_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_seq_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_seq_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/jjw/competition/KDDCUP2023/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_prev_items, batch_locale, batch_candidate_set, batch_len, batch_label, batch_seq_cat, batch_seq_num, batch_mask, train)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;31m# LSTM with Mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mpack_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m             \u001b[0mpack_lstm_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0mlstm_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_lstm_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, len, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    695\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    696\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "# import re\n",
    "# import math\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "# from xgboost import XGBRegressor, XGBClassifier\n",
    "# from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import catboost as cab\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "# from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "# from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
    "# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "# from sklearn import metrics\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import json \n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# import sentence_transformers \n",
    "# from sklearn.preprocessing import KBinsDiscretizer\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import argparse\n",
    "from utils import set_random_seed, get_logger, ensure_dir, str2bool, str2float\n",
    "from data import NNDataset, NNDatasetV2\n",
    "from model import MatchModel, BaseModel, MatchModelV2\n",
    "\n",
    "\"\"\"\n",
    "相比于trainDeeep.py，加入一些手动聚合的序列特征，例如历史序列的平均价格，历史序列的不同类别数之类的\n",
    "SeqFeatureEmbedding现在使用2层全连接，可以换多层！\n",
    "# TODO: product-id作为特征之一\n",
    "# TODO: emb_dim变大 不同特征用不同emb_dim等\n",
    "# TODO: DIN等序列模型\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "emb_dim = 16\n",
    "dense_bins = 10\n",
    "hid_dim = 256\n",
    "dropout = 0.1\n",
    "layers = 4\n",
    "bidirectional = False\n",
    "seq_emb_factor = 4  # 人工序列特征的嵌入是emb_dim的几倍\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "len_candidate_set = 100\n",
    "device = torch.device('cuda:3')\n",
    "dense_norm = True\n",
    "num_workers = 0\n",
    "\n",
    "load_init = True\n",
    "\n",
    "load_exp_id = [\n",
    "    93545,\n",
    "    43234,\n",
    "    84653,\n",
    "    29020,\n",
    "]\n",
    "load_epoch = [\n",
    "    52,\n",
    "    51,\n",
    "    53,\n",
    "    50\n",
    "]\n",
    "load_Fold = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3\n",
    "]\n",
    "\n",
    "# TODO: 调整batch-size， 调整hidden-size\n",
    "# TODO: 跑Fold 4\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "early_stop_lr = 1e-6\n",
    "lr_patience = 5\n",
    "lr_decay_ratio = 0.1\n",
    "clip = 5\n",
    "log_every = 100\n",
    "early_stop = True\n",
    "patience = 10\n",
    "kfold = 5\n",
    "attn_match = True \n",
    "\n",
    "w2v_window = 3\n",
    "w2v_min_count = 1\n",
    "w2v_epochs = 500\n",
    "w2v_vector_size = 128\n",
    "\n",
    "seed = 2023\n",
    "set_random_seed(seed)\n",
    "\n",
    "model_name = 'MatchModelV2withATTMatch'\n",
    "loc2id = {'DE': 0, 'JP': 1, 'UK': 2, 'ES': 3, 'FR': 4, 'IT': 5}\n",
    "\n",
    "config = locals()\n",
    "\n",
    "# 加载必要的数据\n",
    "\n",
    "exp_id = config.get('exp_id', None)\n",
    "if exp_id is None:\n",
    "    exp_id = int(random.SystemRandom().random() * 100000)\n",
    "    config['exp_id'] = exp_id\n",
    "\n",
    "logger = get_logger(config)\n",
    "logger.info('Exp_id {}'.format(exp_id))\n",
    "logger.info(config)\n",
    "\n",
    "logger.info('load_exp_id = {}'.format(load_exp_id))\n",
    "logger.info('load_epoch = {}'.format(load_epoch))\n",
    "\n",
    "logger.info('read data')\n",
    "\n",
    "titles_embedding = np.load('./data/titles_embedding.npy')\n",
    "descs_embedding = np.load('./data/descs_embedding.npy')\n",
    "logger.info('titles_embedding: {}'.format(titles_embedding.shape))\n",
    "logger.info('descs_embedding: {}'.format(descs_embedding.shape))\n",
    "\n",
    "product2id = json.load(open('data/product2id.json', 'r'))\n",
    "id2product = json.load(open('data/id2product.json', 'r'))\n",
    "id2product = {int(k): v for k, v in id2product.items()}\n",
    "logger.info('product2id: {}'.format(len(product2id)))\n",
    "logger.info('id2product: {}'.format(len(id2product)))\n",
    "\n",
    "word2vec_embedding = np.load('./data/word2vec_embedding.npy')\n",
    "logger.info('word2vec_embedding: {}'.format(word2vec_embedding.shape))\n",
    "\n",
    "top200 = pickle.load(open('data/top200_new.pkl', 'rb'))\n",
    "\n",
    "df_train_encoded = pd.read_csv('data/df_train_encoded.csv')\n",
    "df_test_encoded = pd.read_csv('data/df_test_encoded.csv')\n",
    "products_encoded = pd.read_csv('./data/products_encoded.csv')\n",
    "logger.info('df_train_encoded: {}'.format(df_train_encoded.shape))\n",
    "logger.info('df_test_encoded: {}'.format(df_test_encoded.shape))\n",
    "logger.info('products_encoded: {}'.format(products_encoded.shape))\n",
    "\n",
    "num_features = ['price', 'len_title', 'len_desc']\n",
    "if dense_norm:\n",
    "    logger.info('MinMaxScaler Norm products_num_feas')\n",
    "    mms = MinMaxScaler(feature_range=(0,1))\n",
    "    products_encoded[num_features] = mms.fit_transform(products_encoded[num_features])\n",
    "for fe in num_features:\n",
    "    products_encoded.loc[:, fe] = products_encoded.loc[:, fe].astype('float32')\n",
    "\n",
    "logger.info('Load Hand-made Seq Features')\n",
    "df_train_seqs_feas_all = pd.read_csv('data/df_train_seqs_feas_all.csv')  # 29维特征\n",
    "df_test_seqs_feas_all = pd.read_csv('data/df_test_seqs_feas_all.csv')\n",
    "logger.info('df_train_seqs_feas_all: {}'.format(df_train_seqs_feas_all.shape))\n",
    "logger.info('df_test_seqs_feas_all: {}'.format(df_test_seqs_feas_all.shape))\n",
    "seqs_cat_feas = [f for f in df_train_seqs_feas_all.columns if 'NUNIQUE' in f or 'COUNT' in f]\n",
    "seqs_num_feas = [f for f in df_train_seqs_feas_all.columns if f not in seqs_cat_feas]\n",
    "logger.info('seqs_cat_feas: {}'.format(seqs_cat_feas))\n",
    "logger.info('seqs_num_feas: {}'.format(seqs_num_feas))\n",
    "\n",
    "if dense_norm:\n",
    "    logger.info('MinMaxScaler Norm seqs_num_feas')\n",
    "    mms = MinMaxScaler(feature_range=(0,1))\n",
    "    df_train_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_train_seqs_feas_all[seqs_num_feas])\n",
    "    df_test_seqs_feas_all[seqs_num_feas] = mms.fit_transform(df_test_seqs_feas_all[seqs_num_feas])\n",
    "for fe in seqs_num_feas:\n",
    "    df_train_seqs_feas_all.loc[:, fe] = df_train_seqs_feas_all.loc[:, fe].astype('float32')\n",
    "    df_test_seqs_feas_all.loc[:, fe] = df_test_seqs_feas_all.loc[:, fe].astype('float32')\n",
    "\n",
    "df_train_seqs_cat_feas = df_train_seqs_feas_all[seqs_cat_feas]\n",
    "df_train_seqs_num_feas = df_train_seqs_feas_all[seqs_num_feas]\n",
    "df_test_seqs_cat_feas = df_test_seqs_feas_all[seqs_cat_feas]\n",
    "df_test_seqs_num_feas = df_test_seqs_feas_all[seqs_num_feas]\n",
    "logger.info('df_train_seqs_cat_feas: {}'.format(df_train_seqs_cat_feas.shape))\n",
    "logger.info('df_train_seqs_num_feas: {}'.format(df_train_seqs_num_feas.shape))\n",
    "logger.info('df_test_seqs_cat_feas: {}'.format(df_test_seqs_cat_feas.shape))\n",
    "logger.info('df_test_seqs_num_feas: {}'.format(df_test_seqs_num_feas.shape))\n",
    "\n",
    "id_count = products_encoded.shape[0]\n",
    "\n",
    "train_preds_encoded = pickle.load(open('./data/train_preds_all_encoded_new.pkl', 'rb'))  # (len_train, 100)\n",
    "test_preds_encoded = pickle.load(open('./data/test_preds_all_encoded_new.pkl', 'rb'))  # (len_test, 100)\n",
    "test_preds = pickle.load(open('./data/test_preds_all.pkl', 'rb'))\n",
    "logger.info('train_preds_encoded: {}'.format(len(train_preds_encoded)))\n",
    "logger.info('test_preds_encoded: {}'.format(len(test_preds_encoded)))\n",
    "logger.info('test_preds: {}'.format(len(test_preds)))\n",
    "\n",
    "logger.info('Cutting the candidate_set to {}'.format(len_candidate_set))\n",
    "cut_train_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(train_preds_encoded, total=len(train_preds_encoded))]\n",
    "df_train_encoded['recall'] = cut_train_preds_encoded\n",
    "cut_test_preds_encoded = [lst[:len_candidate_set] for lst in tqdm(test_preds_encoded, total=len(test_preds_encoded))]\n",
    "df_test_encoded['recall'] = cut_test_preds_encoded\n",
    "\n",
    "logger.info('Eval the prev_items')\n",
    "df_train_encoded['prev_items'] = df_train_encoded['prev_items'].apply(eval)\n",
    "df_test_encoded['prev_items'] = df_test_encoded['prev_items'].apply(eval)\n",
    "\n",
    "df_test = pd.read_csv('data/sessions_test_task1.csv')\n",
    "logger.info('df_test: {}'.format(df_test.shape))\n",
    "\n",
    "tmp = pd.concat([df_train_seqs_feas_all[seqs_cat_feas], df_test_seqs_feas_all[seqs_cat_feas]])\n",
    "tmp_nunique = (tmp.max() + 1).to_dict()  # 不是nunique，因为这个是计数特征，不是连续的0~n-1\n",
    "\n",
    "data_feature = {}\n",
    "data_feature['len_encode_brand'] = products_encoded['encode_brand'].nunique()\n",
    "data_feature['len_encode_color'] = products_encoded['encode_color'].nunique()\n",
    "data_feature['len_encode_size'] = products_encoded['encode_size'].nunique()\n",
    "data_feature['len_encode_model'] = products_encoded['encode_model'].nunique()\n",
    "data_feature['len_encode_material'] = products_encoded['encode_material'].nunique()\n",
    "data_feature['len_encode_author'] = products_encoded['encode_author'].nunique()\n",
    "data_feature['len_locale'] = len(loc2id)\n",
    "data_feature['dense_bins'] = dense_bins\n",
    "data_feature['id_count'] = id_count\n",
    "data_feature['len_features'] = products_encoded.shape[1] - 1\n",
    "data_feature['len_emb_features'] = 3\n",
    "data_feature['len_candidate_set'] = len_candidate_set\n",
    "data_feature['w2v_vector_size'] = w2v_vector_size\n",
    "data_feature['sentence_vector_size'] = 384\n",
    "data_feature['len_seqs_cat_feas'] = len(seqs_cat_feas)\n",
    "data_feature['len_seqs_num_feas'] = len(seqs_num_feas)\n",
    "data_feature['seq_emb_factor'] = seq_emb_factor\n",
    "data_feature.update(tmp_nunique)\n",
    "logger.info('data_feature:')\n",
    "logger.info(data_feature)\n",
    "\n",
    "del tmp\n",
    "\n",
    "# 加载模型等\n",
    "\n",
    "logger.info('create model')\n",
    "\n",
    "products_input = {name: torch.tensor(products_encoded[name].values).to(device) for name in products_encoded.columns}\n",
    "\n",
    "if 'BaseModel' in model_name:\n",
    "    model = BaseModel(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\n",
    "elif 'MatchModel' in model_name:\n",
    "    model = MatchModelV2(config, data_feature, products_input, word2vec_embedding, titles_embedding, descs_embedding).to(device)\n",
    "else:\n",
    "    raise ValueError('Error model name {}'.format(model_name))\n",
    "logger.info(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', patience=lr_patience, factor=lr_decay_ratio)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    logger.info(str(name) + '\\t' + str(param.shape) + '\\t' +\n",
    "                              str(param.device) + '\\t' + str(param.requires_grad))\n",
    "total_num = sum([param.nelement() for param in model.parameters()])\n",
    "logger.info('Total parameter numbers: {}'.format(total_num))\n",
    "\n",
    "\n",
    "# 数据集DataLoader\n",
    "train_set = NNDatasetV2(df_train_encoded, df_train_seqs_cat_feas, \n",
    "                       df_train_seqs_num_feas)\n",
    "logger.info('train_set: {}'.format(len(train_set)))\n",
    "test_set = NNDatasetV2(df_test_encoded, df_test_seqs_cat_feas, \n",
    "                       df_test_seqs_num_feas)\n",
    "logger.info('test_set: {}'.format(len(test_set)))\n",
    "\n",
    "\n",
    "def collate_fn(indices):\n",
    "    batch_prev_items = []\n",
    "    batch_locale = []\n",
    "    batch_candidate_set = []\n",
    "    batch_len = []\n",
    "    batch_mask = []\n",
    "    batch_label = []\n",
    "    batch_label_index = []  # 交叉熵需要的是label在候选集中的index\n",
    "    batch_seq_cat = []\n",
    "    batch_seq_num = []\n",
    "    for item in indices:\n",
    "        batch_len.append(len(item[0]))  # prev_items\n",
    "    max_len = max(batch_len)\n",
    "    for item in indices:\n",
    "        l = len(item[0])\n",
    "        batch_mask.append([1] * (l) + [0] * (max_len - l))  # 0代表padding的位置，需要mask\n",
    "    for item in indices:\n",
    "        # ['prev_items', 'locale', 'recall', 'next_item', 'seqs_cat_feas', 'seqs_num_feas']\n",
    "        prev_items = item[0].copy()\n",
    "        while (len(prev_items) < max_len):\n",
    "            prev_items.append(id_count)  # embdding的时候id_count+1，把id_count作为padding了\n",
    "        batch_prev_items.append(prev_items)\n",
    "        batch_locale.append(item[1])\n",
    "        batch_candidate_set.append(item[2].copy())\n",
    "        batch_label.append(item[3])\n",
    "        if item[3] in item[2]:\n",
    "            batch_label_index.append(item[2].index(item[3]))\n",
    "        else:\n",
    "            batch_label_index.append(len(item[2]))\n",
    "        batch_seq_cat.append(item[4])\n",
    "        batch_seq_num.append(item[5])\n",
    "    return [torch.LongTensor(batch_prev_items).to(device), torch.LongTensor(batch_locale).to(device), \n",
    "            torch.LongTensor(batch_candidate_set).to(device),\n",
    "            torch.LongTensor(batch_len).to(device), torch.LongTensor(batch_mask).to(device), \n",
    "            torch.LongTensor(batch_label).to(device), torch.LongTensor(batch_label_index).to(device),\n",
    "            torch.LongTensor(batch_seq_cat).to(device), torch.FloatTensor(batch_seq_num).to(device)]\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "logger.info('train_loader: {}'.format(len(train_loader)))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
    "logger.info('test_loader: {}'.format(len(test_loader)))\n",
    "\n",
    "output_dir = 'ckpt/{}'.format(exp_id)\n",
    "ensure_dir(output_dir)\n",
    "\n",
    "if load_init:\n",
    "    assert len(load_exp_id) == len(load_epoch)\n",
    "\n",
    "\n",
    "logger.info('Testing...')\n",
    "test_scores_all = []\n",
    "test_res_all = []\n",
    "for index, exp_id_index in enumerate(load_exp_id):\n",
    "    logger.info('Index {}'.format(index))\n",
    "    epoch_index = load_epoch[index]\n",
    "    fold_index = load_Fold[index]\n",
    "    model_name_index = 'MatchModelV2withATTMatchFold{}'.format(fold_index)\n",
    "\n",
    "    load_dir = 'ckpt/{}'.format(exp_id_index)\n",
    "    load_path = '{}/{}_{}_{}.pt'.format(load_dir, exp_id_index, model_name_index, epoch_index)\n",
    "    logger.info('Load Init model from {}'.format(load_path))\n",
    "    model.load_state_dict(torch.load(load_path, map_location='cpu'))\n",
    "    model.to(device)\n",
    "\n",
    "    # 开始评估\n",
    "    test_scores = []\n",
    "    test_res = []\n",
    "    model.eval()\n",
    "    for batch_prev_items, batch_locale, batch_candidate_set, batch_len, batch_mask, \\\n",
    "            _, batch_label_index, batch_seq_cat, batch_seq_num in tqdm(test_loader, desc='test model {}'.format(exp_id), total=len(test_loader)):\n",
    "        score, _ = model.predict(batch_prev_items=batch_prev_items, batch_locale=batch_locale, \n",
    "                                        batch_candidate_set=batch_candidate_set, batch_len=batch_len, \n",
    "                                        batch_label=batch_label_index, batch_mask=batch_mask,\n",
    "                                        batch_seq_cat=batch_seq_cat, batch_seq_num=batch_seq_num)  # (batch_size, 10)\n",
    "        test_scores.append(score.detach().cpu().numpy())\n",
    "        sorted_indices = torch.argsort(score, dim=1, descending=True)\n",
    "        sorted_candidate_set = batch_candidate_set.gather(dim=1, index=sorted_indices)  # (B, 10)\n",
    "        test_res.append(sorted_candidate_set.detach().cpu().numpy())\n",
    "    test_scores = np.concatenate(test_scores, axis=0)  # (N, 10)\n",
    "    test_res = np.concatenate(test_res, axis=0)\n",
    "    test_scores_all.append(test_scores)\n",
    "    test_res_all.append(test_res_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libcityng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
